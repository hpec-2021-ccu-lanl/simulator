diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/CHANGELOG.md /home/craig/LANL/Batsim/April_9/theres/batsched_edit/CHANGELOG.md
--- /home/craig/LANL/Batsim/April_9/theres/batsched/CHANGELOG.md	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/CHANGELOG.md	2021-04-02 05:01:50.000000000 -0400
@@ -12,6 +12,10 @@
 [//]: =========================================================================
 ## [Unreleased]
 
+### Fixed
+- The `easy_bf_fast` did not try to backfill previously submitted jobs in many
+  events (when the priority job could not be executed).
+
 [//]: =========================================================================
 ## [1.3.0] - 2019-01-15 - For [Batsim v3.0.0][Batsim v3.0.0]
 ### Added
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/ci/analyze-coverage.bash /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/analyze-coverage.bash
--- /home/craig/LANL/Batsim/April_9/theres/batsched/ci/analyze-coverage.bash	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/analyze-coverage.bash	1969-12-31 19:00:00.000000000 -0500
@@ -1,37 +0,0 @@
-#!/usr/bin/env nix-shell
-#! nix-shell . -i bash -A test_deps_pinned
-set -eu
-
-echo "Prepare directories"
-rm -rf ./cover
-mkdir -p ./cover/tmp
-cd ./cover/tmp
-
-echo "Call gcov"
-gcov_files=$(find ../../build -name '*.gcda')
-for gcov_file in ${gcov_files[@]}; do
-    gcov ${gcov_file} 1>/dev/null 2>&1
-done
-
-echo "Only keep interesting files"
-interesting_sources=$(find ../../src -name '*.?pp' | sort | grep -v 'pempek_assert\|taywee_args')
-set +e
-for interesting_source in ${interesting_sources[@]}; do
-    interesting_file="./$(basename ${interesting_source}).gcov"
-    cp -f ${interesting_file} ../ 2>/dev/null
-done
-set -e
-
-cd ../..
-rm -rf ./cover/tmp
-
-echo "Run gcovr analysis (human-readable report)"
-gcovr -gk -o ./cover/summary.txt
-cat ./cover/summary.txt
-
-echo "Run gcovr analysis (html report)"
-rm -rf ./cover/html
-mkdir -p ./cover/html
-gcovr -gk --html-details -o ./cover/html/index.html
-
-exit 0
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/ci/build.bash /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/build.bash
--- /home/craig/LANL/Batsim/April_9/theres/batsched/ci/build.bash	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/build.bash	1969-12-31 19:00:00.000000000 -0500
@@ -1,16 +0,0 @@
-#!/usr/bin/env nix-shell
-#! nix-shell . -i bash -A batsched_local
-set -eux
-
-# Start from a clean build directory
-rm -rf ./build
-mkdir -p ./build
-
-# Usual cmake build stuff
-cd ./build
-cmake .. \
-    -DCMAKE_BUILD_TYPE=Debug \
-    -Denable_warnings=ON \
-    -Dtreat_warnings_as_errors=OFF \
-    -Ddo_coverage=ON
-make -j $(nproc)
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/ci/default.nix /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/default.nix
--- /home/craig/LANL/Batsim/April_9/theres/batsched/ci/default.nix	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/default.nix	1969-12-31 19:00:00.000000000 -0500
@@ -1,53 +0,0 @@
-let
-  pkgs = import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/18.03.tar.gz") {};
-  kapack = import
-    ( fetchTarball "https://github.com/oar-team/kapack/archive/master.tar.gz") {};
-in
-
-let
-  callPackage = pkgs.lib.callPackageWith (pkgs // pkgs.xlibs // self // kapack);
-  self = rec {
-    inherit pkgs kapack;
-
-    # Redefine some packages for clarity's sake
-    batexpe = kapack.batexpe;
-    batsim_pinned = (kapack.batsim.override {simgrid = kapack.simgrid_dev_working; }).overrideAttrs (attrs: rec {
-      name = "batsim-${version}";
-      version = "3.0.0-pinned";
-      src = pkgs.fetchgit {
-        url = "https://framagit.org/batsim/batsim.git";
-        rev = "12db5085210ac24d82657b21fafe0ca198dcf48d";
-        sha256 = "07b9npm5qvrzanp14rwp743dxsh7dwpvpywmlpxla5j4kxk665hc";
-      };
-    });
-    batsim_dev = (kapack.batsim.override {simgrid = kapack.simgrid_dev_working; }).overrideAttrs (attrs: rec {
-      nativeBuildInputs = attrs.nativeBuildInputs ++ [kapack.intervalset];
-      name = "batsim-${version}";
-      version = "3.1.0-dev";
-      src = fetchTarball "https://gitlab.inria.fr/batsim/batsim/repository/master/archive.tar.gz";
-    });
-
-    pytest = pkgs.python36Packages.pytest;
-    gcovr = kapack.gcovr;
-
-    # Packages defined in this tree
-    batsched_local = callPackage ./local.nix {};
-    test_deps_pinned = callPackage ./test-deps.nix {
-      batsim = batsim_pinned;
-    };
-    test_deps_dev = callPackage ./test-deps.nix {
-      batsim = batsim_dev;
-    };
-
-    # Packages meant to be used as shells
-    test_pinned = callPackage ./test-env.nix {
-      batsched = batsched_local;
-      test_deps = test_deps_pinned;
-    };
-    test_dev = callPackage ./test-env.nix {
-      batsched = batsched_local;
-      test_deps = test_deps_dev;
-    };
-  };
-in
-  self
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/ci/list-store-paths-for-cachix.bash /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/list-store-paths-for-cachix.bash
--- /home/craig/LANL/Batsim/April_9/theres/batsched/ci/list-store-paths-for-cachix.bash	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/list-store-paths-for-cachix.bash	2021-04-02 05:00:07.000000000 -0400
@@ -0,0 +1,10 @@
+#!/usr/bin/env bash
+function list_store_paths_for_cachix {
+    var_path=$1
+    echo $var_path | tr ':' '\n' | sed -E -n 'sW(/nix/store/.*)/.*W\1Wp'
+}
+
+list_store_paths_for_cachix ${CMAKE_INCLUDE_PATH}
+list_store_paths_for_cachix ${CMAKE_LIBRARY_PATH}
+list_store_paths_for_cachix ${PATH}
+list_store_paths_for_cachix ${PYTHONPATH}
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/ci/local.nix /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/local.nix
--- /home/craig/LANL/Batsim/April_9/theres/batsched/ci/local.nix	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/local.nix	1969-12-31 19:00:00.000000000 -0500
@@ -1,23 +0,0 @@
-{ stdenv, batsched_dev }:
-
-(batsched_dev.override {}).overrideAttrs (attrs: rec {
-    name = "batsched-1.4.0-nix-ci";
-    src = stdenv.lib.sourceByRegex ../. [
-      "^src$"
-      "^src/algo$"
-      "^src/external$"
-        ".*\.cpp$" ".*\.hpp$"
-      "^cmake$"
-      "^cmake/Modules$"
-        ".*\.cmake"
-        ".*\.cmake.in"
-      "^CMakeLists\.txt$"
-    ];
-    enableParallelBuilding = true;
-    doCheck = false;
-
-    preConfigure = ''
-      # Always start from a clean build directory
-      rm -rf ./build
-    '';
-})
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/ci/pin-batsim.bash /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/pin-batsim.bash
--- /home/craig/LANL/Batsim/April_9/theres/batsched/ci/pin-batsim.bash	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/pin-batsim.bash	1969-12-31 19:00:00.000000000 -0500
@@ -1,13 +0,0 @@
-#!/usr/bin/env nix-shell
-#! nix-shell . -i bash -A test_deps_dev
-
-if [ "$#" -ne 1 ]; then
-    echo 'usage: pin-batsim.bash BATSIM-REV'
-    exit 1
-fi
-
-rev=$1
-nix-prefetch-git \
-    --url https://framagit.org/batsim/batsim.git \
-    --rev ${rev} \
-    > batsim-pinned.json
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/ci/README.md /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/README.md
--- /home/craig/LANL/Batsim/April_9/theres/batsched/ci/README.md	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/README.md	1969-12-31 19:00:00.000000000 -0500
@@ -1,60 +0,0 @@
-This directory is essentially a nix repository with some scripts.
-
-# Packages overview
-## batsched_local
-This is the current version of batsched (built from current file hierarchy).
-
-## batsim_pinned
-This is the current *reference* Batsim version for batsched.
-batsched should always work with this version.
-
-## batsim_dev
-This is the up-to-date Batsim version.
-batsched should work with this version.
-
-## test_deps_(pinned|dev)
-The list of packages needed to run tests.
-This is meant to be used as a shell, not meant to be installed.
-
-## test_(pinned|dev)
-A shell used to run tests. Essentially batsim_local + test_deps.
-Not meant to be installed either.
-
-# Useful commands
-In all the following examples, the current directory is expected to be
-batsched's root.
-
-## Building packages
-This can be done via `nix-build`. Result will be in `./result/`.
-Some examples:
-``` bash
-nix-build ./ci -A batsched_local
-nix-build ./ci -A batsim_pinned
-```
-
-## Install packages
-This is done via `nix-env`:
-``` bash
-nix-env -f ./ci -iA batsched_local
-nix-env -f ./ci -iA batsim_dev
-```
-
-To uninstall them, use `nix-env --uninstall`.
-
-## Get into a shell to build packages
-`nix-shell` is your friend here. Example:
-``` bash
-nix-shell ./ci -A batsched_local
-# your shell now has all batsched's build dependencies!
-# you can freely build the project (cmake, make...)
-```
-
-## Run the tests
-This is essentially "run the test script in the desired environment".
-``` bash
-# test current batsched with batsim_pinned:
-nix-shell ./ci -A test_pinned --command './ci/run-tests.bash'
-
-# or test with batsim_dev:
-nix-shell ./ci -A test_dev --command './ci/run-tests.bash'
-```
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/ci/run-tests.bash /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/run-tests.bash
--- /home/craig/LANL/Batsim/April_9/theres/batsched/ci/run-tests.bash	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/run-tests.bash	1969-12-31 19:00:00.000000000 -0500
@@ -1,45 +0,0 @@
-#!/usr/bin/env nix-shell
-#! nix-shell . -i bash -A test_deps_pinned
-set -eu
-
-initial_dir=$(realpath .)
-
-# Run a redis server if needed
-redis_launched_here=0
-r=$(ps faux | grep redis-server | grep -v grep | wc -l)
-if [ $r -eq 0 ]
-then
-    echo "Running a Redis server..."
-    redis-server>/dev/null &
-    redis_launched_here=1
-
-    while ! nc -z localhost 6379; do
-      sleep 1
-    done
-fi
-
-# Add built batsched in PATH
-export PATH=$(realpath ./build):${PATH}
-
-# Set TEST_ROOT so simulation input files can be found
-export TEST_ROOT=$(realpath ./test)
-
-# Print which versions are used
-echo "batsched realpath: $(realpath $(which batsched))"
-echo "batsim realpath: $(realpath $(which batsim))"
-echo "robin realpath: $(realpath $(which robin))"
-
-# Execute the tests (TODO: clean tests)
-cd test
-pytest
-failed=$?
-
-# Stop the redis server if it has been launched by this script
-if [ $redis_launched_here -eq 1 ]
-then
-    echo "Stopping the Redis server..."
-    killall redis-server
-fi
-
-cd ${initial_dir}
-exit ${failed}
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/ci/test-deps.nix /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/test-deps.nix
--- /home/craig/LANL/Batsim/April_9/theres/batsched/ci/test-deps.nix	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/test-deps.nix	1969-12-31 19:00:00.000000000 -0500
@@ -1,16 +0,0 @@
-{ stdenv, batsim, batexpe,
-  which, redis, procps, psmisc, pytest, gcovr,
-  nix-prefetch-git
-}:
-
-stdenv.mkDerivation rec {
-  name = "batsched-test-deps";
-
-  # This package is not meant to be built
-  unpackPhase = "true";
-  installPhase = "true";
-  propagatedBuildInputs = [ batsim batexpe
-    which redis procps psmisc pytest gcovr
-    nix-prefetch-git
-  ];
-}
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/ci/test-env.nix /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/test-env.nix
--- /home/craig/LANL/Batsim/April_9/theres/batsched/ci/test-env.nix	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/test-env.nix	1969-12-31 19:00:00.000000000 -0500
@@ -1,10 +0,0 @@
-{ stdenv, batsched, test_deps }:
-
-stdenv.mkDerivation rec {
-  name = "batsched-test-env";
-
-  # This package is not meant to be built
-  unpackPhase = "true";
-  installPhase = "true";
-  propagatedBuildInputs = [ batsched test_deps ];
-}
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/ci/update-batsched_dev-cache.bash /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/update-batsched_dev-cache.bash
--- /home/craig/LANL/Batsim/April_9/theres/batsched/ci/update-batsched_dev-cache.bash	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/update-batsched_dev-cache.bash	1969-12-31 19:00:00.000000000 -0500
@@ -1,6 +0,0 @@
-#!/usr/bin/env nix-shell
-#! nix-shell -i bash -p nix
-set -eu
-
-# Build up-to-date batsched_dev package, push it on binary cache
-nix-build https://github.com/oar-team/kapack/archive/master.tar.gz -A batsched_dev | cachix push batsim
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/ci/update-dependencies-cache.bash /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/update-dependencies-cache.bash
--- /home/craig/LANL/Batsim/April_9/theres/batsched/ci/update-dependencies-cache.bash	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/ci/update-dependencies-cache.bash	1969-12-31 19:00:00.000000000 -0500
@@ -1,7 +0,0 @@
-#!/usr/bin/env nix-shell
-#! nix-shell -i bash -p nix
-set -eu
-
-# (re)build up-to-date CI batsched package + deps, push them on binary cache
-nix-build ./ci -A test_pinned | cachix push batsim
-nix-build ./ci -A test_dev | cachix push batsim
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/default.nix /home/craig/LANL/Batsim/April_9/theres/batsched_edit/default.nix
--- /home/craig/LANL/Batsim/April_9/theres/batsched/default.nix	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/default.nix	1969-12-31 19:00:00.000000000 -0500
@@ -1,16 +0,0 @@
-let
-  pkgs = import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/18.03.tar.gz") {};
-  kapack = import
-    ( fetchTarball "https://github.com/oar-team/kapack/archive/master.tar.gz")
-  { inherit pkgs; };
-in
-
-with kapack;
-with pkgs;
-
-(batsched_dev.override {}).overrideAttrs (attrs: rec {
-    name = "batsched-1.4.0-nix-local";
-    src = ../.;
-    enableParallelBuilding = true;
-    doCheck = false;
-})
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/.gitignore /home/craig/LANL/Batsim/April_9/theres/batsched_edit/.gitignore
--- /home/craig/LANL/Batsim/April_9/theres/batsched/.gitignore	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/.gitignore	2021-04-02 05:00:04.000000000 -0400
@@ -3,3 +3,4 @@
 test-out
 test-instances
 cover
+result
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/.gitlab-ci.yml /home/craig/LANL/Batsim/April_9/theres/batsched_edit/.gitlab-ci.yml
--- /home/craig/LANL/Batsim/April_9/theres/batsched/.gitlab-ci.yml	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/.gitlab-ci.yml	2021-04-02 05:01:51.000000000 -0400
@@ -4,86 +4,33 @@
   GIT_SUBMODULE_STRATEGY: none
 
 stages:
-  - build
-  - update_dependencies_cache
-  - test
-  - coverage
+  - big_stage
   - deploy
 
 ###############################################################################
-# Build stage
+# Do most tasks
 ###############################################################################
-build:
-  stage: build
+build_and_test:
+  stage: big_stage
   script:
-    - ./ci/build.bash
+    # Build batsched
+    - nix-shell --pure ./release.nix -A batsched --command ${CI_PROJECT_DIR}/ci/list-store-paths-for-cachix.bash | cachix push batsim
+    - nix-build ./release.nix -A batsched && cp -rL result ./batsched
+    # Test against pinned batsim
+    - nix-shell --pure ./release.nix -A integration_tests --command ${CI_PROJECT_DIR}/ci/list-store-paths-for-cachix.bash | cachix push batsim
+    - nix-build ./release.nix -A integration_tests && cp -rL result ./integration_tests
+    # Test against up-to-date batsim
+    - nix-shell --pure ./release.nix -A integration_tests_batlatest --command ${CI_PROJECT_DIR}/ci/list-store-paths-for-cachix.bash | cachix push batsim
+    - nix-build ./release.nix -A integration_tests_batlatest && rm result
+    # Fail job if tests failed
+    - if [[ "$(cat ./integration_tests/pytest_returncode)" -ne 0 ]] ; then echo "pytest returned non-zero (against pinned batsim), aborting" ; exit 1 ; fi
+    - if [[ "$(cat ./integration_tests_batlatest/pytest_returncode)" -ne 0 ]] ; then echo "pytest returned non-zero (against latest batsim), aborting" ; exit 1 ; fi
+    # Send coverage results to codecov.io
+    - nix-env -i gcc
+    - mkdir -p merged
+    - cp ./batsched/gcno/* ./integration_tests/gcda/* merged/
+    - bash <(curl -s https://codecov.io/bash)
   artifacts:
+    when: always
     paths:
-      - /builds/batsim/batsched/build
-
-###############################################################################
-# Dependencies cache stage
-###############################################################################
-update_dependencies_cache:
-  stage: update_dependencies_cache
-  script:
-    - ./ci/update-dependencies-cache.bash
-
-###############################################################################
-# Test stage
-###############################################################################
-test_pinned_batsim:
-  stage: test
-  script:
-    - nix-shell ./ci -A test_pinned --command 'bash ./ci/run-tests.bash'
-  dependencies:
-    - build
-  artifacts:
-    paths:
-      - /builds/batsim/batsched/build
-
-test_dev_batsim:
-  stage: test
-  script:
-    - nix-shell ./ci -A test_dev --command 'bash ./ci/run-tests.bash'
-  dependencies:
-    - build
-
-###############################################################################
-# Coverage stage
-###############################################################################
-coverage:
-  stage: coverage
-  script:
-    - nix-shell ./ci -A test_deps_pinned --command 'bash ./ci/analyze-coverage.bash'
-  dependencies:
-    - test_pinned_batsim
-  artifacts:
-    paths:
-      - /builds/batsim/batsched/cover
-
-###############################################################################
-# Deploy stage
-###############################################################################
-deploy_coverage:
-  stage: deploy
-  script:
-      # Pushes Batsim's code doc (doxygen) onto the gforge website.
-      # SSH setup (do NOT run these commands on your machine)
-      - eval $(ssh-agent -s)
-      - ssh-add <(echo "$SSH_PRIVATE_KEY")
-      - mkdir -p ~/.ssh
-      - '[[ -f /.dockerenv ]] && echo -e "Host *\n\tStrictHostKeyChecking no\n\n" >> ~/.ssh/config'
-      # Finally push the code documentation on the gforge website
-      - rsync -rlgoDz --delete cover/html/ mpoquet@scm.gforge.inria.fr:/home/groups/batsim/htdocs/batsched/coverage
-  dependencies:
-    - coverage
-  only:
-    - master
-
-deploy_batsched_dev_cachix:
-  stage: deploy
-  script:
-    - ./ci/update-batsched_dev-cache.bash
-  only:
-    - master
+      - /builds/batsim/batsched/integration_tests
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/meson.build /home/craig/LANL/Batsim/April_9/theres/batsched_edit/meson.build
--- /home/craig/LANL/Batsim/April_9/theres/batsched/meson.build	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/meson.build	2021-04-02 05:01:52.000000000 -0400
@@ -0,0 +1,120 @@
+project('batsched', 'cpp',
+    version: '1.3.0',
+    license: 'free',
+    default_options: ['cpp_std=c++17'],
+    meson_version: '>=0.40.0'
+)
+
+# Dependencies
+add_project_link_arguments(['-lstdc++fs'], language : 'cpp')
+boost_dep = dependency('boost',
+    modules : ['locale', 'regex', 'system']
+)
+rapidjson_dep = dependency('RapidJSON')
+redox_dep = dependency('redox')
+libzmq_dep = dependency('libzmq')
+loguru_dep = dependency('loguru')
+intervalset_dep = dependency('intervalset')
+gmpxx_dep = dependency('gmpxx')
+
+batsched_deps = [
+    boost_dep,
+    rapidjson_dep,
+    redox_dep,
+    libzmq_dep,
+    loguru_dep,
+    intervalset_dep,
+    gmpxx_dep
+]
+
+# Source files
+src = [
+    'src/algo/conservative_bf.cpp',
+    'src/algo/conservative_bf.hpp',
+    'src/algo/crasher.cpp',
+    'src/algo/crasher.hpp',
+    'src/algo/easy_bf.cpp',
+    'src/algo/easy_bf_fast.cpp',
+    'src/algo/easy_bf_fast.hpp',
+    'src/algo/easy_bf.hpp',
+    'src/algo/easy_bf_plot_liquid_load_horizon.cpp',
+    'src/algo/easy_bf_plot_liquid_load_horizon.hpp',
+    'src/algo/energy_bf.cpp',
+    'src/algo/energy_bf_dicho.cpp',
+    'src/algo/energy_bf_dicho.hpp',
+    'src/algo/energy_bf.hpp',
+    'src/algo/energy_bf_idle_sleeper.cpp',
+    'src/algo/energy_bf_idle_sleeper.hpp',
+    'src/algo/energy_bf_machine_subpart_sleeper.cpp',
+    'src/algo/energy_bf_machine_subpart_sleeper.hpp',
+    'src/algo/energy_bf_monitoring_inertial_shutdown.cpp',
+    'src/algo/energy_bf_monitoring_inertial_shutdown.hpp',
+    'src/algo/energy_bf_monitoring_period.cpp',
+    'src/algo/energy_bf_monitoring_period.hpp',
+    'src/algo/energy_watcher.cpp',
+    'src/algo/energy_watcher.hpp',
+    'src/algo/fcfs_fast.cpp',
+    'src/algo/fcfs_fast.hpp',
+    'src/algo/fcfs_fast2.cpp',
+    'src/algo/fcfs_fast2.hpp',
+    'src/algo/filler.cpp',
+    'src/algo/filler.hpp',
+    'src/algo/killer2.cpp',
+    'src/algo/killer2.hpp',
+    'src/algo/killer.cpp',
+    'src/algo/killer.hpp',
+    'src/algo/random.cpp',
+    'src/algo/random.hpp',
+    'src/algo/rejecter.cpp',
+    'src/algo/rejecter.hpp',
+    'src/algo/sequencer.cpp',
+    'src/algo/sequencer.hpp',
+    'src/algo/sleeper.cpp',
+    'src/algo/sleeper.hpp',
+    'src/algo/submitter.cpp',
+    'src/algo/submitter.hpp',
+    'src/algo/wt_estimator.cpp',
+    'src/algo/wt_estimator.hpp',
+    'src/data_storage.cpp',
+    'src/data_storage.hpp',
+    'src/decision.cpp',
+    'src/decision.hpp',
+    'src/exact_numbers.hpp',
+    'src/external/taywee_args.hpp',
+    'src/isalgorithm.cpp',
+    'src/isalgorithm.hpp',
+    'src/json_workload.cpp',
+    'src/json_workload.hpp',
+    'src/locality.cpp',
+    'src/locality.hpp',
+    'src/main.cpp',
+    'src/network.cpp',
+    'src/network.hpp',
+    'src/pempek_assert.cpp',
+    'src/pempek_assert.hpp',
+    'src/protocol.cpp',
+    'src/protocol.hpp',
+    'src/queue.cpp',
+    'src/queue.hpp',
+    'src/queueing_theory_waiting_time_estimator.cpp',
+    'src/queueing_theory_waiting_time_estimator.hpp',
+    'src/schedule.cpp',
+    'src/schedule.hpp',
+    'src/external/batsched_workload.hpp',
+    'src/external/batsched_workload.cpp',
+    'src/external/batsched_profile.hpp',
+    'src/external/batsched_profile.cpp',
+    'src/external/batsched_job.hpp',
+    'src/external/batsched_job.cpp'
+    
+    
+    
+]
+include_dir = include_directories('src')
+
+batsched = executable('batsched', src,
+    include_directories: include_dir,
+    dependencies: batsched_deps,
+    cpp_args: '-DBATSCHED_VERSION=@0@'.format(meson.project_version()),
+    install: true
+)
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/README.md /home/craig/LANL/Batsim/April_9/theres/batsched_edit/README.md
--- /home/craig/LANL/Batsim/April_9/theres/batsched/README.md	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/README.md	2021-04-02 05:01:51.000000000 -0400
@@ -1,7 +1,50 @@
 [![pipeline status](https://framagit.org/batsim/batsched/badges/master/pipeline.svg)](https://framagit.org/batsim/batsched/pipelines)
-[![coverage report](https://framagit.org/batsim/batsched/badges/master/coverage.svg)](http://batsim.gforge.inria.fr/batsched/coverage/)
+[![coverage](https://img.shields.io/codecov/c/github/oar-team/batsched.svg)](https://codecov.io/gh/oar-team/batsched)
 [![changelog](https://img.shields.io/badge/doc-changelog-blue.svg)](./CHANGELOG.md)
 
+# Instructions
+
+
+Go to GDrive -> Scheduler Simulator -> Craig -> Batsim Documentation -> Setting Up Batsim and Batsched
+
+Read README.txt and follow along
+
+## How to contribute
+
+once the repo is cloned to your local hard drive, create a new branch:
+```
+git checkout -b NEW FEATURE    # replace NEW FEATURE with your new branch name
+```
+... make changes to the code
+
+```
+git add .            #Don't forget the period at the end
+
+git commit -m "message describing changes"
+```
+
+... do a series of `git add .` and `git commit -m "msg"` and test the code on your computer
+
+... when you want to publish to our group:
+
+```
+git push origin NEW FEATURE               #where NEW FEATURE is the created branch name
+```
+
+
+## Contact
+
+slack - channel or direct message
+
+<cswalke1@coastal.edu>
+
+
+
+
+
+
+# Pre-fork instructions
+
 **batsched** is a set of [Batsim]-compatible algorithms implemented in C++.
 
 ## Install
@@ -23,16 +66,17 @@
 make install
 ```
 
-Up-to-date dependencies and versions are fully defined in [batsched's CI nix recipe](./default.nix).  
+Up-to-date dependencies and versions are fully defined in [batsched's CI nix recipe](./release.nix).  
 Here is a quick (and probably outdated) list:
 - decent clang/gcc and cmake
 - zmq (C and C++)
 - redox (hiredis + libev)
 - [loguru]
 - [intervalset]
-- decent boost, gmp, rapidjson, openssl...
+- decent boost, gmp, rapidjson...
 
 [Batsim]: https://framagit.org/batsim/batsim/
 [intervalset]: https://framagit.org/batsim/intervalset
 [loguru]: https://github.com/emilk/loguru
 [Nix]: https://nixos.org/nix/
+
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/README.md.orig /home/craig/LANL/Batsim/April_9/theres/batsched_edit/README.md.orig
--- /home/craig/LANL/Batsim/April_9/theres/batsched/README.md.orig	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/README.md.orig	2021-04-02 05:01:27.000000000 -0400
@@ -0,0 +1,43 @@
+[![pipeline status](https://framagit.org/batsim/batsched/badges/master/pipeline.svg)](https://framagit.org/batsim/batsched/pipelines)
+[![coverage](https://img.shields.io/codecov/c/github/oar-team/batsched.svg)](https://codecov.io/gh/oar-team/batsched)
+[![changelog](https://img.shields.io/badge/doc-changelog-blue.svg)](./CHANGELOG.md)
+
+**batsched** is a set of [Batsim]-compatible algorithms implemented in C++.
+<<<<<<< HEAD
+
+this text
+=======
+hello
+>>>>>>> My-Edits
+## Install
+### For [Nix] users
+``` bash
+# Up-to-date version
+nix-env -iA batsched_dev -f 'https://github.com/oar-team/kapack/archive/master.tar.gz'
+# Latest release
+nix-env -iA batsched -f 'https://github.com/oar-team/kapack/archive/master.tar.gz'
+```
+
+### Manually
+``` bash
+git clone https://framagit.org/batsim/batsched.git
+mkdir -p batsched/build
+cd batsched/build
+cmake ..
+make
+make install
+```
+
+Up-to-date dependencies and versions are fully defined in [batsched's CI nix recipe](./release.nix).  
+Here is a quick (and probably outdated) list:
+- decent clang/gcc and cmake
+- zmq (C and C++)
+- redox (hiredis + libev)
+- [loguru]
+- [intervalset]
+- decent boost, gmp, rapidjson...
+
+[Batsim]: https://framagit.org/batsim/batsim/
+[intervalset]: https://framagit.org/batsim/intervalset
+[loguru]: https://github.com/emilk/loguru
+[Nix]: https://nixos.org/nix/
diff -Naur '--exclude=.git' "/home/craig/LANL/Batsim/April_9/theres/batsched/release (2).nix" "/home/craig/LANL/Batsim/April_9/theres/batsched_edit/release (2).nix"
--- "/home/craig/LANL/Batsim/April_9/theres/batsched/release (2).nix"	1969-12-31 19:00:00.000000000 -0500
+++ "/home/craig/LANL/Batsim/April_9/theres/batsched_edit/release (2).nix"	2021-04-02 05:00:45.000000000 -0400
@@ -0,0 +1,148 @@
+{ kapack ? import
+    (fetchTarball "https://github.com/oar-team/kapack/archive/master.tar.gz")
+  {}
+, doCheck ? false
+, doCoverage ? true
+, batsim ? kapack.batsim
+, batsim_dev ? kapack.batsim_dev
+, batexpe ? kapack.batexpe
+}:
+
+let
+  pkgs = kapack.pkgs;
+  pythonPackages = pkgs.python37Packages;
+  buildPythonPackage = pythonPackages.buildPythonPackage;
+  optionOnOff = option: "${if option then "on" else "off"}";
+
+  jobs = rec {
+    # Batsched executable file (built from local sources)
+    batsched = kapack.batsched.overrideAttrs (attr: rec {
+      src = pkgs.lib.sourceByRegex ./. [
+        "^src"
+        "^src/.*\.?pp"
+        "^src/algo"
+        "^src/algo/.*\.?pp"
+        "^src/external"
+        "^src/external/.*\.?pp"
+        "^meson\.build"
+      ];
+      mesonFlags = []
+        ++ pkgs.lib.optional doCoverage [ "-Db_coverage=true" ];
+      nativeBuildInputs = with kapack; [pkgs.meson pkgs.ninja pkgs.pkgconfig
+        pkgs.boost gmp rapidjson intervalset loguru redox pkgs.cppzmq pkgs.zeromq];
+      # Debug build, without any Nix stripping magic.
+      mesonBuildType = "debug";
+      hardeningDisable = [ "all" ];
+      dontStrip = true;
+      # Keep files generated by GCOV, so depending jobs can use them.
+      postInstall = pkgs.lib.optionalString doCoverage ''
+        mkdir -p $out/gcno
+        cp batsched@exe/*.gcno $out/gcno/
+      '';
+    });
+
+    # Batsched integration tests.
+    integration_tests = pkgs.stdenv.mkDerivation rec {
+      pname = "batsched-integration-tests";
+      version = toString builtins.currentTime; # Forces rebuild
+      src = pkgs.lib.sourceByRegex ./. [
+        "^test"
+        "^test/.*\.py"
+        "^test/platforms"
+        "^test/platforms/.*\.xml"
+        "^test/workloads"
+        "^test/workloads/.*\.json"
+      ];
+      buildInputs = with pkgs.python37Packages; [
+        batsim batsched batexpe pkgs.redis
+        pytest pytest_html pandas];
+      preBuild = pkgs.lib.optionalString doCoverage ''
+        mkdir -p gcda
+        export GCOV_PREFIX=$(realpath gcda)
+        export GCOV_PREFIX_STRIP=5
+      '';
+      buildPhase = ''
+        runHook preBuild
+        set +e
+        (cd test && pytest -ra --html=../report/pytest_report.html)
+        echo $? > ./pytest_returncode
+        set -e
+      '';
+      checkPhase = ''
+        pytest_return_code=$(cat ./pytest_returncode)
+        echo "pytest return code: $pytest_return_code"
+        if [ $pytest_return_code -ne 0 ] ; then
+          exit 1
+        fi
+      '';
+      inherit doCheck;
+      installPhase = ''
+        mkdir -p $out
+        mv ./report/* ./pytest_returncode $out/
+      '' + pkgs.lib.optionalString doCoverage ''
+        mv ./gcda $out/
+      '';
+    };
+    # Essentially the same as integration_tests, but with an up-to-date Batsim.
+    integration_tests_batlatest = integration_tests.overrideAttrs (attr: rec {
+      buildInputs = with pkgs.python37Packages; [
+        batsim_dev batsched batexpe pkgs.redis
+        pytest pytest_html pandas];
+    });
+
+    # Batsched doxygen documentation.
+    doxydoc = pkgs.stdenv.mkDerivation rec {
+      name = "batsim-doxygen-documentation";
+      src = pkgs.lib.sourceByRegex ./. [
+        "^src"
+        "^src/.*\.?pp"
+        "^doc"
+        "^doc/Doxyfile"
+        "^doc/doxygen_mainpage.md"
+      ];
+      buildInputs = [pkgs.doxygen];
+      buildPhase = "(cd doc && doxygen)";
+      installPhase = ''
+        mkdir -p $out
+        mv doc/doxygen_doc/html/* $out/
+      '';
+      checkPhase = ''
+        nb_warnings=$(cat doc/doxygen_warnings.log | wc -l)
+        if [[ $nb_warnings -gt 0 ]] ; then
+          echo "FAILURE: There are doxygen warnings!"
+          cat doc/doxygen_warnings.log
+          exit 1
+        fi
+      '';
+      doCheck = true;
+    };
+
+    # The following packages are not in Nixpkgs so they are defined here.
+    pytest_metadata = buildPythonPackage {
+      name = "pytest-metadata-1.8.0";
+      doCheck = false;
+      propagatedBuildInputs = [
+        pythonPackages.pytest
+        pythonPackages.setuptools_scm
+      ];
+      src = builtins.fetchurl {
+        url = "https://files.pythonhosted.org/packages/12/38/eed3a1e00c765e4da61e4e833de41c3458cef5d18e819d09f0f160682993/pytest-metadata-1.8.0.tar.gz";
+        sha256 = "1fk6icip2x1nh4kzhbc8cnqrs77avpqvj7ny3xadfh6yhn9aaw90";
+      };
+    };
+
+    pytest_html = buildPythonPackage {
+      name = "pytest-html-1.20.0";
+      doCheck = false;
+      propagatedBuildInputs = [
+        pythonPackages.pytest
+        pytest_metadata
+      ];
+      src = builtins.fetchurl {
+        url = "https://files.pythonhosted.org/packages/08/3e/63d998f26c7846d3dac6da152d1b93db3670538c5e2fe18b88690c1f52a7/pytest-html-1.20.0.tar.gz";
+        sha256 = "17jyn4czkihrs225nkpj0h113hc03y0cl07myb70jkaykpfmrim7";
+      };
+    };
+  };
+in
+  jobs
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/release.nix /home/craig/LANL/Batsim/April_9/theres/batsched_edit/release.nix
--- /home/craig/LANL/Batsim/April_9/theres/batsched/release.nix	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/release.nix	2021-04-02 05:01:27.000000000 -0400
@@ -0,0 +1,148 @@
+{ kapack ? import
+    (fetchTarball "https://github.com/oar-team/kapack/archive/master.tar.gz")
+  {}
+, doCheck ? false
+, doCoverage ? true
+, batsim ? kapack.batsim
+, batsim_dev ? kapack.batsim_dev
+, batexpe ? kapack.batexpe
+}:
+
+let
+  pkgs = kapack.pkgs;
+  pythonPackages = pkgs.python37Packages;
+  buildPythonPackage = pythonPackages.buildPythonPackage;
+  optionOnOff = option: "${if option then "on" else "off"}";
+
+  jobs = rec {
+    # Batsched executable file (built from local sources)
+    batsched = kapack.batsched.overrideAttrs (attr: rec {
+      src = pkgs.lib.sourceByRegex ./. [
+        "^src"
+        "^src/.*\.?pp"
+        "^src/algo"
+        "^src/algo/.*\.?pp"
+        "^src/external"
+        "^src/external/.*\.?pp"
+        "^meson\.build"
+      ];
+      mesonFlags = []
+        ++ pkgs.lib.optional doCoverage [ "-Db_coverage=true" ];
+      nativeBuildInputs = with kapack; [pkgs.meson pkgs.ninja pkgs.pkgconfig
+        pkgs.boost gmp rapidjson intervalset loguru redox pkgs.cppzmq pkgs.zeromq];
+      # Debug build, without any Nix stripping magic.
+      mesonBuildType = "debug";
+      hardeningDisable = [ "all" ];
+      dontStrip = true;
+      # Keep files generated by GCOV, so depending jobs can use them.
+      postInstall = pkgs.lib.optionalString doCoverage ''
+        mkdir -p $out/gcno
+        cp batsched@exe/*.gcno $out/gcno/
+      '';
+    });
+
+    # Batsched integration tests.
+    integration_tests = pkgs.stdenv.mkDerivation rec {
+      pname = "batsched-integration-tests";
+      version = toString builtins.currentTime; # Forces rebuild
+      src = pkgs.lib.sourceByRegex ./. [
+        "^test"
+        "^test/.*\.py"
+        "^test/platforms"
+        "^test/platforms/.*\.xml"
+        "^test/workloads"
+        "^test/workloads/.*\.json"
+      ];
+      buildInputs = with pkgs.python37Packages; [
+        batsim batsched batexpe pkgs.redis
+        pytest pytest_html pandas];
+      preBuild = pkgs.lib.optionalString doCoverage ''
+        mkdir -p gcda
+        export GCOV_PREFIX=$(realpath gcda)
+        export GCOV_PREFIX_STRIP=5
+      '';
+      buildPhase = ''
+        runHook preBuild
+        set +e
+        (cd test && pytest -ra --html=../report/pytest_report.html)
+        echo $? > ./pytest_returncode
+        set -e
+      '';
+      checkPhase = ''
+        pytest_return_code=$(cat ./pytest_returncode)
+        echo "pytest return code: $pytest_return_code"
+        if [ $pytest_return_code -ne 0 ] ; then
+          exit 1
+        fi
+      '';
+      inherit doCheck;
+      installPhase = ''
+        mkdir -p $out
+        mv ./report/* ./pytest_returncode $out/
+      '' + pkgs.lib.optionalString doCoverage ''
+        mv ./gcda $out/
+      '';
+    };
+    # Essentially the same as integration_tests, but with an up-to-date Batsim.
+    integration_tests_batlatest = integration_tests.overrideAttrs (attr: rec {
+      buildInputs = with pkgs.python37Packages; [
+        batsim_dev batsched batexpe pkgs.redis
+        pytest pytest_html pandas];
+    });
+
+    # Batsched doxygen documentation.
+    doxydoc = pkgs.stdenv.mkDerivation rec {
+      name = "batsim-doxygen-documentation";
+      src = pkgs.lib.sourceByRegex ./. [
+        "^src"
+        "^src/.*\.?pp"
+        "^doc"
+        "^doc/Doxyfile"
+        "^doc/doxygen_mainpage.md"
+      ];
+      buildInputs = [pkgs.doxygen];
+      buildPhase = "(cd doc && doxygen)";
+      installPhase = ''
+        mkdir -p $out
+        mv doc/doxygen_doc/html/* $out/
+      '';
+      checkPhase = ''
+        nb_warnings=$(cat doc/doxygen_warnings.log | wc -l)
+        if [[ $nb_warnings -gt 0 ]] ; then
+          echo "FAILURE: There are doxygen warnings!"
+          cat doc/doxygen_warnings.log
+          exit 1
+        fi
+      '';
+      doCheck = true;
+    };
+
+    # The following packages are not in Nixpkgs so they are defined here.
+    pytest_metadata = buildPythonPackage {
+      name = "pytest-metadata-1.8.0";
+      doCheck = false;
+      propagatedBuildInputs = [
+        pythonPackages.pytest
+        pythonPackages.setuptools_scm
+      ];
+      src = builtins.fetchurl {
+        url = "https://files.pythonhosted.org/packages/12/38/eed3a1e00c765e4da61e4e833de41c3458cef5d18e819d09f0f160682993/pytest-metadata-1.8.0.tar.gz";
+        sha256 = "1fk6icip2x1nh4kzhbc8cnqrs77avpqvj7ny3xadfh6yhn9aaw90";
+      };
+    };
+
+    pytest_html = buildPythonPackage {
+      name = "pytest-html-1.20.0";
+      doCheck = false;
+      propagatedBuildInputs = [
+        pythonPackages.pytest
+        pytest_metadata
+      ];
+      src = builtins.fetchurl {
+        url = "https://files.pythonhosted.org/packages/08/3e/63d998f26c7846d3dac6da152d1b93db3670538c5e2fe18b88690c1f52a7/pytest-html-1.20.0.tar.gz";
+        sha256 = "17jyn4czkihrs225nkpj0h113hc03y0cl07myb70jkaykpfmrim7";
+      };
+    };
+  };
+in
+  jobs
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/conservative_bf.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/conservative_bf.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/conservative_bf.cpp	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/conservative_bf.cpp	2021-04-02 05:00:30.000000000 -0400
@@ -1,11 +1,28 @@
 #include "conservative_bf.hpp"
 
+#include <loguru.hpp>
+
+#include "../pempek_assert.hpp"
+
 using namespace std;
 
 ConservativeBackfilling::ConservativeBackfilling(Workload *workload, SchedulingDecision *decision,
                                                  Queue *queue, ResourceSelector * selector, double rjms_delay, rapidjson::Document *variant_options) :
     ISchedulingAlgorithm(workload, decision, queue, selector, rjms_delay, variant_options)
 {
+    if (variant_options->HasMember("dump_previsional_schedules"))
+    {
+        PPK_ASSERT_ERROR((*variant_options)["dump_previsional_schedules"].IsBool(),
+                "Invalid options: 'dump_previsional_schedules' should be a boolean");
+        _dump_provisional_schedules = (*variant_options)["dump_previsional_schedules"].GetBool();
+    }
+
+    if (variant_options->HasMember("dump_prefix"))
+    {
+        PPK_ASSERT_ERROR((*variant_options)["dump_prefix"].IsString(),
+                "Invalid options: 'dump_prefix' should be a string");
+        _dump_prefix = (*variant_options)["dump_prefix"].GetString();
+    }
 }
 
 ConservativeBackfilling::~ConservativeBackfilling()
@@ -32,14 +49,26 @@
         _schedule.remove_job((*_workload)[ended_job_id]);
 
     // Let's handle recently released jobs
+    std::vector<std::string> recently_queued_jobs;
     for (const string & new_job_id : _jobs_released_recently)
     {
         const Job * new_job = (*_workload)[new_job_id];
 
         if (new_job->nb_requested_resources > _nb_machines)
+        {
             _decision->add_reject_job(new_job_id, date);
+        }
+        else if (!new_job->has_walltime)
+        {
+            LOG_SCOPE_FUNCTION(INFO);
+            LOG_F(INFO, "Date=%g. Rejecting job '%s' as it has no walltime", date, new_job_id.c_str());
+            _decision->add_reject_job(new_job_id, date);
+        }
         else
+        {
             _queue->append_job(new_job, update_info);
+            recently_queued_jobs.push_back(new_job_id);
+        }
     }
 
     // Let's update the schedule's present
@@ -51,7 +80,7 @@
     // If no resources have been released, we can just insert the new jobs into the schedule
     if (_jobs_ended_recently.empty())
     {
-        for (const string & new_job_id : _jobs_released_recently)
+        for (const string & new_job_id : recently_queued_jobs)
         {
             const Job * new_job = (*_workload)[new_job_id];
             Schedule::JobAlloc alloc = _schedule.add_job_first_fit(new_job, _selector);
@@ -78,7 +107,11 @@
             const Job * job = (*job_it)->job;
 
             _schedule.remove_job_if_exists(job);
+//            if (_dump_provisional_schedules)
+//                _schedule.incremental_dump_as_batsim_jobs_file(_dump_prefix);
             Schedule::JobAlloc alloc = _schedule.add_job_first_fit(job, _selector);
+//            if (_dump_provisional_schedules)
+//                _schedule.incremental_dump_as_batsim_jobs_file(_dump_prefix);
 
             if (alloc.started_in_first_slice)
             {
@@ -94,8 +127,11 @@
     
     for (const std::string & job_id : _jobs_whose_waiting_time_estimation_has_been_requested_recently)
     {
-	const Job * new_job = (*_workload)[job_id];
-	double answer = _schedule.query_wait(new_job->nb_requested_resources, new_job->walltime, _selector);
-        _decision->add_answer_estimate_waiting_time(job_id, answer, date);
+        const Job * new_job = (*_workload)[job_id];
+        double answer = _schedule.query_wait(new_job->nb_requested_resources, new_job->walltime, _selector);
+            _decision->add_answer_estimate_waiting_time(job_id, answer, date);
     }
+
+    if (_dump_provisional_schedules)
+        _schedule.incremental_dump_as_batsim_jobs_file(_dump_prefix);
 }
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/conservative_bf.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/conservative_bf.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/conservative_bf.hpp	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/conservative_bf.hpp	2021-04-02 05:00:39.000000000 -0400
@@ -24,4 +24,6 @@
 
 private:
     Schedule _schedule;
+    bool _dump_provisional_schedules = false;
+    std::string _dump_prefix = "/tmp/dump";
 };
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/easy_bf.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/easy_bf.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/easy_bf.cpp	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/easy_bf.cpp	2021-04-02 05:00:40.000000000 -0400
@@ -43,14 +43,26 @@
         _schedule.remove_job((*_workload)[ended_job_id]);
 
     // Let's handle recently released jobs
+    std::vector<std::string> recently_queued_jobs;
     for (const string & new_job_id : _jobs_released_recently)
     {
         const Job * new_job = (*_workload)[new_job_id];
 
         if (new_job->nb_requested_resources > _nb_machines)
+        {
+            _decision->add_reject_job(new_job_id, date);
+        }
+        else if (!new_job->has_walltime)
+        {
+            LOG_SCOPE_FUNCTION(INFO);
+            LOG_F(INFO, "Date=%g. Rejecting job '%s' as it has no walltime", new_job_id.c_str());
             _decision->add_reject_job(new_job_id, date);
+        }
         else
+        {
             _queue->append_job(new_job, update_info);
+            recently_queued_jobs.push_back(new_job_id);
+        }
     }
 
     // Let's update the schedule's present
@@ -65,9 +77,9 @@
     {
         int nb_available_machines = _schedule.begin()->available_machines.size();
 
-        for (unsigned int i = 0; i < _jobs_released_recently.size() && nb_available_machines > 0; ++i)
+        for (unsigned int i = 0; i < recently_queued_jobs.size() && nb_available_machines > 0; ++i)
         {
-            const string & new_job_id = _jobs_released_recently[i];
+            const string & new_job_id = recently_queued_jobs[i];
             const Job * new_job = (*_workload)[new_job_id];
 
             // The job could have already been executed by sort_queue_while_handling_priority_job,
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/easy_bf_fast.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/easy_bf_fast.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/easy_bf_fast.cpp	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/easy_bf_fast.cpp	2021-04-02 05:00:24.000000000 -0400
@@ -1,5 +1,7 @@
 #include "easy_bf_fast.hpp"
 
+//#include <loguru.hpp>
+
 #include "../pempek_assert.hpp"
 
 EasyBackfillingFast::EasyBackfillingFast(Workload *workload,
@@ -72,15 +74,17 @@
     {
         if (_priority_job != nullptr)
         {
+            Allocation alloc;
+            FinishedHorizonPoint point;
+
             if (_priority_job->nb_requested_resources <= _nb_available_machines)
             {
-                Allocation alloc;
+                //LOG_F(INFO, "Priority job fits!");
                 alloc.machines = _available_machines.left(
                     _priority_job->nb_requested_resources);
                 _decision->add_execute_job(_priority_job->id, alloc.machines,
                     date);
 
-                FinishedHorizonPoint point;
                 point.nb_released_machines = _priority_job->nb_requested_resources;
                 point.date = date + (double)_priority_job->walltime;
                 alloc.horizon_it = insert_horizon_point(point);
@@ -124,14 +128,16 @@
                         break;
                     }
                 }
+            }
 
-                // Continue traversal, backfilling jobs that does not hinder
-                // priority job.
+            // Backfill jobs that does not hinder priority job.
+            if (_nb_available_machines > 0)
+            {
                 for (auto job_it = _pending_jobs.begin();
                      job_it != _pending_jobs.end(); )
                 {
                     const Job * pending_job = *job_it;
-                    // Does the job can be executed now ?
+                    // Can the job be executed now ?
                     if (pending_job->nb_requested_resources <= _nb_available_machines &&
                         date + pending_job->walltime <= _priority_job->completion_time)
                     {
@@ -150,6 +156,10 @@
                         _nb_available_machines -= pending_job->nb_requested_resources;
                         _current_allocations[pending_job->id] = alloc;
                         job_it = _pending_jobs.erase(job_it);
+
+                        // Directly get out of the backfilling loop if all machines are busy.
+                        if (_nb_available_machines <= 0)
+                            break;
                     }
                     else
                     {
@@ -168,10 +178,12 @@
         // Can the job be executed right now?
         if (new_job->nb_requested_resources <= _nb_available_machines)
         {
+            //LOG_F(INFO, "There are enough available resources (%d) to execute job %s", _nb_available_machines, new_job->id.c_str());
             // Can it be executed now (without hindering priority job?)
             if (_priority_job == nullptr ||
                 date + new_job->walltime <= _priority_job->completion_time)
             {
+                //LOG_F(INFO, "Job %s can be started right away!", new_job->id.c_str());
                 // Yes, the job can be executed right away!
                 Allocation alloc;
 
@@ -192,6 +204,8 @@
             else
             {
                 // No, the job cannot be executed (hinders priority job.)
+                /*LOG_F(INFO, "Not enough time to execute job %s (walltime=%g, priority job expected starting time=%g)",
+                      new_job->id.c_str(), (double)new_job->walltime, _priority_job->completion_time);*/
                 _pending_jobs.push_back(new_job);
             }
         }
@@ -202,6 +216,8 @@
             // Is the job valid on this platform?
             if (new_job->nb_requested_resources > _nb_machines)
             {
+                /*LOG_F(INFO, "Rejecing job %s (required %d machines, while platform size is %d)",
+                      new_job->id.c_str(), new_job->nb_requested_resources, _nb_machines);*/
                 _decision->add_reject_job(new_job_id, date);
             }
             else
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/energy_bf.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/energy_bf.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/energy_bf.cpp	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/energy_bf.cpp	2021-04-02 05:00:28.000000000 -0400
@@ -1343,3 +1343,31 @@
 {
     return boost::starts_with(job_id, "fakejob_");
 }
+
+bool EnergyBackfilling::contains_any_fake_job(const Schedule &schedule)
+{
+    for (auto slice_it = schedule.begin(); slice_it != schedule.end(); ++slice_it)
+    {
+        for (auto mit : slice_it->allocated_jobs)
+        {
+            const Job * job = mit.first;
+            if (is_fake_job(job->id))
+                return true;
+        }
+    }
+    return false;
+}
+
+bool EnergyBackfilling::contains_any_nonfake_job(const Schedule &schedule)
+{
+    for (auto slice_it = schedule.begin(); slice_it != schedule.end(); ++slice_it)
+    {
+        for (auto mit : slice_it->allocated_jobs)
+        {
+            const Job * job = mit.first;
+            if (!is_fake_job(job->id))
+                return true;
+        }
+    }
+    return false;
+}
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/energy_bf.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/energy_bf.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/energy_bf.hpp	2021-04-04 01:01:41.809519327 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/energy_bf.hpp	2021-04-02 05:00:38.000000000 -0400
@@ -156,6 +156,9 @@
     static bool is_potential_sleep_job(const std::string & job_id);
     static bool is_fake_job(const std::string & job_id);
 
+    static bool contains_any_fake_job(const Schedule & schedule);
+    static bool contains_any_nonfake_job(const Schedule & schedule);
+
 protected:
     Schedule _schedule;
     bool _debug = false;
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/energy_bf_monitoring_inertial_shutdown.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/energy_bf_monitoring_inertial_shutdown.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/energy_bf_monitoring_inertial_shutdown.cpp	2021-04-04 01:01:41.813519288 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/energy_bf_monitoring_inertial_shutdown.cpp	2021-04-02 05:00:29.000000000 -0400
@@ -120,26 +120,34 @@
                      "Invalid nb_machines_sedated_for_being_idle value: %d\n",
                      _nb_machines_sedated_for_being_idle);
 
-    // Let's remove finished jobs from the schedule
-    for (const string & ended_job_id : _jobs_ended_recently)
+    if (!_jobs_ended_recently.empty())
     {
-        const Job * ended_job = (*_workload)[ended_job_id];
-        ++_nb_jobs_completed;
+        // Let's remove finished jobs from the schedule
+        for (const string & ended_job_id : _jobs_ended_recently)
+        {
+            const Job * ended_job = (*_workload)[ended_job_id];
+            ++_nb_jobs_completed;
 
-        PPK_ASSERT_ERROR(_schedule.contains_job(ended_job),
-                         "Invalid schedule: job '%s' just finished, "
-                         "but it not in the schedule...\n%s",
-                         ended_job_id.c_str(), _schedule.to_string().c_str());
-        PPK_ASSERT_ERROR(!_queue->contains_job(ended_job),
-                         "Job '%s' just ended, but it is still in the "
-                         "queue...\nQueue : %s",
-                         ended_job_id.c_str(),
-                         _queue->to_string().c_str());
+            PPK_ASSERT_ERROR(_schedule.contains_job(ended_job),
+                             "Invalid schedule: job '%s' just finished, "
+                             "but it not in the schedule...\n%s",
+                             ended_job_id.c_str(), _schedule.to_string().c_str());
+            PPK_ASSERT_ERROR(!_queue->contains_job(ended_job),
+                             "Job '%s' just ended, but it is still in the "
+                             "queue...\nQueue : %s",
+                             ended_job_id.c_str(),
+                             _queue->to_string().c_str());
 
-        // Let's remove the finished job from the schedule
-        _schedule.remove_job(ended_job);
-    }
+            // Let's remove the finished job from the schedule
+            _schedule.remove_job(ended_job);
+        }
 
+        // Stop sending CALL_ME_LATER if all jobs have been executed.
+        if (_no_more_static_job_to_submit_received &&
+            _queue->is_empty() &&
+            !EnergyBackfilling::contains_any_nonfake_job(_schedule))
+            _stop_sending_call_me_later = true;
+    }
 
     // Let's update the first slice of the schedule
     update_first_slice_taking_sleep_jobs_into_account(date);
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/energy_bf_monitoring_period.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/energy_bf_monitoring_period.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/energy_bf_monitoring_period.cpp	2021-04-04 01:01:41.813519288 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/energy_bf_monitoring_period.cpp	2021-04-02 05:00:37.000000000 -0400
@@ -65,13 +65,16 @@
         // Let's execute on_monitoring_stage
         on_monitoring_stage(date);
 
-        // Let's request a call for the next monitoring stage
-        _next_monitoring_period_expected_date = date + _period_between_monitoring_stages;
-        _decision->add_call_me_later((double)(_next_monitoring_period_expected_date), date);
-        _nb_call_me_later_running++;
+        if (!_stop_sending_call_me_later)
+        {
+            // Let's request a call for the next monitoring stage
+            _next_monitoring_period_expected_date = date + _period_between_monitoring_stages;
+            _decision->add_call_me_later((double)(_next_monitoring_period_expected_date), date);
+            _nb_call_me_later_running++;
 
-        LOG_F(INFO, "EnergyBackfillingMonitoringPeriod: 'Chose to launch a call_me_later at %g",
-               (double)_next_monitoring_period_expected_date);
+            LOG_F(INFO, "EnergyBackfillingMonitoringPeriod: 'Chose to launch a call_me_later at %g",
+                   (double)_next_monitoring_period_expected_date);
+        }
     }
 }
 
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/energy_bf_monitoring_period.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/energy_bf_monitoring_period.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/energy_bf_monitoring_period.hpp	2021-04-04 01:01:41.813519288 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/energy_bf_monitoring_period.hpp	2021-04-02 05:00:37.000000000 -0400
@@ -29,6 +29,7 @@
 
 protected:
     std::string _output_dir;
+    bool _stop_sending_call_me_later = false;
 
 private:
     bool _monitoring_period_launched = false;
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/fcfs_fast2.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/fcfs_fast2.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/fcfs_fast2.cpp	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/fcfs_fast2.cpp	2021-04-02 05:00:34.000000000 -0400
@@ -0,0 +1,561 @@
+#include <math.h>
+#include "fcfs_fast2.hpp"
+
+#include "../pempek_assert.hpp"
+
+#include <loguru.hpp>
+#include "../external/batsched_workload.hpp"
+#include "../external/batsched_job.hpp"
+#include "../external/batsched_profile.hpp"
+#include "../external/pointers.hpp"
+#include <rapidjson/document.h>
+#include <rapidjson/writer.h>
+#include <chrono>
+
+namespace myB = myBatsched;
+namespace r = rapidjson;
+
+FCFSFast2::FCFSFast2(Workload *workload,
+    SchedulingDecision *decision, Queue *queue, ResourceSelector *selector,
+    double rjms_delay, rapidjson::Document *variant_options) :
+    ISchedulingAlgorithm(workload, decision, queue, selector, rjms_delay,
+        variant_options)
+{
+    LOG_F(INFO,"created 4");
+    _myWorkloads = new myBatsched::Workloads;
+    
+}
+
+FCFSFast2::~FCFSFast2()
+{
+    
+}
+
+void FCFSFast2::on_simulation_start(double date,
+    const rapidjson::Value &batsim_config)
+{
+    bool seedFailures = false;
+    if (batsim_config.HasMember("seed-failures"))
+        seedFailures = batsim_config["seed-failures"].GetBool();
+    unsigned seed = 0;
+    if (seedFailures)
+        seed = std::chrono::system_clock::now().time_since_epoch().count();
+         
+    generator.seed(seed);
+    generator2.seed(seed);
+    _available_machines.insert(IntervalSet::ClosedInterval(0, _nb_machines - 1));
+    _nb_available_machines = _nb_machines;
+    PPK_ASSERT_ERROR(_available_machines.size() == (unsigned int) _nb_machines);
+    _oldDate=date;
+
+    if (_myWorkloads->_SMTBF!=-1.0)
+    {
+        distribution = new std::exponential_distribution<double>(1.0/_myWorkloads->_SMTBF);
+        unif_distribution = new std::uniform_int_distribution<int>(0,_nb_machines-1);
+        std::exponential_distribution<double>::param_type new_lambda(1.0/_myWorkloads->_SMTBF);
+        distribution->param(new_lambda);
+        double number;         
+        number = distribution->operator()(generator);
+        _decision->add_call_me_later(number+date,date);
+    }
+    else if (_myWorkloads->_MTBF!=-1.0)
+    {
+        distribution = new std::exponential_distribution<double>(1.0/_myWorkloads->_MTBF);
+        std::exponential_distribution<double>::param_type new_lambda(1.0/_myWorkloads->_MTBF);
+        distribution->param(new_lambda);
+        double number;         
+        number = distribution->operator()(generator);
+        _decision->add_call_me_later(number+date,date);
+    }
+        
+}
+
+void FCFSFast2::on_simulation_end(double date)
+{
+    (void) date;
+}
+void FCFSFast2::on_machine_unavailable_notify_event(double date, IntervalSet machines){
+    LOG_F(INFO,"unavailable %s",machines.to_string_hyphen().c_str());
+    _unavailable_machines+=machines;
+    _available_machines-=machines;
+    for(auto key_value : _current_allocations)
+    {
+            if (!((key_value.second & machines).is_empty()))
+                _decision->add_kill_job({key_value.first},date);
+    }
+    
+}
+void FCFSFast2::set_workloads(myBatsched::Workloads *w){
+    _myWorkloads = w;
+    _checkpointing_on = w->_checkpointing_on;
+    
+}
+void FCFSFast2::on_machine_available_notify_event(double date, IntervalSet machines){
+    ISchedulingAlgorithm::on_machine_available_notify_event(date, machines);
+    _unavailable_machines-=machines;
+    _available_machines+=machines;
+    _nb_available_machines+=machines.size();
+    
+}
+
+void FCFSFast2::on_machine_state_changed(double date, IntervalSet machines, int new_state)
+{
+   
+
+}
+void FCFSFast2::on_myKillJob_notify_event(double date){
+    
+    if (!_running_jobs.empty()){
+        _my_kill_jobs.insert(*_running_jobs.begin());
+    }
+        
+    
+}
+void FCFSFast2::on_machine_instant_down_up(double date){
+    //get a random number of a machine to kill
+    int number = unif_distribution->operator()(generator2);
+    //make it an intervalset so we can find the intersection of it with current allocations
+    IntervalSet machine = number;
+    //if there are no running jobs, then there are none to kill
+    if (!_running_jobs.empty()){
+        for(auto key_value : _current_allocations)
+        {
+            if (!((key_value.second & machine).is_empty()))
+                _my_kill_jobs.insert(key_value.first);
+        }
+    }
+}
+void FCFSFast2::on_job_fault_notify_event(double date, std::string job){
+    std::unordered_set<std::string>::const_iterator found = _running_jobs.find(job);
+  LOG_F(INFO,"on_job_fault_notify_event called");
+  if ( found != _running_jobs.end() )    
+        _decision->add_kill_job({job},date);
+  else
+      LOG_F(INFO,"Job %s was not running but was supposed to be killed due to job_fault event",job.c_str());
+}
+
+void FCFSFast2::on_requested_call(double date)
+{
+  if (_myWorkloads->_SMTBF!=-1.0)
+  {
+      if (!_running_jobs.empty() || !_pending_jobs.empty() || !_no_more_static_job_to_submit_received)
+        {
+            double number = distribution->operator()(generator);
+            on_machine_instant_down_up(date);
+            _decision->add_call_me_later(number+date,date);
+        }
+  }
+  else if (_myWorkloads->_MTBF!=-1.0)    
+    {
+        if (!_running_jobs.empty() || !_pending_jobs.empty() || !_no_more_static_job_to_submit_received)
+        {
+            double number = distribution->operator()(generator);
+            on_myKillJob_notify_event(date);
+            _decision->add_call_me_later(number+date,date);
+        }
+       
+        
+    }
+}
+void FCFSFast2::on_no_more_static_job_to_submit_received(double date){
+    ISchedulingAlgorithm::on_no_more_static_job_to_submit_received(date);
+
+}
+void FCFSFast2::on_no_more_external_event_to_occur(double date){
+    
+    _wrap_it_up = true;    
+    
+    
+}
+void FCFSFast2::on_job_end(double date, std::vector<std::string> job_ids)
+{
+    (void) date;
+    (void) job_ids;
+}
+
+void FCFSFast2::make_decisions(double date,
+    SortableJobOrder::UpdateInformation *update_info,
+    SortableJobOrder::CompareInformation *compare_info)
+{
+    (void) update_info;
+    (void) compare_info;
+    
+    // This algorithm is a fast version of FCFS without backfilling.
+    // It is meant to be fast in the usual case, not to handle corner cases.
+    // It is not meant to be easily readable or hackable ;).
+
+    // This fast FCFS variant in a few words:
+    // - only handles the FCFS queue order
+    // - only handles the basic resource selection policy
+    // - only handles finite jobs (no switchoff)
+    // - only handles time as floating-point (-> precision errors).
+    
+        
+    
+    bool job_ended = false;
+    LOG_F(INFO,"DEBUG GANTT, avail machines %d",_nb_available_machines);
+    // Handle newly finished jobs
+    for (const std::string & ended_job_id : _jobs_ended_recently)
+    {
+        job_ended = true;
+        LOG_F(INFO,"job ended: %s %f",ended_job_id.c_str(),date);
+        
+        // Update data structures
+        
+        _available_machines.insert(_current_allocations[ended_job_id]-_unavailable_machines);
+        _nb_available_machines += (_current_allocations[ended_job_id]-_unavailable_machines).size();
+        _current_allocations.erase(ended_job_id);
+        _running_jobs.erase(ended_job_id);
+        _my_kill_jobs.erase(ended_job_id);
+    }
+    
+    //Handle new jobs to kill
+   
+    if(!_my_kill_jobs.empty()){
+         std::vector<std::string> kills;
+        for( std::string kill:_my_kill_jobs)
+            kills.push_back(kill);
+        _decision->add_kill_job(kills,date);
+        _my_kill_jobs.clear();
+    }
+    //************************************************************resubmission if killed
+    //Handle jobs to queue back up (if killed)  
+    handle_resubmission(date);    
+    //*************************************************************
+    
+    
+    
+    
+    
+    if (!(_machines_that_became_available_recently.is_empty()))
+    {
+        for (auto job_it = _pending_jobs.begin();
+             job_it != _pending_jobs.end(); )
+        {
+            Job * pending_job = *job_it;
+            if (pending_job->nb_requested_resources <= _nb_available_machines)
+            {
+                IntervalSet machines = _available_machines.left(
+                    pending_job->nb_requested_resources);
+                _decision->add_execute_job(pending_job->id,
+                    machines, date);
+                
+
+                // Update data structures
+                _available_machines -= machines;
+                _nb_available_machines -= pending_job->nb_requested_resources;
+                 _current_allocations[pending_job->id] = machines;
+                job_it = _pending_jobs.erase(job_it);
+                _running_jobs.insert(pending_job->id);
+                
+            }
+            else
+            {
+                // The job becomes priority!
+                // As there is no backfilling, we can simply leave this loop.
+                break;
+            }
+        }
+    }
+    
+
+    // If jobs have finished, execute jobs as long as they fit
+    if (job_ended)
+    {
+        for (auto job_it = _pending_jobs.begin();
+             job_it != _pending_jobs.end(); )
+        {
+            Job * pending_job = *job_it;
+            if (pending_job->nb_requested_resources <= _nb_available_machines)
+            {
+                IntervalSet machines = _available_machines.left(
+                    pending_job->nb_requested_resources);
+                _decision->add_execute_job(pending_job->id,
+                    machines, date);
+                
+
+                // Update data structures
+                _available_machines -= machines;
+                _nb_available_machines -= pending_job->nb_requested_resources;
+                 _current_allocations[pending_job->id] = machines;
+                job_it = _pending_jobs.erase(job_it);
+                _running_jobs.insert(pending_job->id);
+                
+            }
+            else
+            {
+                
+                // The job becomes priority!
+                // As there is no backfilling, we can simply leave this loop.
+                break;
+            }
+        }
+    }
+
+    // Handle newly released jobs
+    for (const std::string & new_job_id : _jobs_released_recently)
+    {
+        Job * new_job = (*_workload)[new_job_id];
+
+        // Is this job valid?
+        if (new_job->nb_requested_resources > _nb_machines)
+        {
+            // Invalid!
+            LOG_F(INFO,"Job being rejected HERE %s",new_job_id.c_str());
+            _decision->add_reject_job(new_job_id, date);
+            continue;
+        }
+
+        // Is there a waiting job?
+        if (!_pending_jobs.empty())
+        {   
+            // submitted job is a resubmitted one, put at front of pending jobs
+           if (new_job_id.find("#")!=std::string::npos)
+               _pending_jobs.push_front(new_job);
+            else
+            // Yes. The new job is queued up.
+            _pending_jobs.push_back(new_job);
+        }
+        else
+        {
+            // No, the queue is empty.
+            // Can the new job be executed now?
+            if (new_job->nb_requested_resources <= _nb_available_machines)
+            {
+                // Yes, the job can be executed right away!
+                IntervalSet machines = _available_machines.left(
+                    new_job->nb_requested_resources);
+                _decision->add_execute_job(new_job_id, machines, date);
+
+                // Update data structures
+                _available_machines -= machines;
+                _nb_available_machines -= new_job->nb_requested_resources;
+                _current_allocations[new_job_id] = machines;
+                _running_jobs.insert(new_job_id);
+                
+            }
+            else
+            {
+                // No. The job is queued up.
+                _pending_jobs.push_back(new_job);
+            }
+        }
+    }
+    if (_jobs_killed_recently.empty() && _wrap_it_up && _need_to_send_finished_submitting_jobs && !_myWorkloads->_compute_checkpointing)
+    {
+        
+        _decision->add_scheduler_finished_submitting_jobs(date);
+        _need_to_send_finished_submitting_jobs = false;
+    }
+    else if (_jobs_killed_recently.empty() && _pending_jobs.empty() && _running_jobs.empty() &&
+             _need_to_send_finished_submitting_jobs && _no_more_static_job_to_submit_received && !date<1.0 )
+    {
+        _decision->add_scheduler_finished_submitting_jobs(date);
+        _need_to_send_finished_submitting_jobs = false;
+    }
+      
+}
+std::string FCFSFast2::to_json_desc(rapidjson::Document * doc)
+{
+  rapidjson::StringBuffer buffer;
+
+  buffer.Clear();
+
+  rapidjson::Writer<rapidjson::StringBuffer> writer(buffer);
+  doc->Accept(writer);
+
+  return std::string( buffer.GetString() );
+}
+void FCFSFast2::handle_resubmission(double date)
+{
+ for(const auto & killed_map:_jobs_killed_recently)
+    {
+        std::string killed_job=killed_map.first;
+        double progress = killed_map.second;
+        
+        auto start = killed_job.find("!")+1;
+        auto end = killed_job.find("#");
+        std::string basename = (end ==std::string::npos) ? killed_job.substr(start) : killed_job.substr(start,end-start); 
+        
+        const std::string workload_str = killed_job.substr(0,start-1); 
+        //get the workload
+        myB::Workload * w0= (*_myWorkloads)[workload_str];
+        //get the job that was killed
+        myB::JobPtr job_to_queue =(*(w0->jobs))[myB::JobIdentifier(killed_job)];
+        //get the job identifier of the job that was killed
+        myB::JobIdentifier ji = job_to_queue->id;
+        
+        std::string profile_jd=job_to_queue->profile->json_description;
+        std::string job_jd=job_to_queue->json_description;
+        r::Document profile_doc;
+        profile_doc.Parse(profile_jd.c_str());
+        r::Document doc;
+        doc.Parse(job_jd.c_str());
+        if (_checkpointing_on)
+        {
+            double progress_time = 0;
+            if (progress > 0)
+            {
+                
+                
+                progress_time =progress * profile_doc["delay"].GetDouble();
+                
+                
+                bool has_checkpointed = false;
+                std::string meta_str = "null";
+                int num_checkpoints_completed = 0;
+                r::Document meta_doc;
+                //check whether there is a checkpointed value and set has_checkpointed if so
+                if (doc.HasMember("metadata"))
+                {
+                    
+                    meta_str = doc["metadata"].GetString();
+                    std::replace(meta_str.begin(),meta_str.end(),'\'','\"');
+                    meta_doc.Parse(meta_str.c_str());
+                    if (meta_doc.HasMember("checkpointed"))
+                    {
+                        has_checkpointed = meta_doc["checkpointed"].GetBool();
+                        
+                    }
+                }
+                //if has checkpointed we need to alter how we check num_checkpoints_completed and progress time
+                if (has_checkpointed)
+                {
+                    
+                    //progress_time must be subtracted by read_time to see how many checkpoints we have gone through
+                    num_checkpoints_completed = floor((progress_time-job_to_queue->read_time)/(job_to_queue->checkpoint_time + job_to_queue->dump_time));
+                    if (meta_doc.HasMember("work_progress"))
+                    {
+                        double work = meta_doc["work_progress"].GetDouble();
+                        if (num_checkpoints_completed > 0)
+                            work += num_checkpoints_completed * job_to_queue->checkpoint_time;
+                        meta_doc["work_progress"] = work;
+                    }
+                    else if (num_checkpoints_completed > 0)
+                    {
+                        meta_doc.AddMember("work_progress",r::Value().SetDouble(num_checkpoints_completed * job_to_queue->checkpoint_time),meta_doc.GetAllocator());
+                    }
+                    if (meta_doc.HasMember("num_dumps"))
+                    {
+                            int num_dumps = meta_doc["num_dumps"].GetInt();
+                            if (num_checkpoints_completed > 0)
+                                num_dumps += num_checkpoints_completed;
+                            meta_doc["num_dumps"] = num_dumps;
+                            
+                    }
+                    else if (num_checkpoints_completed > 0)
+                    {
+                            meta_doc.AddMember("num_dumps",r::Value().SetInt(num_checkpoints_completed),meta_doc.GetAllocator());
+                        
+                    }
+                    std::string meta_str = to_json_desc(&meta_doc);
+                    doc["metadata"].SetString(meta_str.c_str(),doc.GetAllocator());
+                    // the progress_time needs to add back in the read_time
+                    progress_time = num_checkpoints_completed * (job_to_queue->checkpoint_time + job_to_queue->dump_time) + job_to_queue->read_time;
+                 
+                }
+                else // there hasn't been any checkpoints in the past, do normal check on num_checkpoints_completed
+                {
+                    num_checkpoints_completed = floor(progress_time/(job_to_queue->checkpoint_time + job_to_queue->dump_time ));
+                    progress_time = num_checkpoints_completed * (job_to_queue->checkpoint_time + job_to_queue->dump_time);
+                    
+                    
+                    //if a checkpoint has completed set the metadata to reflect this
+                    if (num_checkpoints_completed > 0)
+                    {
+                        meta_doc.SetObject();
+                        //if there was previous metadata make sure to include it
+                        if (meta_str!="null")
+                        {
+                            meta_doc.Parse(meta_str.c_str());
+                        }    
+                        r::Document::AllocatorType& myAlloc = meta_doc.GetAllocator();
+                        meta_doc.AddMember("checkpointed",r::Value().SetBool(true),myAlloc);
+                        meta_doc.AddMember("num_dumps",r::Value().SetInt(num_checkpoints_completed),meta_doc.GetAllocator());
+                        meta_doc.AddMember("work_progress",r::Value().SetDouble(num_checkpoints_completed * job_to_queue->checkpoint_time),meta_doc.GetAllocator());
+                        std::string myString = to_json_desc(&meta_doc);
+                        r::Document::AllocatorType& myAlloc2 = doc.GetAllocator();
+                                               
+                        if (meta_str=="null")
+                            doc.AddMember("metadata",r::Value().SetString(myString.c_str(),myAlloc2),myAlloc2);
+                        else
+                            doc["metadata"].SetString(myString.c_str(),myAlloc2);
+                    }
+    
+                }        
+                 //only if a new checkpoint has been reached does the delay time change
+                 
+                if (num_checkpoints_completed > 0)
+                {
+                    
+                    double delay = profile_doc["delay"].GetDouble() - progress_time + job_to_queue->read_time;
+                    profile_doc["delay"].SetDouble(delay);
+                    
+                    
+                }
+            }
+           
+        }
+        doc["subtime"]=date;
+                
+        //check if resubmitted and get the next resubmission number
+        int resubmit = 1;
+        if (end!=std::string::npos) //if job name has # in it...was resubmitted b4
+        {
+            resubmit = std::stoi(killed_job.substr(end+1));   // then get the resubmitted number
+            resubmit++; // and add 1 to it
+        }
+        std::string resubmit_str = std::to_string(resubmit);
+        
+        
+        std::string profile_name = basename + "#" + resubmit_str;
+        std::string job_name = basename + "#" + resubmit_str;
+        std::string job_id = workload_str+"!" + basename + "#" + resubmit_str;
+        std::string workload_name = workload_str;
+        doc["profile"].SetString(profile_name.data(), profile_name.size(), doc.GetAllocator());
+        doc["id"].SetString(job_id.data(),job_id.size(),doc.GetAllocator());
+         std::string error_prefix = "Invalid JSON job '" + killed_job + "'";
+         profile_jd = to_json_desc(&profile_doc);
+        myB::ProfilePtr p = myB::Profile::from_json(profile_name,profile_jd);
+        w0->profiles->add_profile(p);
+        myB::JobPtr j = myB::Job::from_json(doc,w0,error_prefix);
+        w0->jobs->add_job(j);
+        job_jd = to_json_desc(&doc);
+        LOG_F(INFO,"workload: %s  job: %s, profile: %s",workload_name.c_str(),job_name.c_str(),profile_name.c_str());
+        _decision->add_submit_profile(workload_name,
+                                      profile_name,
+                                      profile_jd,
+                                      date);
+                                      
+        _decision->add_submit_job(workload_name,
+                                      job_name,
+                                      profile_name,
+                                      job_jd,
+                                      profile_jd,
+                                      date,
+                                      true);
+        if (doc.HasMember("metadata"))
+        {
+            std::string meta = doc["metadata"].GetString();
+            //must replace double quotes with single quotes.  Remember to
+            //replace single quotes with double quotes before parsing metadata
+            std::replace( meta.begin(), meta.end(), '\"', '\'');
+           _decision->add_set_job_metadata(job_id,
+                                        meta,
+                                        date);
+        }                               
+                
+    }   
+}
+/*(Document, Swap) {
+    Document d1;
+    Document::AllocatorType& a = d1.GetAllocator();
+
+    d1.SetArray().PushBack(1, a).PushBack(2, a);
+
+    Value o;
+    o.SetObject().AddMember("a", 1, a);
+
+    // Swap between Document and Value
+    d1.Swap(o);
+*/
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/fcfs_fast2.cpp.orig /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/fcfs_fast2.cpp.orig
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/fcfs_fast2.cpp.orig	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/fcfs_fast2.cpp.orig	2021-04-02 05:00:32.000000000 -0400
@@ -0,0 +1,492 @@
+#include <math.h>
+#include "fcfs_fast2.hpp"
+
+#include "../pempek_assert.hpp"
+
+#include <loguru.hpp>
+#include "../external/batsched_workload.hpp"
+#include "../external/batsched_job.hpp"
+#include "../external/batsched_profile.hpp"
+#include "../external/pointers.hpp"
+#include <rapidjson/document.h>
+#include <rapidjson/writer.h>
+
+namespace myB = myBatsched;
+namespace r = rapidjson;
+
+FCFSFast2::FCFSFast2(Workload *workload,
+    SchedulingDecision *decision, Queue *queue, ResourceSelector *selector,
+    double rjms_delay, rapidjson::Document *variant_options) :
+    ISchedulingAlgorithm(workload, decision, queue, selector, rjms_delay,
+        variant_options)
+{
+    LOG_F(INFO,"created 4");
+    _myWorkloads = new myBatsched::Workloads;
+    
+}
+
+FCFSFast2::~FCFSFast2()
+{
+    
+}
+
+void FCFSFast2::on_simulation_start(double date,
+    const rapidjson::Value &batsim_config)
+{
+    (void) date;
+    (void) batsim_config;
+
+    _available_machines.insert(IntervalSet::ClosedInterval(0, _nb_machines - 1));
+    _nb_available_machines = _nb_machines;
+    PPK_ASSERT_ERROR(_available_machines.size() == (unsigned int) _nb_machines);
+    _oldDate=date;
+        
+}
+
+void FCFSFast2::on_simulation_end(double date)
+{
+    (void) date;
+}
+void FCFSFast2::on_machine_unavailable_notify_event(double date, IntervalSet machines){
+    LOG_F(INFO,"unavailable %s",machines.to_string_hyphen().c_str());
+    _unavailable_machines+=machines;
+    _available_machines-=machines;
+    for(auto key_value : _current_allocations)
+    {
+            if (!((key_value.second & machines).is_empty()))
+                _decision->add_kill_job({key_value.first},date);
+    }
+    
+}
+<<<<<<< HEAD
+void FCFSFast2::set_workload_file(myBatsched::Workload *w){
+    _myWorkloads->insert_workload(std::string("w0"),w);
+=======
+void FCFSFast2::set_workloads(myBatsched::Workloads *w){
+    _myWorkloads = w;
+>>>>>>> 4740e92
+    _checkpointing_on = w->_checkpointing_on;
+}
+void FCFSFast2::on_machine_available_notify_event(double date, IntervalSet machines){
+    ISchedulingAlgorithm::on_machine_available_notify_event(date, machines);
+    _unavailable_machines-=machines;
+    _available_machines+=machines;
+    _nb_available_machines+=machines.size();
+    
+}
+
+void FCFSFast2::on_machine_state_changed(double date, IntervalSet machines, int new_state)
+{
+   
+
+}
+void FCFSFast2::on_myKillJob_notify_event(double date){
+    
+    if (!_running_jobs.empty()){
+        _my_kill_jobs.insert(*_running_jobs.begin());
+    }
+        
+    
+}
+void FCFSFast2::on_job_fault_notify_event(double date, std::string job){
+    std::unordered_set<std::string>::const_iterator found = _running_jobs.find(job);
+  LOG_F(INFO,"on_job_fault_notify_event called");
+  if ( found != _running_jobs.end() )    
+        _decision->add_kill_job({job},date);
+  else
+      LOG_F(INFO,"Job %s was not running but was supposed to be killed due to job_fault event",job.c_str());
+}
+
+void FCFSFast2::on_requested_call(double date)
+{
+  
+}
+void FCFSFast2::on_no_more_external_event_to_occur(double date){
+    
+    _wrap_it_up = true;    
+    
+    
+}
+void FCFSFast2::on_job_end(double date, std::vector<std::string> job_ids)
+{
+    (void) date;
+    (void) job_ids;
+}
+
+void FCFSFast2::make_decisions(double date,
+    SortableJobOrder::UpdateInformation *update_info,
+    SortableJobOrder::CompareInformation *compare_info)
+{
+    (void) update_info;
+    (void) compare_info;
+    
+    // This algorithm is a fast version of FCFS without backfilling.
+    // It is meant to be fast in the usual case, not to handle corner cases.
+    // It is not meant to be easily readable or hackable ;).
+
+    // This fast FCFS variant in a few words:
+    // - only handles the FCFS queue order
+    // - only handles the basic resource selection policy
+    // - only handles finite jobs (no switchoff)
+    // - only handles time as floating-point (-> precision errors).
+    
+        
+    
+    bool job_ended = false;
+
+    // Handle newly finished jobs
+    for (const std::string & ended_job_id : _jobs_ended_recently)
+    {
+        job_ended = true;
+        LOG_F(INFO,"job ended: %s %f",ended_job_id.c_str(),date);
+        
+        // Update data structures
+        
+        _available_machines.insert(_current_allocations[ended_job_id]-_unavailable_machines);
+        _nb_available_machines += (_current_allocations[ended_job_id]-_unavailable_machines).size();
+        _current_allocations.erase(ended_job_id);
+        _running_jobs.erase(ended_job_id);
+        _my_kill_jobs.erase(ended_job_id);
+    }
+    
+    //Handle new jobs to kill
+   
+    if(!_my_kill_jobs.empty()){
+         std::vector<std::string> kills;
+        for( std::string kill:_my_kill_jobs)
+            kills.push_back(kill);
+        _decision->add_kill_job(kills,date);
+        _my_kill_jobs.clear();
+    }
+    //************************************************************resubmission if killed
+    //Handle jobs to queue back up (if killed)  
+    handle_resubmission(date);    
+    //*************************************************************
+    
+    if (_jobs_killed_recently.empty() && _wrap_it_up && _need_to_send_finished_submitting_jobs)
+    {
+        _decision->add_scheduler_finished_submitting_jobs(date);
+        _need_to_send_finished_submitting_jobs = false;
+    }
+    
+    
+    
+    if (!(_machines_that_became_available_recently.is_empty()))
+    {
+        for (auto job_it = _pending_jobs.begin();
+             job_it != _pending_jobs.end(); )
+        {
+            Job * pending_job = *job_it;
+            if (pending_job->nb_requested_resources <= _nb_available_machines)
+            {
+                IntervalSet machines = _available_machines.left(
+                    pending_job->nb_requested_resources);
+                _decision->add_execute_job(pending_job->id,
+                    machines, date);
+                
+
+                // Update data structures
+                _available_machines -= machines;
+                _nb_available_machines -= pending_job->nb_requested_resources;
+                 _current_allocations[pending_job->id] = machines;
+                job_it = _pending_jobs.erase(job_it);
+                _running_jobs.insert(pending_job->id);
+                
+            }
+            else
+            {
+                // The job becomes priority!
+                // As there is no backfilling, we can simply leave this loop.
+                break;
+            }
+        }
+    }
+    
+
+    // If jobs have finished, execute jobs as long as they fit
+    if (job_ended)
+    {
+        for (auto job_it = _pending_jobs.begin();
+             job_it != _pending_jobs.end(); )
+        {
+            Job * pending_job = *job_it;
+            if (pending_job->nb_requested_resources <= _nb_available_machines)
+            {
+                IntervalSet machines = _available_machines.left(
+                    pending_job->nb_requested_resources);
+                _decision->add_execute_job(pending_job->id,
+                    machines, date);
+                
+
+                // Update data structures
+                _available_machines -= machines;
+                _nb_available_machines -= pending_job->nb_requested_resources;
+                 _current_allocations[pending_job->id] = machines;
+                job_it = _pending_jobs.erase(job_it);
+                _running_jobs.insert(pending_job->id);
+                
+            }
+            else
+            {
+                // The job becomes priority!
+                // As there is no backfilling, we can simply leave this loop.
+                break;
+            }
+        }
+    }
+
+    // Handle newly released jobs
+    for (const std::string & new_job_id : _jobs_released_recently)
+    {
+        Job * new_job = (*_workload)[new_job_id];
+
+        // Is this job valid?
+        if (new_job->nb_requested_resources > _nb_machines)
+        {
+            // Invalid!
+            LOG_F(INFO,"Job being rejected HERE %s",new_job_id.c_str());
+            _decision->add_reject_job(new_job_id, date);
+            continue;
+        }
+
+        // Is there a waiting job?
+        if (!_pending_jobs.empty())
+        {   
+            // submitted job is a resubmitted one, put at front of pending jobs
+           if (new_job_id.find("#")!=std::string::npos)
+               _pending_jobs.push_front(new_job);
+            else
+            // Yes. The new job is queued up.
+            _pending_jobs.push_back(new_job);
+        }
+        else
+        {
+            // No, the queue is empty.
+            // Can the new job be executed now?
+            if (new_job->nb_requested_resources <= _nb_available_machines)
+            {
+                // Yes, the job can be executed right away!
+                IntervalSet machines = _available_machines.left(
+                    new_job->nb_requested_resources);
+                _decision->add_execute_job(new_job_id, machines, date);
+
+                // Update data structures
+                _available_machines -= machines;
+                _nb_available_machines -= new_job->nb_requested_resources;
+                _current_allocations[new_job_id] = machines;
+                _running_jobs.insert(new_job_id);
+                
+            }
+            else
+            {
+                // No. The job is queued up.
+                _pending_jobs.push_back(new_job);
+            }
+        }
+    }
+      
+}
+std::string FCFSFast2::to_json_desc(rapidjson::Document * doc)
+{
+  rapidjson::StringBuffer buffer;
+
+  buffer.Clear();
+
+  rapidjson::Writer<rapidjson::StringBuffer> writer(buffer);
+  doc->Accept(writer);
+
+  return std::string( buffer.GetString() );
+}
+void FCFSFast2::handle_resubmission(double date)
+{
+ for(const auto & killed_map:_jobs_killed_recently)
+    {
+        std::string killed_job=killed_map.first;
+        double progress = killed_map.second;
+        
+        auto start = killed_job.find("!")+1;
+        auto end = killed_job.find("#");
+        std::string basename = (end ==std::string::npos) ? killed_job.substr(start) : killed_job.substr(start,end-start); 
+        
+        const std::string workload_str = killed_job.substr(0,start-1); 
+        //get the workload
+        myB::Workload * w0= (*_myWorkloads)[workload_str];
+        //get the job that was killed
+        myB::JobPtr job_to_queue =(*(w0->jobs))[myB::JobIdentifier(killed_job)];
+        //get the job identifier of the job that was killed
+        myB::JobIdentifier ji = job_to_queue->id;
+        
+        std::string profile_jd=job_to_queue->profile->json_description;
+        std::string job_jd=job_to_queue->json_description;
+        r::Document profile_doc;
+        profile_doc.Parse(profile_jd.c_str());
+        r::Document doc;
+        doc.Parse(job_jd.c_str());
+        if (_checkpointing_on)
+        {
+            double progress_time = 0;
+            if (progress > 0)
+            {
+                
+                
+                progress_time =round( progress * profile_doc["delay"].GetDouble());
+                
+                
+                bool has_checkpointed = false;
+                std::string meta_str = "null";
+                int num_checkpoints_completed = 0;
+                r::Document meta_doc;
+                //check whether there is a checkpointed value and set has_checkpointed if so
+                if (doc.HasMember("metadata"))
+                {
+                    
+                    meta_str = doc["metadata"].GetString();
+                    std::replace(meta_str.begin(),meta_str.end(),'\'','\"');
+                    meta_doc.Parse(meta_str.c_str());
+                    if (meta_doc.HasMember("checkpointed"))
+                    {
+                        has_checkpointed = meta_doc["checkpointed"].GetBool();
+                        
+                    }
+                }
+                //if has checkpointed we need to alter how we check num_checkpoints_completed and progress time
+                if (has_checkpointed)
+                {
+                    
+                    //progress_time must be subtracted by read_time to see how many checkpoints we have gone through
+                    num_checkpoints_completed = floor((progress_time-job_to_queue->read_time)/(job_to_queue->checkpoint_time + job_to_queue->dump_time));
+                    if (meta_doc.HasMember("work_progress"))
+                    {
+                        int work = meta_doc["work_progress"].GetInt();
+                        if (num_checkpoints_completed > 0)
+                            work += num_checkpoints_completed * job_to_queue->checkpoint_time;
+                        meta_doc["work_progress"] = work;
+                    }
+                    else if (num_checkpoints_completed > 0)
+                    {
+                        meta_doc.AddMember("work_progress",r::Value().SetInt(num_checkpoints_completed * job_to_queue->checkpoint_time),meta_doc.GetAllocator());
+                    }
+                    if (meta_doc.HasMember("num_dumps"))
+                    {
+                            int num_dumps = meta_doc["num_dumps"].GetInt();
+                            if (num_checkpoints_completed > 0)
+                                num_dumps += num_checkpoints_completed;
+                            meta_doc["num_dumps"] = num_dumps;
+                            
+                    }
+                    else if (num_checkpoints_completed > 0)
+                    {
+                            meta_doc.AddMember("num_dumps",r::Value().SetInt(num_checkpoints_completed),meta_doc.GetAllocator());
+                        
+                    }
+                    std::string meta_str = to_json_desc(&meta_doc);
+                    doc["metadata"].SetString(meta_str.c_str(),doc.GetAllocator());
+                    // the progress_time needs to add back in the read_time
+                    progress_time = num_checkpoints_completed * (job_to_queue->checkpoint_time + job_to_queue->dump_time) + job_to_queue->read_time;
+                    LOG_F(INFO,"progress has: %f",progress_time);
+                }
+                else // there hasn't been any checkpoints in the past, do normal check on num_checkpoints_completed
+                {
+                    num_checkpoints_completed = floor(progress_time/(job_to_queue->checkpoint_time + job_to_queue->dump_time ));
+                    progress_time = num_checkpoints_completed * (job_to_queue->checkpoint_time + job_to_queue->dump_time);
+                    LOG_F(INFO,"progress hasn't: %f",progress_time);
+                    
+                    //if a checkpoint has completed set the metadata to reflect this
+                    if (num_checkpoints_completed > 0)
+                    {
+                        meta_doc.SetObject();
+                        //if there was previous metadata make sure to include it
+                        if (meta_str!="null")
+                        {
+                            meta_doc.Parse(meta_str.c_str());
+                        }    
+                        r::Document::AllocatorType& myAlloc = meta_doc.GetAllocator();
+                        meta_doc.AddMember("checkpointed",r::Value().SetBool(true),myAlloc);
+                        meta_doc.AddMember("num_dumps",r::Value().SetInt(num_checkpoints_completed),meta_doc.GetAllocator());
+                        meta_doc.AddMember("work_progress",r::Value().SetInt(num_checkpoints_completed * job_to_queue->checkpoint_time),meta_doc.GetAllocator());
+                        std::string myString = to_json_desc(&meta_doc);
+                        r::Document::AllocatorType& myAlloc2 = doc.GetAllocator();
+                                               
+                        if (meta_str=="null")
+                            doc.AddMember("metadata",r::Value().SetString(myString.c_str(),myAlloc2),myAlloc2);
+                        else
+                            doc["metadata"].SetString(myString.c_str(),myAlloc2);
+                    }
+    
+                }        
+                 //only if a new checkpoint has been reached does the delay time change
+                 
+                if (num_checkpoints_completed > 0)
+                {
+                    
+                    double delay = profile_doc["delay"].GetDouble() - progress_time + job_to_queue->read_time;
+                    profile_doc["delay"].SetDouble(delay);
+                    
+                    
+                }
+            }
+           
+        }
+        doc["subtime"]=date;
+                
+        //check if resubmitted and get the next resubmission number
+        int resubmit = 1;
+        if (end!=std::string::npos) //if job name has # in it...was resubmitted b4
+        {
+            resubmit = std::stoi(killed_job.substr(end+1));   // then get the resubmitted number
+            resubmit++; // and add 1 to it
+        }
+        std::string resubmit_str = std::to_string(resubmit);
+        
+        
+        std::string profile_name = basename + "#" + resubmit_str;
+        std::string job_name = basename + "#" + resubmit_str;
+        std::string job_id = workload_str+"!" + basename + "#" + resubmit_str;
+        std::string workload_name = workload_str;
+        doc["profile"].SetString(profile_name.data(), profile_name.size(), doc.GetAllocator());
+        doc["id"].SetString(job_id.data(),job_id.size(),doc.GetAllocator());
+         std::string error_prefix = "Invalid JSON job '" + killed_job + "'";
+         profile_jd = to_json_desc(&profile_doc);
+        myB::ProfilePtr p = myB::Profile::from_json(profile_name,profile_jd);
+        w0->profiles->add_profile(p);
+        myB::JobPtr j = myB::Job::from_json(doc,w0,error_prefix);
+        w0->jobs->add_job(j);
+        job_jd = to_json_desc(&doc);
+        LOG_F(INFO,"workload: %s  job: %s, profile: %s",workload_name.c_str(),job_name.c_str(),profile_name.c_str());
+        _decision->add_submit_profile(workload_name,
+                                      profile_name,
+                                      profile_jd,
+                                      date);
+                                      
+        _decision->add_submit_job(workload_name,
+                                      job_name,
+                                      profile_name,
+                                      job_jd,
+                                      profile_jd,
+                                      date,
+                                      true);
+        if (doc.HasMember("metadata"))
+        {
+            std::string meta = doc["metadata"].GetString();
+            //must replace double quotes with single quotes.  Remember to
+            //replace single quotes with double quotes before parsing metadata
+            std::replace( meta.begin(), meta.end(), '\"', '\'');
+           _decision->add_set_job_metadata(job_id,
+                                        meta,
+                                        date);
+        }                               
+                
+    }   
+}
+/*(Document, Swap) {
+    Document d1;
+    Document::AllocatorType& a = d1.GetAllocator();
+
+    d1.SetArray().PushBack(1, a).PushBack(2, a);
+
+    Value o;
+    o.SetObject().AddMember("a", 1, a);
+
+    // Swap between Document and Value
+    d1.Swap(o);
+*/
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/fcfs_fast2.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/fcfs_fast2.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/fcfs_fast2.hpp	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/fcfs_fast2.hpp	2021-04-02 05:00:29.000000000 -0400
@@ -0,0 +1,68 @@
+#pragma once
+
+#include <unordered_map>
+#include <unordered_set>
+#include <list>
+#include <random>
+
+#include "../isalgorithm.hpp"
+#include "../json_workload.hpp"
+#include "../locality.hpp"
+#include "../external/batsched_workload.hpp"
+#include <rapidjson/document.h>
+
+class FCFSFast2 : public ISchedulingAlgorithm
+{
+public:
+    FCFSFast2(Workload * workload, SchedulingDecision * decision,
+        Queue * queue, ResourceSelector * selector,
+        double rjms_delay,
+        rapidjson::Document * variant_options);
+    virtual ~FCFSFast2();
+
+    virtual void on_simulation_start(double date,
+        const rapidjson::Value & batsim_config);
+
+    virtual void on_simulation_end(double date);
+    virtual void on_machine_unavailable_notify_event(double date, IntervalSet machines);
+    virtual void on_machine_available_notify_event(double date, IntervalSet machines);
+    virtual void on_job_fault_notify_event(double date, std::string job);
+    virtual void on_myKillJob_notify_event(double date);
+    virtual void on_requested_call(double date);
+    virtual void set_workloads(myBatsched::Workloads * w);
+    virtual void make_decisions(double date,
+        SortableJobOrder::UpdateInformation * update_info,
+        SortableJobOrder::CompareInformation * compare_info);
+    std::string to_json_desc(rapidjson::Document *doc);
+    void handle_resubmission(double date);
+    void on_machine_instant_down_up(double date);
+    virtual void on_no_more_external_event_to_occur(double date);
+    virtual void on_job_end(double date, std::vector<std::string> job_ids);
+    virtual void on_machine_state_changed(double date, IntervalSet machines, int new_state);
+    virtual void on_no_more_static_job_to_submit_received(double date);
+
+private:
+    // Machines currently available
+    IntervalSet _available_machines;
+    IntervalSet _unavailable_machines;
+    int _nb_available_machines = -1;
+
+    // Pending jobs (queue)
+    std::list<Job *> _pending_jobs;
+    std::unordered_set<std::string> _my_kill_jobs;
+    std::unordered_set<std::string> _running_jobs;
+    myBatsched::Workloads * _myWorkloads;
+    double _oldDate=-1;
+    int _killed=0;
+    bool _wrap_it_up = false;
+    bool _need_to_send_finished_submitting_jobs = true;
+    bool _checkpointing_on=false;
+    std::vector<double> _call_me_laters;
+    std::mt19937 generator;
+    std::exponential_distribution<double> * distribution;
+    std::mt19937 generator2;
+    std::uniform_int_distribution<int> * unif_distribution;
+
+    // Allocations of running jobs
+    std::unordered_map<std::string, IntervalSet> _current_allocations;
+};
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/fcfs_fast2.hpp.orig /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/fcfs_fast2.hpp.orig
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/fcfs_fast2.hpp.orig	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/fcfs_fast2.hpp.orig	2021-04-02 05:00:42.000000000 -0400
@@ -0,0 +1,65 @@
+#pragma once
+
+#include <unordered_map>
+#include <unordered_set>
+#include <list>
+
+#include "../isalgorithm.hpp"
+#include "../json_workload.hpp"
+#include "../locality.hpp"
+#include "../external/batsched_workload.hpp"
+#include <rapidjson/document.h>
+
+class FCFSFast2 : public ISchedulingAlgorithm
+{
+public:
+    FCFSFast2(Workload * workload, SchedulingDecision * decision,
+        Queue * queue, ResourceSelector * selector,
+        double rjms_delay,
+        rapidjson::Document * variant_options);
+    virtual ~FCFSFast2();
+
+    virtual void on_simulation_start(double date,
+        const rapidjson::Value & batsim_config);
+
+    virtual void on_simulation_end(double date);
+    virtual void on_machine_unavailable_notify_event(double date, IntervalSet machines);
+    virtual void on_machine_available_notify_event(double date, IntervalSet machines);
+    virtual void on_job_fault_notify_event(double date, std::string job);
+    virtual void on_myKillJob_notify_event(double date);
+    virtual void on_requested_call(double date);
+    virtual void set_workloads(myBatsched::Workloads * w);
+    virtual void make_decisions(double date,
+        SortableJobOrder::UpdateInformation * update_info,
+        SortableJobOrder::CompareInformation * compare_info);
+    std::string to_json_desc(rapidjson::Document *doc);
+    void handle_resubmission(double date);
+    virtual void on_no_more_external_event_to_occur(double date);
+    virtual void on_job_end(double date, std::vector<std::string> job_ids);
+    virtual void on_machine_state_changed(double date, IntervalSet machines, int new_state);
+
+private:
+    // Machines currently available
+    IntervalSet _available_machines;
+    IntervalSet _unavailable_machines;
+    int _nb_available_machines = -1;
+
+    // Pending jobs (queue)
+    std::list<Job *> _pending_jobs;
+    std::unordered_set<std::string> _my_kill_jobs;
+    std::unordered_set<std::string> _running_jobs;
+    myBatsched::Workloads * _myWorkloads;
+    double _oldDate=-1;
+    int _killed=0;
+    bool _wrap_it_up = false;
+<<<<<<< HEAD
+    bool _checkpointing_on=false;
+=======
+    bool _need_to_send_finished_submitting_jobs = true;
+    bool _checkpointing_on=false;
+    std::vector<double> _call_me_laters;
+>>>>>>> 4740e92
+
+    // Allocations of running jobs
+    std::unordered_map<std::string, IntervalSet> _current_allocations;
+};
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/filler.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/filler.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/filler.cpp	2021-04-04 01:01:41.813519288 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/filler.cpp	2021-04-02 05:00:42.000000000 -0400
@@ -68,13 +68,14 @@
     // Let's update available machines
     for (const string & ended_job_id : _jobs_ended_recently)
     {
-        int nb_available_before = available_machines.size();
         available_machines.insert(current_allocations[ended_job_id]);
-        int nb_job_resources = ceil((*_workload)[ended_job_id]->nb_requested_resources * fraction_of_machines_to_use);
-        PPK_ASSERT_ERROR(nb_available_before + nb_job_resources == (int)available_machines.size());
         current_allocations.erase(ended_job_id);
     }
 
+    // Handle machine (un)availability from user events
+    unavailable_machines -= _machines_that_became_available_recently;
+    unavailable_machines += _machines_that_became_unavailable_recently;
+
     // Let's handle recently released jobs
     for (const string & new_job_id : _jobs_released_recently)
     {
@@ -94,18 +95,19 @@
 
 void Filler::fill(double date)
 {
+    IntervalSet usable_machines = available_machines - unavailable_machines;
     if (_debug)
-        LOG_F(1, "fill, availableMachines=%s", available_machines.to_string_hyphen().c_str());
+        LOG_F(1, "fill, usable_machines=%s", usable_machines.to_string_hyphen().c_str());
 
-    int nb_available = available_machines.size();
-    for (auto job_it = _queue->begin(); job_it != _queue->end() && nb_available > 0; )
+    int nb_usable = usable_machines.size();
+    for (auto job_it = _queue->begin(); job_it != _queue->end() && nb_usable > 0; )
     {
         const Job * job = (*job_it)->job;
 
         // If it fits I sits (http://knowyourmeme.com/memes/if-it-fits-i-sits)
         IntervalSet used_machines;
 
-        if (_selector->fit(job, available_machines, used_machines))
+        if (_selector->fit(job, usable_machines, used_machines))
         {
             // Fewer machines might be used that those selected by the fitting algorithm
             int nb_machines_to_allocate = ceil(fraction_of_machines_to_use * job->nb_requested_resources);
@@ -127,9 +129,9 @@
 
             current_allocations[job->id] = used_machines;
 
+            usable_machines.remove(used_machines);
             available_machines.remove(used_machines);
-            PPK_ASSERT_ERROR(nb_available - used_machines.size() == available_machines.size());
-            nb_available -= used_machines.size();
+            nb_usable -= used_machines.size();
 
             if (set_job_metadata)
                 _decision->add_set_job_metadata(job->id,
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/filler.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/filler.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/algo/filler.hpp	2021-04-04 01:01:41.813519288 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/algo/filler.hpp	2021-04-02 05:00:31.000000000 -0400
@@ -36,7 +36,8 @@
     bool set_job_metadata = false; //! If set to true, metadata will be associated to jobs when they are started.
     bool custom_mapping = true;
 
-    IntervalSet available_machines;
+    IntervalSet available_machines; // Corresponds to classical availability: no job is running on those machines.
+    IntervalSet unavailable_machines; // This is NOT the complement of available_machines! This correspond to user-supplied events, that may overlap strangely with job executions as I write these lines.
     std::map<std::string, IntervalSet> current_allocations;
     bool _debug = true;
 };
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/decision.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/decision.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/decision.cpp	2021-04-04 01:01:32.685607441 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/decision.cpp	2021-04-02 05:00:09.000000000 -0400
@@ -81,6 +81,14 @@
     _proto_writer->append_set_resource_state(machines, std::to_string(new_state), date);
 }
 
+void SchedulingDecision::add_set_resources_available(IntervalSet machines, double date)
+{
+    _proto_writer->append_set_resources_available(machines,date);
+}
+void SchedulingDecision::add_set_resources_unavailable(IntervalSet machines,double date)
+{
+    _proto_writer->append_set_resources_unavailable(machines,date);
+}
 void SchedulingDecision::add_set_job_metadata(const string &job_id,
                                               const string &metadata,
                                               double date)
@@ -98,6 +106,11 @@
     _proto_writer->append_scheduler_finished_submitting_jobs(date);
 }
 
+void SchedulingDecision::add_scheduler_continue_submitting_jobs(double date)
+{
+    _proto_writer->append_scheduler_continue_submitting_jobs(date);
+}
+
 void SchedulingDecision::add_query_energy_consumption(double date)
 {
     _proto_writer->append_query_consumed_energy(date);
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/decision.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/decision.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/decision.hpp	2021-04-04 01:01:32.685607441 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/decision.hpp	2021-04-02 05:00:22.000000000 -0400
@@ -46,9 +46,12 @@
     void add_set_job_metadata(const std::string & job_id,
                               const std::string & metadata,
                               double date);
+    void add_set_resources_available(IntervalSet machines,double date);
+    void add_set_resources_unavailable(IntervalSet machines,double date);
 
     void add_call_me_later(double future_date, double date);
     void add_scheduler_finished_submitting_jobs(double date);
+    void add_scheduler_continue_submitting_jobs(double date);
 
     void add_query_energy_consumption(double date);
     void add_answer_estimate_waiting_time(const std::string & job_id,
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/batsched_job.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/batsched_job.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/batsched_job.cpp	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/batsched_job.cpp	2021-04-02 05:00:14.000000000 -0400
@@ -0,0 +1,429 @@
+#include <string>
+#include <fstream>
+#include <streambuf>
+#include <algorithm>
+#include <regex>
+#include <math.h>
+
+#include <boost/algorithm/string.hpp>
+#include <boost/algorithm/string/join.hpp>
+#include <loguru.hpp>
+#include <rapidjson/document.h>
+#include <rapidjson/writer.h>
+#include <rapidjson/stringbuffer.h>
+#include "batsched_job.hpp"
+#include "batsched_profile.hpp"
+#include "batsched_workload.hpp"
+
+using namespace rapidjson;
+namespace myBatsched{
+Jobs::~Jobs()
+{
+    _jobs.clear();
+}
+
+JobIdentifier::JobIdentifier(const std::string & workload_name,
+                             const std::string & job_name) :
+    _workload_name(workload_name),
+    _job_name(job_name)
+{
+    _representation = representation();
+}
+
+JobIdentifier::JobIdentifier(const std::string & job_id_str)
+{
+    // Split the job_identifier by '!'
+    std::vector<std::string> job_identifier_parts;
+    boost::split(job_identifier_parts, job_id_str,
+                 boost::is_any_of("!"), boost::token_compress_on);
+
+    CHECK_F(job_identifier_parts.size() == 2,
+               "Invalid string job identifier '%s': should be formatted as two '!'-separated "
+               "parts, the second one being any string without '!'. Example: 'some_text!42'.",
+               job_id_str.c_str());
+
+    this->_workload_name = job_identifier_parts[0];
+    this->_job_name = job_identifier_parts[1];
+    _representation = representation();
+}
+std::string JobIdentifier::to_string() const
+{
+    return _representation;
+}
+const char *JobIdentifier::to_cstring() const
+{
+    return _representation.c_str();
+}
+std::string JobIdentifier::representation() const
+{
+    return _workload_name + '!' + _job_name;
+}
+bool operator<(const JobIdentifier &ji1, const JobIdentifier &ji2)
+{
+    return ji1.to_string() < ji2.to_string();
+}
+
+bool operator==(const JobIdentifier &ji1, const JobIdentifier &ji2)
+{
+    return ji1.to_string() == ji2.to_string();
+}
+std::size_t JobIdentifierHasher::operator()(const JobIdentifier & id) const
+{
+    return std::hash<std::string>()(id.to_string());
+}
+std::string JobIdentifier::workload_name() const
+{
+    return _workload_name;
+}
+
+std::string JobIdentifier::job_name() const
+{
+    return _job_name;
+}
+
+void Jobs::set_profiles(Profiles *profiles)
+{
+    _profiles = profiles;
+}
+
+void Jobs::set_workload(Workload *workload)
+{
+    _workload = workload;
+}
+void Jobs::add_job(JobPtr job)
+{
+ _jobs[job->id]=job;
+ _jobs_met.insert({job->id,true}); // not sure about this, whether it's really needed or not    
+    
+}
+void Jobs::load_from_json(const rapidjson::Document &doc, const std::string &filename)
+{
+    std::string error_prefix = "Invalid JSON file '" + filename + "'";
+
+    CHECK_F(doc.IsObject(), "%s: not a JSON object", error_prefix.c_str());
+    CHECK_F(doc.HasMember("jobs"), "%s: the 'jobs' array is missing", error_prefix.c_str());
+    const Value & jobs = doc["jobs"];
+    if (jobs.IsArray())
+    {
+
+        for (SizeType i = 0; i < jobs.Size(); i++) // Uses SizeType instead of size_t
+        {
+            //const Value & job_json_description = jobs[i];
+            std::string job_desc = jobs[i].GetString();
+            LOG_F(INFO,"our string: %s",job_desc.c_str());
+            job_desc=Job::not_escaped(job_desc);
+            Document d;
+            d.Parse(job_desc.c_str());
+            const rapidjson::Value & job_json_description = d.GetObject();
+            auto j = Job::from_json(job_json_description, _workload, error_prefix);
+
+            CHECK_F(!exists(j->id), "%s: duplication of job id '%s'",
+                    error_prefix.c_str(), j->id.to_string().c_str());
+            _jobs[j->id] = j;
+            _jobs_met.insert({j->id, true});
+        }
+    }
+    
+}
+
+std::string Job::not_escaped(const std::string& input)
+{
+    std::string output;
+    output.reserve(input.size());
+    for (const char c: input) {
+        switch (c) {
+            case '\\':  output += "";        break;
+            default:    output += c;            break;
+        }
+    }
+
+    return output;
+}
+std::string Job::to_json_desc(rapidjson::Document * doc)
+{
+  rapidjson::StringBuffer buffer;
+
+  buffer.Clear();
+
+  rapidjson::Writer<rapidjson::StringBuffer> writer(buffer);
+  doc->Accept(writer);
+
+  return std::string( buffer.GetString() );
+}
+JobPtr Job::from_json(const rapidjson::Value & json_desc,
+                     Workload * workload,
+                     const std::string & error_prefix)
+{
+        
+    // Create and initialize with default values
+    auto j =std::make_shared<Job>();
+    j->workload = workload;
+    j->starting_time = -1;
+    j->runtime = -1;
+    j->state = JobState::JOB_STATE_NOT_SUBMITTED;
+    
+
+    CHECK_F(json_desc.IsObject(), "%s: one job is not an object", error_prefix.c_str());
+
+    // Get job id and create a JobIdentifier
+    CHECK_F(json_desc.HasMember("id"), "%s: one job has no 'id' field", error_prefix.c_str());
+    CHECK_F(json_desc["id"].IsString() || json_desc["id"].IsInt(), "%s: on job id field is invalid, it should be a string or an integer", error_prefix.c_str());
+    std::string job_id_str;
+    if (json_desc["id"].IsString())
+    {
+        job_id_str = json_desc["id"].GetString();
+    }
+    else if (json_desc["id"].IsInt())
+    {
+        job_id_str = std::to_string(json_desc["id"].GetInt());
+    }
+
+    if (job_id_str.find(workload->name) == std::string::npos)
+    {
+        // the workload name is not present in the job id string
+        j->id = JobIdentifier(workload->name, job_id_str);
+    }
+    else
+    {
+        j->id = JobIdentifier(job_id_str);
+    }
+
+    // Get submission time
+    CHECK_F(json_desc.HasMember("subtime"), "%s: job '%s' has no 'subtime' field",
+               error_prefix.c_str(), j->id.to_string().c_str());
+    CHECK_F(json_desc["subtime"].IsNumber(), "%s: job '%s' has a non-number 'subtime' field",
+               error_prefix.c_str(), j->id.to_string().c_str());
+    j->submission_time = static_cast<long double>(json_desc["subtime"].GetDouble());
+
+    // Get walltime (optional)
+    if (!json_desc.HasMember("walltime"))
+    {
+        LOG_F(INFO,"job '%s' has no 'walltime' field", j->id.to_string().c_str());
+    }
+    else
+    {
+        CHECK_F(json_desc["walltime"].IsNumber(), "%s: job %s has a non-number 'walltime' field",
+                   error_prefix.c_str(), j->id.to_string().c_str());
+        j->walltime = static_cast<long double>(json_desc["walltime"].GetDouble());
+    }
+    CHECK_F(j->walltime == -1 || j->walltime > 0,
+               "%s: job '%s' has an invalid walltime (%Lg). It should either be -1 (no walltime) "
+               "or a strictly positive number.",
+               error_prefix.c_str(), j->id.to_string().c_str(), j->walltime);
+
+    // Get number of requested resources
+    CHECK_F(json_desc.HasMember("res"), "%s: job %s has no 'res' field",
+               error_prefix.c_str(), j->id.to_string().c_str());
+    CHECK_F(json_desc["res"].IsInt(), "%s: job %s has a non-number 'res' field",
+               error_prefix.c_str(), j->id.to_string().c_str());
+    CHECK_F(json_desc["res"].GetInt() >= 0, "%s: job %s has a negative 'res' field (%d)",
+               error_prefix.c_str(), j->id.to_string().c_str(), json_desc["res"].GetInt());
+    j->requested_nb_res = static_cast<unsigned int>(json_desc["res"].GetInt());
+
+    // Get the job profile
+    CHECK_F(json_desc.HasMember("profile"), "%s: job %s has no 'profile' field",
+               error_prefix.c_str(), j->id.to_string().c_str());
+    CHECK_F(json_desc["profile"].IsString(), "%s: job %s has a non-string 'profile' field",
+               error_prefix.c_str(), j->id.to_string().c_str());
+
+    // TODO raise exception when the profile does not exist.
+    std::string profile_name = json_desc["profile"].GetString();
+    CHECK_F(workload->profiles->exists(profile_name), "%s: the profile %s for job %s does not exist",
+               error_prefix.c_str(), profile_name.c_str(), j->id.to_string().c_str());
+    j->profile = workload->profiles->at(profile_name);
+    
+    if(workload->_checkpointing_on)
+    {
+    
+        //***** got rid of CHECK_F statements as checkpointing is optional so, so are the fields
+        /*CHECK_F(json_desc.HasMember("checkpoint"),"%s: job %s has no 'checkpoint' field",
+                error_prefix.c_str(),j->id.to_string().c_str());
+        CHECK_F(json_desc["checkpoint"].IsNumber(), "%s: job %s has non double 'checkpoint' field",
+                error_prefix.c_str(),j->id.to_string().c_str()); */
+        if (json_desc.HasMember("checkpoint") && json_desc["checkpoint"].IsNumber())
+            j->checkpoint_time=json_desc["checkpoint"].GetDouble();
+        /*
+        CHECK_F(json_desc.HasMember("dumptime"),"%s: job %s has no 'dumptime' field",
+                error_prefix.c_str(),j->id.to_string().c_str());
+        CHECK_F(json_desc["dumptime"].IsNumber(), "%s: job %s has non double 'dumptime' field",
+                error_prefix.c_str(),j->id.to_string().c_str());   */
+        if (json_desc.HasMember("dumptime") && json_desc["dumptime"].IsNumber())
+            j->dump_time=json_desc["dumptime"].GetDouble();
+        /*
+        CHECK_F(json_desc.HasMember("readtime"),"%s: job %s has no 'readtime' field",
+                error_prefix.c_str(),j->id.to_string().c_str());
+        CHECK_F(json_desc["readtime"].IsNumber(), "%s: job %s has non double 'readtime' field",
+                error_prefix.c_str(),j->id.to_string().c_str()); */
+        if (json_desc.HasMember("readtime") && json_desc["readtime"].IsNumber())
+            j->read_time=json_desc["readtime"].GetDouble();
+        
+        /*if (j->id.job_name().find("#")== std::string::npos)
+        {    
+            Document profile_doc;
+            profile_doc.Parse(j->profile->json_description.c_str());
+            
+            DelayProfileData * data =static_cast<DelayProfileData *>(j->profile->data);
+            double delay = data->delay;
+            int subtract = 0;
+            data->real_delay = delay;
+            if (std::fmod(delay,j->checkpoint_time) == 0)
+                subtract = 1;
+            delay = (floor(delay / j->checkpoint_time) - subtract )* j->dump_time + delay;
+            data->delay = delay;
+            j->profile->data = data;
+            profile_doc["delay"]=delay;
+            j->profile->json_description = Job::to_json_desc(& profile_doc);
+           LOG_F(INFO,"DEBUG delay: %f",delay);
+           LOG_F(INFO,"DEBUG profile data %f",(static_cast<DelayProfileData *>(j->profile->data))->delay);
+        }
+        }*/
+    
+    }
+    LOG_F(INFO,"Profile name %s and '%s'", profile_name.c_str(), j->profile->name.c_str());
+    // Let's get the JSON string which originally described the job
+    // (to conserve potential fields unused by Batsim)
+    rapidjson::StringBuffer buffer;
+    rapidjson::Writer<rapidjson::StringBuffer> writer(buffer);
+    json_desc.Accept(writer);
+
+    // Let's replace the job ID by its WLOAD!NUMBER counterpart if needed
+    // in the json raw description
+    std::string json_description_tmp(buffer.GetString(), buffer.GetSize());
+    /// @cond DOXYGEN_FAILS_PARSING_THIS_REGEX
+    std::regex r(R"("id"\s*:\s*(?:"*[^(,|})]*"*)\s*)");
+    /// @endcond
+    std::string replacement_str = "\"id\":\"" + j->id.to_string() + "\"";
+    // LOG_F(INFO,"Before regexp: %s", json_description_tmp.c_str());
+    j->json_description = std::regex_replace(json_description_tmp, r, replacement_str);
+
+    // Let's check that the new description is a valid JSON string
+    rapidjson::Document check_doc;
+    check_doc.Parse(j->json_description.c_str());
+    CHECK_F(!check_doc.HasParseError(),
+               "A problem occured when replacing the job_id by its WLOAD!job_name counterpart:"
+               "The output string '%s' is not valid JSON.", j->json_description.c_str());
+    CHECK_F(check_doc.IsObject(),
+               "A problem occured when replacing the job_id by its WLOAD!job_name counterpart: "
+               "The output string '%s' is not valid JSON.", j->json_description.c_str());
+    CHECK_F(check_doc.HasMember("id"),
+               "A problem occured when replacing the job_id by its WLOAD!job_name counterpart: "
+               "The output JSON '%s' has no 'id' field.", j->json_description.c_str());
+    CHECK_F(check_doc["id"].IsString(),
+               "A problem occured when replacing the job_id by its WLOAD!job_name counterpart: "
+               "The output JSON '%s' has a non-string 'id' field.", j->json_description.c_str());
+    CHECK_F(check_doc.HasMember("subtime") && check_doc["subtime"].IsNumber(),
+               "A problem occured when replacing the job_id by its WLOAD!job_name counterpart: "
+               "The output JSON '%s' has no 'subtime' field (or it is not a number)",
+               j->json_description.c_str());
+    CHECK_F((check_doc.HasMember("walltime") && check_doc["walltime"].IsNumber())
+               || (!check_doc.HasMember("walltime")),
+               "A problem occured when replacing the job_id by its WLOAD!job_name counterpart: "
+               "The output JSON '%s' has no 'walltime' field (or it is not a number)",
+               j->json_description.c_str());
+    CHECK_F(check_doc.HasMember("res") && check_doc["res"].IsInt(),
+               "A problem occured when replacing the job_id by its WLOAD!job_name counterpart: "
+               "The output JSON '%s' has no 'res' field (or it is not an integer)",
+               j->json_description.c_str());
+    CHECK_F(check_doc.HasMember("profile") && check_doc["profile"].IsString(),
+               "A problem occured when replacing the job_id by its WLOAD!job_name counterpart: "
+               "The output JSON '%s' has no 'profile' field (or it is not a string)",
+               j->json_description.c_str());
+
+    if (json_desc.HasMember("smpi_ranks_to_hosts_mapping"))
+    {
+        CHECK_F(json_desc["smpi_ranks_to_hosts_mapping"].IsArray(),
+                "%s: job '%s' has a non-array 'smpi_ranks_to_hosts_mapping' field",
+                error_prefix.c_str(), j->id.to_string().c_str());
+
+        const auto & mapping_array = json_desc["smpi_ranks_to_hosts_mapping"];
+        j->smpi_ranks_to_hosts_mapping.resize(mapping_array.Size());
+
+        for (unsigned int i = 0; i < mapping_array.Size(); ++i)
+        {
+            CHECK_F(mapping_array[i].IsInt(),
+                       "%s: job '%s' has a bad 'smpi_ranks_to_hosts_mapping' field: rank "
+                       "%d does not point to an integral number",
+                       error_prefix.c_str(), j->id.to_string().c_str(), i);
+            int host_number = mapping_array[i].GetInt();
+            CHECK_F(host_number >= 0 && static_cast<unsigned int>(host_number) < j->requested_nb_res,
+                       "%s: job '%s' has a bad 'smpi_ranks_to_hosts_mapping' field: rank "
+                       "%d has an invalid value %d : should be in [0,%d[",
+                       error_prefix.c_str(), j->id.to_string().c_str(),
+                       i, host_number, j->requested_nb_res);
+
+            j->smpi_ranks_to_hosts_mapping[i] = host_number;
+        }
+    }
+
+    LOG_F(INFO,"Job '%s' Loaded", j->id.to_string().c_str());
+    return j;
+}
+
+// Do NOT remove namespaces in the arguments (to avoid doxygen warnings)
+JobPtr Job::from_json(const std::string & json_str,
+                     Workload * workload,
+                     const std::string & error_prefix)
+{
+    
+    Document doc;
+    doc.Parse(json_str.c_str());
+    CHECK_F(!doc.HasParseError(),
+               "%s: Cannot be parsed. Content (between '##'):\n#%s#",
+               error_prefix.c_str(), json_str.c_str());
+
+    return Job::from_json(doc, workload, error_prefix);
+}
+int Jobs::nb_jobs() const
+{
+    return static_cast<int>(_jobs.size());
+}
+bool Jobs::exists(const JobIdentifier & job_id) const
+{
+    auto it = _jobs_met.find(job_id);
+    return it != _jobs_met.end();
+}
+std::unordered_map<JobIdentifier, JobPtr,JobIdentifierHasher> &Jobs::jobs()
+{
+    return _jobs;
+}
+
+JobPtr Jobs::operator[](JobIdentifier job_id)
+{
+    auto it = _jobs.find(job_id);
+    CHECK_F(it != _jobs.end(), "Cannot get job '%s': it does not exist",
+               job_id.to_cstring());
+    return it->second;
+}
+void Jobs::delete_job(const JobIdentifier & job_id, const bool & garbage_collect_profiles)
+{
+    CHECK_F(exists(job_id),
+               "Bad Jobs::delete_job call: The job with name='%s' does not exist.",
+               job_id.to_cstring());
+
+    std::string profile_name = _jobs[job_id]->profile->name;
+    _jobs.erase(job_id);
+    if (garbage_collect_profiles)
+    {
+        _workload->profiles->remove_profile(profile_name);
+    }
+}
+
+const JobPtr Jobs::operator[](JobIdentifier job_id) const
+{
+    auto it = _jobs.find(job_id);
+    CHECK_F(it != _jobs.end(), "Cannot get job '%s': it does not exist",
+               job_id.to_cstring());
+    return it->second;
+}
+
+JobPtr Jobs::at(JobIdentifier job_id)
+{
+    return operator[](job_id);
+}
+
+const JobPtr Jobs::at(JobIdentifier job_id) const
+{
+    return operator[](job_id);
+}
+}
+
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/batsched_job.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/batsched_job.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/batsched_job.hpp	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/batsched_job.hpp	2021-04-02 05:00:16.000000000 -0400
@@ -0,0 +1,210 @@
+#ifndef BATSCHED_JOB_HPP
+#define BATSCHED_JOB_HPP
+
+#include <unordered_map>
+#include <vector>
+#include <rapidjson/document.h>
+#include <intervalset.hpp>
+
+#include "pointers.hpp"
+
+
+namespace myBatsched{
+class Profiles;
+struct Profile;
+class Workload;
+struct Job;
+enum class JobState
+{
+     JOB_STATE_NOT_SUBMITTED                //!< The job exists but cannot be scheduled yet.
+    ,JOB_STATE_SUBMITTED                    //!< The job has been submitted, it can now be scheduled.
+    ,JOB_STATE_RUNNING                      //!< The job has been scheduled and is currently being processed.
+    ,JOB_STATE_COMPLETED_SUCCESSFULLY       //!< The job execution finished before its walltime successfully.
+    ,JOB_STATE_COMPLETED_FAILED             //!< The job execution finished before its walltime but the job failed.
+    ,JOB_STATE_COMPLETED_WALLTIME_REACHED   //!< The job has reached its walltime and has been killed.
+    ,JOB_STATE_COMPLETED_KILLED             //!< The job has been killed.
+    ,JOB_STATE_REJECTED                     //!< The job has been rejected by the scheduler.
+};
+
+class JobIdentifier
+{
+public:
+    
+    JobIdentifier() = default;
+    /**
+     * @brief Creates a JobIdentifier
+     * @param[in] workload_name The workload name
+     * @param[in] job_name The job name
+     */
+    explicit JobIdentifier(const std::string & workload_name,
+                           const std::string & job_name);
+
+    /**
+     * @brief Creates a JobIdentifier from a string to parse
+     * @param[in] job_id_str The string to parse
+     */
+    explicit JobIdentifier(const std::string & job_id_str);
+
+    std::string _workload_name;
+    std::string _job_name;
+    std::string _representation;
+
+    std::string representation() const;
+    std::string to_string() const;
+      /**
+     * @brief Returns a null-terminated C string of the JobIdentifier representation.
+     * @return A null-terminated C string of the JobIdentifier representation.
+     */
+    const char * to_cstring() const;
+    std::string workload_name() const;
+
+    /**
+     * @brief Returns the job name within the workload.
+     * @return The job name within the workload.
+     */
+    std::string job_name() const;
+    
+};
+/**
+ * @brief Compares two JobIdentifier thanks to their string representations
+ * @param[in] ji1 The first JobIdentifier
+ * @param[in] ji2 The second JobIdentifier
+ * @return ji1.to_string() < ji2.to_string()
+ */
+bool operator<(const JobIdentifier & ji1, const JobIdentifier & ji2);
+
+/**
+ * @brief Compares two JobIdentifier thanks to their string representations
+ * @param[in] ji1 The first JobIdentifier
+ * @param[in] ji2 The second JobIdentifier
+ * @return ji1.to_string() == ji2.to_string()
+ */
+bool operator==(const JobIdentifier & ji1, const JobIdentifier & ji2);
+struct JobIdentifierHasher
+{
+    /**
+     * @brief Hashes a JobIdentifier.
+     * @param[in] id The JobIdentifier to hash.
+     * @return Whatever is returned by std::hash to match C++ conventions.
+     */
+    std::size_t operator()(const JobIdentifier & id) const;
+};
+struct Job
+{
+        Job()=default;
+        
+        
+        
+        Workload * workload = nullptr;
+        JobIdentifier id;
+        std::string json_description;
+        IntervalSet allocation;
+        std::string metadata;
+        std::vector<int> smpi_ranks_to_hosts_mapping; //!< If the job uses a SMPI profile, stores which host number each MPI rank should use. These numbers must be in [0,required_nb_res[.
+        
+        JobState state;
+        long double starting_time;
+        long double runtime;
+        bool kill_requested = false;
+        ProfilePtr profile;
+        long double submission_time;
+        long double walltime=-1;
+        unsigned int requested_nb_res;
+        double checkpoint_time;
+        double dump_time;
+        double read_time;
+public:
+    static JobPtr from_json(const rapidjson::Value & json_desc,
+                           Workload * workload,
+                           const std::string & error_prefix = "Invalid JSON job");
+
+    /**
+     * @brief Creates a new-allocated Job from a JSON description
+     * @param[in] json_str The JSON description of the job (as a string)
+     * @param[in] workload The Workload the job is in
+     * @param[in] error_prefix The prefix to display when an error occurs
+     * @return The newly allocated Job
+     * @pre The JSON description of the job is valid
+     */
+    static JobPtr from_json(const std::string & json_str,
+                           Workload * workload,
+                           const std::string & error_prefix = "Invalid JSON job");
+    /**
+     * @brief Checks whether a job is complete (regardless of the job success)
+     * @return true if the job is complete (=has started then finished), false otherwise.
+     */
+    bool is_complete() const;
+    
+    static std::string to_json_desc(rapidjson::Document * doc);
+    static std::string not_escaped(const std::string & input);
+
+    //! Functor to hash a JobIdentifier
+    
+    
+};
+
+class Jobs
+{
+public:
+    
+    Jobs() = default;
+    ~Jobs();
+public:    
+    void set_profiles(Profiles * profiles);
+    void set_workload(Workload * workload);
+    void load_from_json(const rapidjson::Document & doc, const std::string & filename);
+    void add_job(JobPtr job);
+    int nb_jobs() const;
+    bool exists(const JobIdentifier & job_id) const;
+       /**
+     * @brief Returns a reference to the map that contains the jobs
+     * @return A reference to the map that contains the jobs
+     */
+    std::unordered_map<JobIdentifier, JobPtr,JobIdentifierHasher> & jobs();
+        /**
+     * @brief Accesses one job thanks to its identifier
+     * @param[in] job_id The job id
+     * @return A pointer to the job associated to the given job id
+     */
+    JobPtr operator[](JobIdentifier job_id);
+
+    /**
+     * @brief Accesses one job thanks to its unique name (const version)
+     * @param[in] job_id The job id
+     * @return A (const) pointer to the job associated to the given job id
+     */
+    const JobPtr operator[](JobIdentifier job_id) const;
+
+    /**
+     * @brief Accesses one job thanks to its unique id
+     * @param[in] job_id The job unique id
+     * @return A pointer to the job associated to the given job id
+     */
+    JobPtr at(JobIdentifier job_id);
+
+    /**
+     * @brief Accesses one job thanks to its unique id (const version)
+     * @param[in] job_id The job unique name
+     * @return A (const) pointer to the job associated to the given job
+     * name
+     */
+    const JobPtr at(JobIdentifier job_id) const;
+   /**
+     * @brief Deletes a job
+     * @param[in] job_id The identifier of the job to delete
+     * @param[in] garbage_collect_profiles Whether to garbage collect its profiles
+     */
+    void delete_job(const JobIdentifier & job_id,
+                    const bool & garbage_collect_profiles);
+
+private:
+    std::unordered_map<JobIdentifier,JobPtr,JobIdentifierHasher> _jobs;
+    std::unordered_map<JobIdentifier, bool, JobIdentifierHasher> _jobs_met; //!< Stores the jobs id already met during the simulation
+    Profiles * _profiles = nullptr;
+    Workload * _workload=nullptr;
+    
+    
+};
+}
+
+#endif
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/batsched_profile.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/batsched_profile.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/batsched_profile.cpp	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/batsched_profile.cpp	2021-04-02 05:00:13.000000000 -0400
@@ -0,0 +1,739 @@
+
+#include <fstream>
+#include <iostream>
+#include <filesystem>
+
+#include <boost/algorithm/string.hpp>
+#include <loguru.hpp>
+
+#include <rapidjson/document.h>
+#include <rapidjson/writer.h>
+#include <rapidjson/stringbuffer.h>
+#include "pointers.hpp"
+#include "batsched_profile.hpp"
+
+using namespace rapidjson;
+namespace fs = std::filesystem;
+namespace myBatsched{
+Profile::~Profile()
+{
+    LOG_F(INFO,"Profile '%s' is being deleted.", name.c_str());
+    if (type == ProfileType::DELAY)
+    {
+        auto * d = static_cast<DelayProfileData *>(data);
+        if (d != nullptr)
+        {
+            delete d;
+            d = nullptr;
+            
+        }
+    }
+    else if (type == ProfileType::PARALLEL)
+    {
+        auto * d = static_cast<ParallelProfileData *>(data);
+        if (d != nullptr)
+        {
+            delete d;
+            d = nullptr;
+        }
+    }
+    else if (type == ProfileType::PARALLEL_HOMOGENEOUS)
+    {
+        auto * d = static_cast<ParallelHomogeneousProfileData *>(data);
+        if (d != nullptr)
+        {
+            delete d;
+            d = nullptr;
+        }
+    }
+    else if (type == ProfileType::PARALLEL_HOMOGENEOUS_TOTAL_AMOUNT)
+    {
+        auto * d = static_cast<ParallelHomogeneousTotalAmountProfileData *>(data);
+        if (d != nullptr)
+        {
+            delete d;
+            d = nullptr;
+        }
+    }
+    else if (type == ProfileType::SMPI)
+    {
+        auto * d = static_cast<SmpiProfileData *>(data);
+        if (d != nullptr)
+        {
+            delete d;
+            d = nullptr;
+        }
+    }
+    else if (type == ProfileType::SEQUENCE)
+    {
+        auto * d = static_cast<SequenceProfileData *>(data);
+        if (d != nullptr)
+        {
+            delete d;
+            d = nullptr;
+        }
+    }
+    else if (type == ProfileType::PARALLEL_HOMOGENEOUS_PFS)
+    {
+        auto * d = static_cast<ParallelHomogeneousPFSProfileData *>(data);
+        if (d != nullptr)
+        {
+            delete d;
+            d = nullptr;
+        }
+    }
+    else if (type == ProfileType::DATA_STAGING)
+    {
+        auto * d = static_cast<DataStagingProfileData *>(data);
+        if (d != nullptr)
+        {
+            delete d;
+            d = nullptr;
+        }
+    }
+    else if (type == ProfileType::SCHEDULER_SEND)
+    {
+        auto * d = static_cast<SchedulerSendProfileData *>(data);
+        if (d != nullptr)
+        {
+            delete d;
+            d = nullptr;
+        }
+    }
+    else if (type == ProfileType::SCHEDULER_RECV)
+    {
+        auto * d = static_cast<SchedulerRecvProfileData *>(data);
+        if (d != nullptr)
+        {
+            delete d;
+            d = nullptr;
+        }
+    }
+    else
+    {
+        LOG_F(INFO,"Deletion of an unknown profile type (%d)", static_cast<int>(type));
+    }
+}
+ParallelProfileData::~ParallelProfileData()
+{
+    if (cpu != nullptr)
+    {
+        delete[] cpu;
+        cpu = nullptr;
+    }
+    if (com != nullptr)
+    {
+        delete[] com;
+        com = nullptr;
+    }
+}
+// Do NOT remove namespaces in the arguments (to avoid doxygen warnings)
+ProfilePtr Profile::from_json(const std::string & profile_name,
+                            const rapidjson::Value & json_desc,
+                            const std::string & error_prefix,
+                            bool is_from_a_file,
+                            const std::string & json_filename)
+{
+    (void) error_prefix; // Avoids a warning if assertions are ignored
+
+    auto profile = std::make_shared<Profile>();
+    profile->name = profile_name;
+
+    CHECK_F(json_desc.IsObject(), "%s: profile '%s' value must be an object",
+               error_prefix.c_str(), profile_name.c_str());
+    CHECK_F(json_desc.HasMember("type"), "%s: profile '%s' has no 'type' field",
+               error_prefix.c_str(), profile_name.c_str());
+    CHECK_F(json_desc["type"].IsString(), "%s: profile '%s' has a non-string 'type' field",
+               error_prefix.c_str(), profile_name.c_str());
+
+    std::string profile_type = json_desc["type"].GetString();
+
+    int return_code = 0;
+    if (json_desc.HasMember("ret"))
+    {
+        return_code = json_desc["ret"].GetInt();
+    }
+    profile->return_code = return_code;
+
+    if (profile_type == "delay")
+    {
+        /*
+        {
+            "type": "delay",
+            "delay": 20.20
+        }
+        */
+        
+        profile->type = ProfileType::DELAY;
+        DelayProfileData * data = new DelayProfileData;
+        
+        CHECK_F(json_desc.HasMember("delay"), "%s: profile '%s' has no 'delay' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(json_desc["delay"].IsNumber(), "%s: profile '%s' has a non-number 'delay' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        
+        data->delay = json_desc["delay"].GetDouble();
+        
+        if (json_desc.HasMember("original_delay")){
+            data->real_delay = json_desc["original_delay"].GetDouble();
+        }
+        else{
+            data->real_delay = data->delay;
+        }
+        
+        CHECK_F(data->delay > 0, "%s: profile '%s' has a non-strictly-positive 'delay' field (%g)",
+                   error_prefix.c_str(), profile_name.c_str(), data->delay);
+        LOG_F(INFO,"if profile_type 6");
+        profile->data = data;
+        LOG_F(INFO,"if profile_type 7");
+    }
+    else if (profile_type == "parallel")
+    {
+        /*
+        {
+            "type": "parallel",
+            "cpu": [5e6,  0,  0,  0],
+            "com": [5e6,  0,  0,  0,
+                    5e6,5e6,  0,  0,
+                    5e6,5e6,  0,  0,
+                    5e6,5e6,5e6,  0]
+        }
+        */
+        profile->type = ProfileType::PARALLEL;
+        ParallelProfileData * data = new ParallelProfileData;
+
+        // basic checks
+        CHECK_F(json_desc.HasMember("cpu"), "%s: profile '%s' has no 'cpu' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(json_desc.HasMember("com"), "%s: profile '%s' has no 'com' field",
+                   error_prefix.c_str(), profile_name.c_str());
+
+        // get and check CPU vector
+        const Value & cpu = json_desc["cpu"];
+        CHECK_F(cpu.IsArray(), "%s: profile '%s' has a non-array 'cpu' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(cpu.Size() > 0, "%s: profile '%s' has an invalid-sized array 'cpu' (size=%d): "
+                   "must be strictly positive",
+                   error_prefix.c_str(), profile_name.c_str(), cpu.Size());
+
+        data->nb_res = cpu.Size();
+        data->cpu = new double[data->nb_res];
+        for (unsigned int i = 0; i < cpu.Size(); ++i)
+        {
+            CHECK_F(cpu[i].IsNumber(), "%s: profile '%s' computation array is invalid: all "
+                       "elements must be numbers", error_prefix.c_str(), profile_name.c_str());
+            data->cpu[i] = cpu[i].GetDouble();
+            CHECK_F(data->cpu[i] >= 0, "%s: profile '%s' computation array is invalid: all "
+                       "elements must be non-negative", error_prefix.c_str(), profile_name.c_str());
+        }
+
+        // get and check Comm vector
+        const Value & com = json_desc["com"];
+        CHECK_F(com.IsArray(), "%s: profile '%s' has a non-array 'com' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(com.Size() == data->nb_res * data->nb_res, "%s: profile '%s' is incoherent: "
+                   "com array has size %d whereas the required array size is %d",
+                   error_prefix.c_str(), profile_name.c_str(), com.Size(), data->nb_res * data->nb_res);
+
+        data->com = new double[data->nb_res * data->nb_res];
+        for (unsigned int i = 0; i < com.Size(); ++i)
+        {
+            CHECK_F(com[i].IsNumber(), "%s: profile '%s' communication array is invalid: all "
+                       "elements must be numbers", error_prefix.c_str(), profile_name.c_str());
+            data->com[i] = com[i].GetDouble();
+            CHECK_F(data->com[i] >= 0, "%s: profile '%s' communication array is invalid: all "
+                       "elements must be non-negative", error_prefix.c_str(), profile_name.c_str());
+        }
+
+        profile->data = data;
+    }
+    else if (profile_type == "parallel_homogeneous")
+    {
+        /*
+        {
+            "type": "parallel_homogeneous",
+            "cpu": 10e6,
+            "com": 1e6
+        }
+        */
+        profile->type = ProfileType::PARALLEL_HOMOGENEOUS;
+        ParallelHomogeneousProfileData * data = new ParallelHomogeneousProfileData;
+
+        CHECK_F(json_desc.HasMember("cpu"), "%s: profile '%s' has no 'cpu' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(json_desc["cpu"].IsNumber(), "%s: profile '%s' has a non-number 'cpu' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        data->cpu = json_desc["cpu"].GetDouble();
+        CHECK_F(data->cpu >= 0, "%s: profile '%s' has a non-positive 'cpu' field (%g)",
+                   error_prefix.c_str(), profile_name.c_str(), data->cpu);
+
+        CHECK_F(json_desc.HasMember("com"), "%s: profile '%s' has no 'com' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(json_desc["com"].IsNumber(), "%s: profile '%s' has a non-number 'com' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        data->com = json_desc["com"].GetDouble();
+        CHECK_F(data->com >= 0, "%s: profile '%s' has a non-positive 'com' field (%g)",
+                   error_prefix.c_str(), profile_name.c_str(), data->com);
+
+        profile->data = data;
+    }
+    else if (profile_type == "parallel_homogeneous_total")
+    {
+        /*
+        {
+            "type": "parallel_homogeneous_total",
+            "cpu": 10e6,
+            "com": 1e6
+        }
+        */
+        profile->type = ProfileType::PARALLEL_HOMOGENEOUS_TOTAL_AMOUNT;
+        ParallelHomogeneousTotalAmountProfileData * data = new ParallelHomogeneousTotalAmountProfileData;
+
+        CHECK_F(json_desc.HasMember("cpu"), "%s: profile '%s' has no 'cpu' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(json_desc["cpu"].IsNumber(), "%s: profile '%s' has a non-number 'cpu' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        data->cpu = json_desc["cpu"].GetDouble();
+        CHECK_F(data->cpu >= 0, "%s: profile '%s' has a non-positive 'cpu' field (%g)",
+                   error_prefix.c_str(), profile_name.c_str(), data->cpu);
+
+        CHECK_F(json_desc.HasMember("com"), "%s: profile '%s' has no 'com' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(json_desc["com"].IsNumber(), "%s: profile '%s' has a non-number 'com' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        data->com = json_desc["com"].GetDouble();
+        CHECK_F(data->com >= 0, "%s: profile '%s' has a non-positive 'com' field (%g)",
+                   error_prefix.c_str(), profile_name.c_str(), data->com);
+
+        profile->data = data;
+    }
+    else if (profile_type == "composed")
+    {
+        /*
+        {
+            "type": "composed",
+            "repeat" : 4,
+            "seq": ["simple","homogeneous","simple"]
+        }
+        */
+        profile->type = ProfileType::SEQUENCE;
+        SequenceProfileData * data = new SequenceProfileData;
+
+        unsigned int repeat = 1;
+        if (json_desc.HasMember("repeat"))
+        {
+            CHECK_F(json_desc["repeat"].IsInt(), "%s: profile '%s' has a non-integral 'repeat' field",
+                   error_prefix.c_str(), profile_name.c_str());
+            CHECK_F(json_desc["repeat"].GetInt() >= 0, "%s: profile '%s' has a negative 'repeat' field (%d)",
+                   error_prefix.c_str(), profile_name.c_str(), json_desc["repeat"].GetInt());
+            repeat = static_cast<unsigned int>(json_desc["repeat"].GetInt());
+        }
+        data->repeat = repeat;
+
+        CHECK_F(data->repeat > 0, "%s: profile '%s' has a non-strictly-positive 'repeat' field (%d)",
+                   error_prefix.c_str(), profile_name.c_str(), data->repeat);
+
+        CHECK_F(json_desc.HasMember("seq"), "%s: profile '%s' has no 'seq' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(json_desc["seq"].IsArray(), "%s: profile '%s' has a non-array 'seq' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        const Value & seq = json_desc["seq"];
+        CHECK_F(seq.Size() > 0, "%s: profile '%s' has an invalid array 'seq': its size must be "
+                   "strictly positive", error_prefix.c_str(), profile_name.c_str());
+        data->sequence.reserve(seq.Size());
+        for (unsigned int i = 0; i < seq.Size(); ++i)
+        {
+            data->sequence.push_back(std::string(seq[i].GetString()));
+        }
+
+        profile->data = data;
+    }
+    else if (profile_type == "parallel_homogeneous_pfs")
+    {
+        /*
+        {
+            "type": "parallel_homogeneous_pfs",
+            "bytes_to_read": 10e5,
+            "bytes_to_write": 10e5,
+            "storage": "my_io_node" //optional (default: 'pfs')
+        }
+        */
+        profile->type = ProfileType::PARALLEL_HOMOGENEOUS_PFS;
+        ParallelHomogeneousPFSProfileData * data = new ParallelHomogeneousPFSProfileData;
+
+        CHECK_F(json_desc.HasMember("bytes_to_read") or json_desc.HasMember("bytes_to_write"), "%s: profile '%s' has no 'bytes_to_read' or 'bytes_to_write' field (0 if not set)",
+                   error_prefix.c_str(), profile_name.c_str());
+        if (json_desc.HasMember("bytes_to_read"))
+        {
+            CHECK_F(json_desc["bytes_to_read"].IsNumber(), "%s: profile '%s' has a non-number 'bytes_to_read' field",
+                       error_prefix.c_str(), profile_name.c_str());
+            data->bytes_to_read = json_desc["bytes_to_read"].GetDouble();
+        }
+        if (json_desc.HasMember("bytes_to_write"))
+        {
+            CHECK_F(json_desc["bytes_to_write"].IsNumber(), "%s: profile '%s' has a non-number 'bytes_to_write' field",
+                       error_prefix.c_str(), profile_name.c_str());
+            data->bytes_to_write = json_desc["bytes_to_write"].GetDouble();
+        }
+
+        // If not set Use the "pfs" label by default
+        if (json_desc.HasMember("storage") or json_desc.HasMember("host"))
+        {
+            std::string key;
+            if (json_desc.HasMember("storage"))
+            {
+                key = "storage";
+            }
+            else
+            {
+                key = "host";
+
+            }
+            CHECK_F(json_desc[key.c_str()].IsString(),
+                           "%s: profile '%s' has a non-string '%s' field",
+                           error_prefix.c_str(), profile_name.c_str(),
+                           key.c_str());
+            data->storage_label = json_desc[key.c_str()].GetString();
+        }
+
+        profile->data = data;
+    }
+    else if (profile_type == "data_staging")
+    {
+        /*
+        {
+            "type": "data_staging",
+            "nb_bytes": 10e5,
+            "from": "pfs",
+            "to": "lcfs"
+        }
+        */
+        profile->type = ProfileType::DATA_STAGING;
+        DataStagingProfileData * data = new DataStagingProfileData;
+
+        CHECK_F(json_desc.HasMember("nb_bytes"), "%s: profile '%s' has no 'nb_bytes' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(json_desc["nb_bytes"].IsNumber(), "%s: profile '%s' has a non-number 'nb_bytes' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        data->nb_bytes = json_desc["nb_bytes"].GetDouble();
+        CHECK_F(data->nb_bytes >= 0, "%s: profile '%s' has a non-positive 'nb_bytes' field (%g)",
+                   error_prefix.c_str(), profile_name.c_str(), data->nb_bytes);
+
+        CHECK_F(json_desc.HasMember("from"), "%s: profile '%s' has no 'from' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(json_desc["from"].IsString(),
+                   "%s: profile '%s' has a non-string 'from' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        data->from_storage_label = json_desc["from"].GetString();
+
+        CHECK_F(json_desc.HasMember("to"), "%s: profile '%s' has no 'to' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(json_desc["to"].IsString(),
+                   "%s: profile '%s' has a non-string 'to' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        data->to_storage_label = json_desc["to"].GetString();
+
+        profile->data = data;
+    }
+    else if (profile_type == "send")
+    {
+        profile->type = ProfileType::SCHEDULER_SEND;
+        SchedulerSendProfileData * data = new SchedulerSendProfileData;
+
+        CHECK_F(json_desc.HasMember("msg"), "%s: profile '%s' has no 'msg' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(json_desc["msg"].IsObject(), "%s: profile '%s' field 'msg' is no object",
+                   error_prefix.c_str(), profile_name.c_str());
+
+        data->message.CopyFrom(json_desc["msg"], data->message.GetAllocator());
+
+        if (json_desc.HasMember("sleeptime"))
+        {
+            CHECK_F(json_desc["sleeptime"].IsNumber(),
+                       "%s: profile '%s' has a non-number 'sleeptime' field",
+                       error_prefix.c_str(), profile_name.c_str());
+            data->sleeptime = json_desc["sleeptime"].GetDouble();
+            CHECK_F(data->sleeptime > 0,
+                       "%s: profile '%s' has a non-positive 'sleeptime' field (%g)",
+                       error_prefix.c_str(), profile_name.c_str(), data->sleeptime);
+        }
+        else
+        {
+            data->sleeptime = 0.0000001;
+        }
+        profile->data = data;
+    }
+    else if (profile_type == "recv")
+    {
+        profile->type = ProfileType::SCHEDULER_RECV;
+        SchedulerRecvProfileData * data = new SchedulerRecvProfileData;
+
+        data->regex =std:: string(".*");
+        if (json_desc.HasMember("regex"))
+        {
+            data->regex = json_desc["regex"].GetString();
+        }
+
+        data->on_success = std::string("");
+        if (json_desc.HasMember("success"))
+        {
+            data->on_success = json_desc["success"].GetString();
+        }
+
+        data->on_failure = std::string("");
+        if (json_desc.HasMember("failure"))
+        {
+            data->on_failure = json_desc["failure"].GetString();
+        }
+
+        data->on_timeout = std::string("");
+        if (json_desc.HasMember("timeout"))
+        {
+            data->on_timeout = json_desc["timeout"].GetString();
+        }
+
+        if (json_desc.HasMember("polltime"))
+        {
+            CHECK_F(json_desc["polltime"].IsNumber(),
+                       "%s: profile '%s' has a non-number 'polltime' field",
+                       error_prefix.c_str(), profile_name.c_str());
+            data->polltime = json_desc["polltime"].GetDouble();
+            CHECK_F(data->polltime > 0,
+                       "%s: profile '%s' has a non-positive 'polltime' field (%g)",
+                       error_prefix.c_str(), profile_name.c_str(), data->polltime);
+        }
+        else
+        {
+            data->polltime = 0.005;
+        }
+        profile->data = data;
+    }
+    else if (profile_type == "smpi")
+    {
+        profile->type = ProfileType::SMPI;
+        SmpiProfileData * data = new SmpiProfileData;
+
+        CHECK_F(json_desc.HasMember("trace"), "%s: profile '%s' has no 'trace' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        CHECK_F(json_desc["trace"].IsString(), "%s: profile '%s' has a non-string 'trace' field",
+                   error_prefix.c_str(), profile_name.c_str());
+        const std::string trace_filename = json_desc["trace"].GetString();
+
+        CHECK_F(is_from_a_file, "Trying to create a SMPI profile from another source than "
+                   "a file workload, which is not implemented at the moment.");
+        (void) is_from_a_file; // Avoids a warning if assertions are ignored
+
+        fs::path base_dir = json_filename;
+        base_dir = base_dir.parent_path();
+        LOG_F(INFO,"base_dir = '%s'", base_dir.string().c_str());
+        CHECK_F(fs::exists(base_dir) && fs::is_directory(base_dir));
+
+        //LOG_F(INFO,"base_dir = '%s'", base_dir.string().c_str());
+        //LOG_F(INFO,"trace = '%s'", trace.c_str());
+        fs::path trace_path = base_dir.string() + "/" + trace_filename;
+        //LOG_F(INFO,"trace_path = '%s'", trace_path.string().c_str());
+        CHECK_F(fs::exists(trace_path) && fs::is_regular_file(trace_path),
+                   "Invalid JSON: profile '%s' has an invalid 'trace' field ('%s'), which leads to a non-existent file ('%s')",
+                   profile_name.c_str(), trace_filename.c_str(), trace_path.string().c_str());
+
+        std::ifstream trace_file(trace_path.string());
+        CHECK_F(trace_file.is_open(), "Cannot open file '%s'", trace_path.string().c_str());
+
+        std::string line;
+        while (std::getline(trace_file, line))
+        {
+            boost::trim_right(line);
+            fs::path rank_trace_path = trace_path.parent_path().string() + "/" + line;
+            data->trace_filenames.push_back(rank_trace_path.string());
+        }
+
+        std::string filenames = boost::algorithm::join(data->trace_filenames, ", ");
+        LOG_F(INFO,"Filenames of profile '%s': [%s]", profile_name.c_str(), filenames.c_str());
+
+        profile->data = data;
+    }
+    else
+    {
+        ABORT_F("Cannot create the profile '%s' of unknown type '%s'",
+                profile_name.c_str(), profile_type.c_str());
+    }
+
+
+    // Let's get the JSON string which describes the profile (to conserve potential fields unused by Batsim)
+    rapidjson::StringBuffer buffer;
+    rapidjson::Writer<rapidjson::StringBuffer> writer(buffer);
+    json_desc.Accept(writer);
+    profile->json_description = std::string(buffer.GetString(), buffer.GetSize());
+
+    return profile;
+}
+
+// Do NOT remove namespaces in the arguments (to avoid doxygen warnings)
+ProfilePtr Profile::from_json(const std::string & profile_name,
+                            const std::string & json_str,
+                            const std::string & error_prefix)
+{
+    Document doc;
+    doc.Parse(json_str.c_str());
+    CHECK_F(!doc.HasParseError(), "%s: Cannot be parsed. Content (between '##'):\n#%s#",
+               error_prefix.c_str(), json_str.c_str());
+
+    return Profile::from_json(profile_name, doc, error_prefix, false);
+}
+
+bool Profile::is_parallel_task() const
+{
+    return (type == ProfileType::PARALLEL) ||
+           (type == ProfileType::PARALLEL_HOMOGENEOUS) ||
+           (type == ProfileType::PARALLEL_HOMOGENEOUS_TOTAL_AMOUNT) ||
+           (type == ProfileType::PARALLEL_HOMOGENEOUS_PFS) ||
+           (type == ProfileType::DATA_STAGING);
+}
+
+
+std::string profile_type_to_string(const ProfileType & type)
+{
+    std::string str;
+
+    switch(type)
+    {
+    case ProfileType::DELAY:
+        str = "DELAY";
+        break;
+    case ProfileType::PARALLEL:
+        str = "PARALLEL";
+        break;
+    case ProfileType::PARALLEL_HOMOGENEOUS:
+        str = "PARALLEL_HOMOGENEOUS";
+        break;
+    case ProfileType::PARALLEL_HOMOGENEOUS_TOTAL_AMOUNT:
+        str = "PARALLEL_HOMOGENEOUS_TOTAL_AMOUNT";
+        break;
+    case ProfileType::SMPI:
+        str = "SMPI";
+        break;
+    case ProfileType::SEQUENCE:
+        str = "SEQUENCE";
+        break;
+    case ProfileType::PARALLEL_HOMOGENEOUS_PFS:
+        str = "PARALLEL_HOMOGENEOUS_PFS";
+        break;
+    case ProfileType::DATA_STAGING:
+        str = "DATA_STAGING";
+        break;
+    case ProfileType::SCHEDULER_SEND:
+        str = "SCHEDULER_SEND";
+        break;
+    case ProfileType::SCHEDULER_RECV:
+        str = "SCHEDULER_RECV";
+        break;
+    default:
+        str = "unset";
+        break;
+    }
+
+    return str;
+}
+void Profiles::add_profile(ProfilePtr profile)
+{
+    _profiles[profile->name]=profile;
+}
+void Profiles::load_from_json(const Document &doc, const std::string & filename)
+{
+    std::string error_prefix = "Invalid JSON file '" + filename + "'";
+   
+    CHECK_F(doc.IsObject(), "%s: not a JSON object", error_prefix.c_str());
+    
+    CHECK_F(doc.HasMember("profiles"), "%s: the 'profiles' object is missing",
+               error_prefix.c_str());
+    
+    const Value & profiles = doc["profiles"];
+    
+    CHECK_F(profiles.IsObject(), "%s: the 'profiles' member is not an object",
+               error_prefix.c_str());
+    
+    for (Value::ConstMemberIterator it = profiles.MemberBegin(); it != profiles.MemberEnd(); ++it)
+    {
+        const Value & key = it->name;
+        const Value & value = it->value;
+       
+        CHECK_F(key.IsString(), "%s: all children of the 'profiles' object must have a "
+                   "string key", error_prefix.c_str());
+        
+        std::string profile_name = key.GetString();
+        
+        auto profile = Profile::from_json(profile_name, value, error_prefix, true, filename);
+        
+        CHECK_F(!exists(std::string(key.GetString())), "%s: duplication of profile name '%s'",
+                   error_prefix.c_str(), key.GetString());
+        
+        _profiles[std::string(key.GetString())] = profile;
+    }
+}
+const std::unordered_map<std::string, ProfilePtr> Profiles::profiles() const
+{
+    return _profiles;
+}
+ProfilePtr Profiles::operator[](const std::string &profile_name)
+{
+    auto mit = _profiles.find(profile_name);
+    CHECK_F(mit != _profiles.end(), "Cannot get profile '%s': it does not exist", profile_name.c_str());
+    CHECK_F(mit->second.get() != nullptr, "Cannot get profile '%s': it existed some time ago but is no longer accessible", profile_name.c_str());
+    return mit->second;
+}
+
+const ProfilePtr Profiles::operator[](const std::string &profile_name) const
+{
+    auto mit = _profiles.find(profile_name);
+    CHECK_F(mit != _profiles.end(), "Cannot get profile '%s': it does not exist", profile_name.c_str());
+    CHECK_F(mit->second.get() != nullptr, "Cannot get profile '%s': it existed some time ago but is no longer accessible", profile_name.c_str());
+    return mit->second;
+}
+ProfilePtr Profiles::at(const std::string & profile_name)
+{
+    return operator[](profile_name);
+}
+
+const ProfilePtr Profiles::at(const std::string & profile_name) const
+{
+    return operator[](profile_name);
+}
+int Profiles::nb_profiles() const
+{
+    return static_cast<int>(_profiles.size());
+}
+bool Profiles::exists(const std::string &profile_name) const
+{
+    auto mit = _profiles.find(profile_name);
+    return mit != _profiles.end();
+}
+Profiles::~Profiles()
+{
+    _profiles.clear();
+}
+void Profiles::remove_profile(const std::string & profile_name)
+{
+    auto mit = _profiles.find(profile_name);
+    CHECK_F(mit != _profiles.end(), "Bad Profiles::remove_profile call: Profile with name='%s' never existed in this workload.", profile_name.c_str());
+
+    // If the profile has aleady been removed, do nothing.
+    if (mit->second.get() == nullptr)
+    {
+        return;
+    }
+
+    // If the profile is composed, also remove links to subprofiles.
+    if (mit->second->type == ProfileType::SEQUENCE)
+    {
+        auto * profile_data = static_cast<SequenceProfileData*>(mit->second->data);
+        for (const auto & subprofile : profile_data->profile_sequence)
+        {
+            remove_profile(subprofile->name);
+        }
+    }
+
+    // Discard link to the profile (implicit memory clean-up)
+    mit->second = nullptr;
+}
+
+}
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/batsched_profile.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/batsched_profile.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/batsched_profile.hpp	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/batsched_profile.hpp	2021-04-02 05:00:15.000000000 -0400
@@ -0,0 +1,216 @@
+#ifndef BATSCHED_PROFILE_HPP
+#define BATSCHED_PROFILE_HPP
+
+#include <string>
+#include <unordered_map>
+#include <vector>
+#include <memory>
+
+#include <rapidjson/document.h>
+
+#include "pointers.hpp"
+
+namespace myBatsched{
+
+enum class ProfileType
+{
+    UNSET
+    ,DELAY                                     //!< a delay. Its data is of type DelayProfileData
+    ,PARALLEL                                  //!< composed of a computation vector and a communication matrix. Its data is of type ParallelProfileData
+    ,PARALLEL_HOMOGENEOUS                      //!< a homogeneous parallel task that executes the given amounts of computation and communication on every node. Its data is of type ParallelHomogeneousProfileData
+    ,PARALLEL_HOMOGENEOUS_TOTAL_AMOUNT         //!< a homogeneous parallel task that spreads the given amounts of computation and communication among all the nodes. Its data is of type ParallelHomogeneousTotalAmountProfileData
+    ,SMPI                                      //!< a SimGrid MPI time-independent trace. Its data is of type SmpiProfileData
+    ,SEQUENCE                                  //!< non-atomic: it is composed of a sequence of other profiles
+    ,PARALLEL_HOMOGENEOUS_PFS                  //!< Read and writes data to a PFS storage nodes. data type ParallelHomogeneousPFSProfileData
+    ,DATA_STAGING                              //!< for moving data between the pfs hosts. Its data is of type DataStagingProfileData
+    ,SCHEDULER_SEND                            //!< a profile simulating a message sent to the scheduler. Its data is of type SchedulerSendProfileData
+    ,SCHEDULER_RECV                            //!< receives a message from the scheduler and can execute a profile based on a value comparison of the message. Its data is of type SchedulerRecvProfileData
+};
+struct Profile{
+    Profile() = default;
+    ~Profile();
+    ProfileType type;
+    void * data;
+    std::string json_description;
+    std::string name;
+    int return_code=0;
+
+    
+    
+        /**
+     * @brief Creates a new-allocated Profile from a JSON description
+     * @param[in] profile_name The name of the profile
+     * @param[in] json_desc The JSON description
+     * @param[in] json_filename The JSON file name
+     * @param[in] is_from_a_file Whether the JSON job comes from a file
+     * @param[in] error_prefix The prefix to display when an error occurs
+     * @return The new-allocated Profile
+     * @pre The JSON description is valid
+     */
+    static ProfilePtr from_json(const std::string & profile_name,
+                               const rapidjson::Value & json_desc,
+                               const std::string & error_prefix = "Invalid JSON profile",
+                               bool is_from_a_file = true,
+                               const std::string & json_filename = "unset");
+
+    /**
+     * @brief Creates a new-allocated Profile from a JSON description
+     * @param[in] profile_name The name of the profile
+     * @param[in] json_str The JSON description (as a string)
+     * @param[in] error_prefix The prefix to display when an error occurs
+     * @return The new-allocated Profile
+     * @pre The JSON description is valid
+     */
+    static ProfilePtr from_json(const std::string & profile_name,
+                               const std::string & json_str,
+                               const std::string & error_prefix = "Invalid JSON profile");
+
+    bool is_parallel_task() const;
+};
+/**
+ * @brief The data associated to PARALLEL profiles
+ */
+struct ParallelProfileData
+{
+    ParallelProfileData() = default;
+
+    /**
+     * @brief Destroys a ParallelProfileData
+     * @details This method cleans the cpu and comm arrays from the memory if they are not set to nullptr
+     */
+    ~ParallelProfileData();
+
+    unsigned int nb_res;    //!< The number of resources
+    double * cpu = nullptr; //!< The computation vector
+    double * com = nullptr; //!< The communication matrix
+};
+
+/**
+ * @brief The data associated to PARALLEL_HOMOGENEOUS profiles
+ */
+struct ParallelHomogeneousProfileData
+{
+    double cpu; //!< The computation amount on each node
+    double com; //!< The communication amount between each pair of nodes
+};
+
+/**
+ * @brief The data associated to PARALLEL_HOMOGENEOUS_TOTAL_AMOUNT profiles
+ */
+struct ParallelHomogeneousTotalAmountProfileData
+{
+    double cpu; //!< The computation amount to spread over the nodes
+    double com; //!< The communication amount to spread over each pair of nodes
+};
+/**
+ * @brief The data associated to DELAY profiles
+ */
+struct DelayProfileData
+{
+    double delay; //!< The time amount, in seconds, that the job is supposed to take
+    double real_delay;
+};
+
+/**
+ * @brief The data associated to SMPI profiles
+ */
+struct SmpiProfileData
+{
+    std::vector<std::string> trace_filenames; //!< all defined tracefiles
+};
+
+/**
+ * @brief The data associated to SEQUENCE profiles
+ */
+struct SequenceProfileData
+{
+    unsigned int repeat;  //!< The number of times the sequence must be repeated
+    std::vector<std::string> sequence; //!< The sequence of profile names, executed in this order
+    std::vector<ProfilePtr> profile_sequence; //!< The sequence of profiles, executed in this order
+};
+
+/**
+ * @brief The data associated to PARALLEL_HOMOGENEOUS_PFS profiles
+ */
+struct ParallelHomogeneousPFSProfileData
+{
+    double bytes_to_read = 0;             //!< The amount of bytes to reads from the PFS storage node for each nodes (default: 0)
+    double bytes_to_write = 0;            //!< The amount of bytes to writes to the PFS storage for each nodes (default: 0)
+    std::string storage_label = "pfs";    //!< A label that defines the PFS storage node (default: "pfs")
+};
+
+/**
+ * @brief The data associated to DATA_STAGING profiles
+ */
+struct DataStagingProfileData
+{
+    double nb_bytes;                  //!< The number of bytes to transfer between the two storage nodes
+    std::string from_storage_label ;  //!< The storage label where data comes from
+    std::string to_storage_label ;    //!< The storage label where data goes to
+};
+
+/**
+ * @brief The data associated to SCHEDULER_SEND profiles
+ */
+struct SchedulerSendProfileData
+{
+    rapidjson::Document message; //!< The message being sent to the scheduler
+    double sleeptime; //!< The time to sleep after sending the message.
+};
+
+/**
+ * @brief The data associated to SCHEDULER_RECV profiles
+ */
+struct SchedulerRecvProfileData
+{
+    std::string regex; //!< The regex which is tested for matching
+    std::string on_success; //!< The profile to execute if it matches
+    std::string on_failure; //!< The profile to execute if it does not match
+    std::string on_timeout; //!< The profile to execute if no message is in the buffer (i.e. the scheduler has not answered in time). Can be omitted which will result that the job will wait until its walltime is reached.
+    double polltime; //!< The time to sleep between polling if on_timeout is not set.
+};
+
+class Profiles
+{
+public:
+    
+    Profiles() = default;
+    ~Profiles();
+    void load_from_json(const rapidjson::Document & doc, const std::string & filename);
+    void add_profile(ProfilePtr profile);
+    ProfilePtr operator[](const std::string & profile_name);
+    const ProfilePtr operator[](const std::string & profile_name) const;
+    
+    bool exists(const std::string & profile_name) const;
+    void add_profile(const std::string & profile_name, ProfilePtr & profile);
+    void remove_profile(const std::string & profile_name);
+    /**
+     * @brief Remove all unreferenced profiles from a Profiles instance (but remembers the profiles existed at some point)
+     */
+    int nb_profiles() const;
+     /**
+     * @brief Accesses one profile thanks to its name
+     * @param[in] profile_name The name of the profile
+     * @return The profile whose name is profile_name
+     * @pre Such a profile exists
+     */
+    ProfilePtr at(const std::string & profile_name);
+    /**
+     * @brief Accesses one profile thanks to its name (const version)
+     * @param[in] profile_name The name of the profile
+     * @return The profile whose name is profile_name
+     * @pre Such a profile exists
+     */
+    const ProfilePtr at(const std::string & profile_name) const;
+     /**
+     * @brief Returns a copy of the internal std::map used in the Profiles
+     * @return A copy of the internal std::map used in the Profiles
+     */
+    const std::unordered_map<std::string, ProfilePtr> profiles() const;
+private:
+    std::unordered_map<std::string, ProfilePtr> _profiles; //!< Stores all the profiles, indexed by their names. Value can be nullptr, meaning that the profile is no longer in memory but existed in the past.
+};
+
+}
+
+#endif
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/batsched_workload.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/batsched_workload.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/batsched_workload.cpp	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/batsched_workload.cpp	2021-04-02 05:00:14.000000000 -0400
@@ -0,0 +1,260 @@
+#include <loguru.hpp>
+#include <fstream>
+#include <streambuf>
+#include <string>
+#include <algorithm>
+#include <regex>
+
+#include <boost/algorithm/string.hpp>
+#include <boost/algorithm/string/join.hpp>
+
+#include <rapidjson/document.h>
+#include <rapidjson/writer.h>
+#include <rapidjson/stringbuffer.h>
+#include "batsched_workload.hpp"
+#include "batsched_profile.hpp"
+#include "batsched_job.hpp"
+
+using namespace rapidjson;
+namespace r = rapidjson;
+namespace myBatsched{
+
+Workload *Workload::new_static_workload(const std::string & workload_name,
+                                        const std::string & workload_file,
+                                        bool checkpointing_on
+                                       )
+{
+    Workload * workload = new Workload;
+
+    workload->jobs = new Jobs;
+    workload->profiles = new Profiles;
+
+    workload->jobs->set_profiles(workload->profiles);
+    workload->jobs->set_workload(workload);
+    workload->name = workload_name;
+    workload->file = workload_file;
+    workload->_checkpointing_on = checkpointing_on;
+    if (! (workload_file == "dynamic"))
+        workload->_is_static = true;
+    return workload;
+}
+
+Workload *Workload::new_dynamic_workload(const std::string & workload_name,bool checkpointing_on)
+{
+    Workload * workload = new_static_workload(workload_name, "dynamic",checkpointing_on);
+
+    workload->_is_static = false;
+    return workload;
+}
+
+void Workload::load_from_batsim(const std::string &json_filename, const r::Value & job_json, const r::Value & profile_json)
+{
+        LOG_F(INFO,"Loading batsim workload %s %s",name.c_str(),json_filename.c_str());
+        Document d;
+        Value jobs_array,profiles_array;
+        
+        jobs_array.CopyFrom(job_json,d.GetAllocator());
+        profiles_array.CopyFrom(profile_json,d.GetAllocator());
+        
+        d.SetObject();
+        d.AddMember("jobs",jobs_array,d.GetAllocator());
+        d.AddMember("profiles",profiles_array,d.GetAllocator());
+       
+        profiles->load_from_json(d,json_filename);
+       
+        jobs->load_from_json(d,json_filename);
+       
+        LOG_F(INFO,"JSON workload parsed sucessfully. Read %d jobs and %d profiles.",
+             jobs->nb_jobs(), profiles->nb_profiles());
+}
+
+void Workload::load_from_json(const std::string &json_filename)
+{
+    LOG_F(INFO,"Loading JSON workload '%s'...", json_filename.c_str());
+    // Let the file content be placed in a string
+    std::ifstream ifile(json_filename);
+    CHECK_F(ifile.is_open(), "Cannot read file '%s'", json_filename.c_str());
+    std::string content;
+
+    ifile.seekg(0, std::ios::end);
+    content.reserve(static_cast<unsigned long>(ifile.tellg()));
+    ifile.seekg(0, std::ios::beg);
+
+    content.assign((std::istreambuf_iterator<char>(ifile)),
+                std::istreambuf_iterator<char>());
+
+    // JSON document creation
+    Document doc;
+    doc.Parse(content.c_str());
+    CHECK_F(!doc.HasParseError(), "Invalid JSON file '%s': could not be parsed", json_filename.c_str());
+    CHECK_F(doc.IsObject(), "Invalid JSON file '%s': not a JSON object", json_filename.c_str());
+
+    // Let's try to read the number of machines in the JSON document
+    CHECK_F(doc.HasMember("nb_res"), "Invalid JSON file '%s': the 'nb_res' field is missing", json_filename.c_str());
+    const Value & nb_res_node = doc["nb_res"];
+    CHECK_F(nb_res_node.IsInt(), "Invalid JSON file '%s': the 'nb_res' field is not an integer", json_filename.c_str());
+    int nb_machines = nb_res_node.GetInt();
+    CHECK_F(nb_machines > 0, "Invalid JSON file '%s': the value of the 'nb_res' field is invalid (%d)",
+               json_filename.c_str(), nb_machines);
+
+    profiles->load_from_json(doc, json_filename);
+    jobs->load_from_json(doc, json_filename);
+
+    LOG_F(INFO,"JSON workload parsed sucessfully. Read %d jobs and %d profiles.",
+             jobs->nb_jobs(), profiles->nb_profiles());
+    LOG_F(INFO,"Checking workload validity...");
+    check_validity();
+    LOG_F(INFO,"Workload seems to be valid.");
+   
+}
+void Workload::check_validity()
+{
+    // Let's check that every SEQUENCE-typed profile points to existing profiles
+    // And update the refcounting of these profiles
+    for (auto mit : profiles->profiles())
+    {
+        ProfilePtr profile = mit.second;
+        if (profile->type == ProfileType::SEQUENCE)
+        {
+            auto * data = static_cast<SequenceProfileData *>(profile->data);
+            data->profile_sequence.reserve(data->sequence.size());
+            for (const auto & prof : data->sequence)
+            {
+                (void) prof; // Avoids a warning if assertions are ignored
+                CHECK_F(profiles->exists(prof),
+                           "Invalid composed profile '%s': the used profile '%s' does not exist",
+                           mit.first.c_str(), prof.c_str());
+                // Adds one to the refcounting for the profile 'prof'
+                data->profile_sequence.push_back(profiles->at(prof));
+            }
+        }
+    }
+
+    // TODO : check that there are no circular calls between composed profiles...
+    // TODO: compute the constraint of the profile number of resources, to check if it matches the jobs that use it
+
+    // Let's check the profile validity of each job
+    for (const auto & mit : jobs->jobs())
+    {
+        check_single_job_validity(mit.second);
+    }
+}
+
+void Workload::check_single_job_validity(const JobPtr job)
+{
+    //TODO This is already checked during creation of the job in Job::from_json
+    CHECK_F(profiles->exists(job->profile->name),
+               "Invalid job %s: the associated profile '%s' does not exist",
+               job->id.to_cstring(), job->profile->name.c_str());
+
+    if (job->profile->type == ProfileType::PARALLEL)
+    {
+        auto * data = static_cast<ParallelProfileData *>(job->profile->data);
+        (void) data; // Avoids a warning if assertions are ignored
+        CHECK_F(data->nb_res == job->requested_nb_res,
+                   "Invalid job %s: the requested number of resources (%d) do NOT match"
+                   " the number of resources of the associated profile '%s' (%d)",
+                   job->id.to_cstring(), job->requested_nb_res, job->profile->name.c_str(), data->nb_res);
+    }
+    /*else if (job->profile->type == ProfileType::SEQUENCE)
+    {
+        // TODO: check if the number of resources matches a resource-constrained composed profile
+    }*/
+}
+bool Workload::is_static() const
+{
+    return _is_static;
+}
+Workload::~Workload()
+{
+    delete jobs;
+    delete profiles;
+
+    jobs = nullptr;
+    profiles = nullptr;
+}
+Workloads::~Workloads()
+{
+    for (auto mit : _workloads)
+    {
+        Workload * workload = mit.second;
+        delete workload;
+    }
+    _workloads.clear();
+}
+Workload *Workloads::operator[](const std::string &workload_name)
+{
+    return at(workload_name);
+}
+Workload *Workloads::at(const std::string &workload_name)
+{
+    return _workloads.at(workload_name);
+}
+const Workload *Workloads::at(const std::string &workload_name) const
+{
+    return _workloads.at(workload_name);
+}
+unsigned int Workloads::nb_workloads() const
+{
+    return static_cast<unsigned int>(_workloads.size());
+}
+
+unsigned int Workloads::nb_static_workloads() const
+{
+    unsigned int count = 0;
+
+    for (auto mit : _workloads)
+    {
+        Workload * workload = mit.second;
+
+        count += static_cast<unsigned int>(workload->is_static());
+    }
+
+    return count;
+}
+JobPtr Workloads::job_at(const JobIdentifier &job_id)
+{
+    return at(job_id.workload_name())->jobs->at(job_id);
+}
+
+const JobPtr Workloads::job_at(const JobIdentifier &job_id) const
+{
+    return at(job_id.workload_name())->jobs->at(job_id);
+}
+void Workloads::delete_jobs(const std::vector<JobIdentifier> & job_ids,
+                            const bool & garbage_collect_profiles)
+{
+    for (const JobIdentifier & job_id : job_ids)
+    {
+        at(job_id.workload_name())->jobs->delete_job(job_id, garbage_collect_profiles);
+    }
+}
+bool Workloads::exists(const std::string &workload_name) const
+{
+    return _workloads.count(workload_name) == 1;
+}
+std::map<std::string, Workload *> &Workloads::workloads()
+{
+    return _workloads;
+}
+bool Workloads::job_is_registered(const JobIdentifier &job_id)
+{
+    return at(job_id.workload_name())->jobs->exists(job_id);
+}
+
+bool Workloads::job_profile_is_registered(const JobIdentifier &job_id)
+{
+    //TODO this could be improved/simplified
+    auto job = at(job_id.workload_name())->jobs->at(job_id);
+    return at(job_id.workload_name())->profiles->exists(job->profile->name);
+}
+void Workloads::insert_workload(const std::string &workload_name, Workload *workload)
+{
+    CHECK_F(!exists(workload_name));
+    CHECK_F(!exists(workload->name));
+
+    workload->name = workload_name;
+    _workloads[workload_name] = workload;
+}
+}
+
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/batsched_workload.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/batsched_workload.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/batsched_workload.hpp	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/batsched_workload.hpp	2021-04-02 05:00:15.000000000 -0400
@@ -0,0 +1,204 @@
+#ifndef BATSCHED_WORKLOAD_HPP
+#define BATSCHED_WORKLOAD_HPP
+
+#include <string>
+#include <vector>
+#include <map>
+#include "pointers.hpp"
+#include <rapidjson/document.h>
+
+namespace r = rapidjson;
+namespace myBatsched{
+class Jobs;
+struct Job;
+class Profiles;
+struct JobIdentifier;
+
+class Workload
+{
+   public:
+    /**
+     * @brief Builds an empty static Workload (via dynamic allocation)
+     * @details Static workloads correspond to Batsim input files (workloads or workflows)
+     * @param[in] workload_name The workload name
+     * @param[in] workload_file The workload file name
+     * @return The newly created workload
+     */
+    static Workload * new_static_workload(const std::string & workload_name,
+                                          const std::string & workload_file,
+                                          bool checkpointing_on = false
+                                         );
+
+    /**
+     * @brief Builds an empty dynamic Workload (via dynamic allocation)
+     * @details Dynamic workloads are created by the decision process
+     * @param[in] workload_name The workload name
+     * @return The newly created workload
+     */
+    static Workload * new_dynamic_workload(const std::string & workload_name,
+                                            bool checkpointing_on = false);
+    
+
+    /**
+     * @brief Destroys a Workload
+     */
+    ~Workload();
+ /**
+     * @brief Loads a static workload from a JSON filename
+     * @param[in] json_filename The name of the JSON file
+     * @param[out] nb_machines The number of machines described in the JSON file
+     */
+    void load_from_json(const std::string & json_filename);
+    
+    void load_from_batsim(const std::string& filename, const r::Value & job_json,const r::Value & profile_json);
+    /**
+     * @brief Returns whether the workload is static (corresponding to a Batsim input workload/workflow) or not
+     * @return Whether the workload is static or not
+     */
+    /**
+     * @brief Checks whether a Workload is valid
+     */
+    void check_validity();
+
+    /**
+     * @brief Checks whether a single job is valid
+     * @param[in] job The job to examine
+     */
+    void check_single_job_validity(const JobPtr job);
+
+     bool is_static() const;
+public:
+std::string name; //!< The Workload name
+    std::string file = ""; //!< The Workload file if it exists
+    Jobs * jobs = nullptr; //!< The Jobs of the Workload
+    Profiles * profiles = nullptr; //!< The Profiles associated to the Jobs of the Workload
+    bool _is_static = false; //!< Whether the workload is dynamic or not
+    bool _checkpointing_on = false;
+    bool _compute_checkpointing = false;
+    double _MTBF = -1.0;
+    double _SMTBF = -1.0;
+
+};
+
+class Workloads
+{
+public:
+    Workloads() = default;
+    ~Workloads();
+    
+    /**
+     * @brief Allows to access a Workload thanks to its name
+     * @param[in] workload_name The name of the workload to access
+     * @return The workload associated with the given workload name
+     * @pre The workload exists
+     */
+    Workload * operator[](const std::string & workload_name);
+
+    /**
+     * @brief Allows to access a Workload thanks to its name
+     * @param[in] workload_name The name of the workload to access
+     * @return The workload associated with the given workload name
+     * @pre The workload exists
+     */
+    const Workload * operator[](const std::string & workload_name) const;
+
+    /**
+     * @brief Allows to access a Workload thanks to its name
+     * @param[in] workload_name The name of the workload to access
+     * @return The workload associated with the given workload name
+     * @pre The workload exists
+     */
+    Workload * at(const std::string & workload_name);
+
+    /**
+     * @brief Allows to access a Workload thanks to its name
+     * @param[in] workload_name The name of the workload to access
+     * @return The workload associated with the given workload name
+     * @pre The workload exists
+     */
+    const Workload * at(const std::string & workload_name) const;
+
+    /**
+     * @brief Returns the number of workloads
+     * @return The number of workloads
+     */
+    unsigned int nb_workloads() const;
+
+    /**
+     * @brief Returns the number of static workloads
+     * @details Static workloads are those corresponding to Batsim input files (input workloads or workflows)
+     * @return The number of static workloads
+     */
+    unsigned int nb_static_workloads() const;
+
+    /**
+     * @brief Allows to get a job from the Workloads
+     * @param[in] job_id The JobIdentifier
+     * @return The job which has been asked
+     * @pre The requested job exists
+     */
+    JobPtr job_at(const JobIdentifier & job_id);
+
+    /**
+     * @brief Allows to get a job from the Workloads (const version)
+     * @param[in] job_id The JobIdentifier
+     * @return The (const) job which has been asked
+     * @pre The requested job exists
+     */
+    const JobPtr job_at(const JobIdentifier & job_id) const;
+    
+    /**
+     * @brief Inserts a new Workload into a Workloads
+     * @param[in] workload_name The name of the new Workload to insert
+     * @param[in] workload The Workload to insert
+     * @pre There should be no existing Workload with the same name in the Workloads
+     */
+    void insert_workload(const std::string & workload_name,
+                         Workload * workload);
+    /**
+     * @brief Deletes jobs from the associated workloads
+     * @param[in] job_ids The vector of identifiers of the jobs to remove
+     * @param[in] garbage_collect_profiles Whether to remove profiles that are not used anymore
+     */
+    void delete_jobs(const std::vector<JobIdentifier> & job_ids,
+                     const bool & garbage_collect_profiles);
+    /**
+     * @brief Checks whether a Workload with the given name exist.
+     * @param[in] workload_name The name of the Workload whose existence is checked
+     * @return true if a Workload with the given name exists in the Workloads, false otherwise.
+     */
+    bool exists(const std::string & workload_name) const;
+        /**
+     * @brief Gets the internal map
+     * @return The internal map
+     */
+    std::map<std::string, Workload*> & workloads();
+     /**
+     * @brief Checks whether a job is registered in the associated workload
+     * @param[in] job_id The JobIdentifier
+     * @return True if the given is registered in the associated workload, false otherwise
+     */
+    bool job_is_registered(const JobIdentifier & job_id);
+      /**
+     * @brief Checks whether a job profile is registered in the workload it
+     * is attached to
+     * @param[in] job_id The JobIdentifier
+     * @return True if the given is registered in the associated workload, false otherwise
+     */
+    bool job_profile_is_registered(const JobIdentifier & job_id);
+public:
+        bool _checkpointing_on = false;
+        bool _compute_checkpointing = false;
+        double _MTBF = -1.0;
+        double _SMTBF = -1.0;
+private:
+    std::map<std::string, Workload*> _workloads; //!< Associates Workloads with their names
+
+
+
+};
+
+}
+
+
+#endif
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/pointers.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/pointers.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/external/pointers.hpp	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/external/pointers.hpp	2021-04-02 05:00:15.000000000 -0400
@@ -0,0 +1,11 @@
+#pragma once
+
+
+
+namespace myBatsched{
+    struct Job;
+struct Profile;
+typedef std::shared_ptr<Job> JobPtr; //!< A smart pointer towards a Job.
+typedef std::shared_ptr<Profile> ProfilePtr; //!< A smart pointer towards a Profile.
+
+}
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/isalgorithm.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/isalgorithm.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/isalgorithm.cpp	2021-04-04 01:01:41.813519288 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/isalgorithm.cpp	2021-04-02 05:00:21.000000000 -0400
@@ -23,6 +23,8 @@
     _jobs_killed_recently.clear();
     _jobs_whose_waiting_time_estimation_has_been_requested_recently.clear();
     _machines_whose_pstate_changed_recently.clear();
+    _machines_that_became_available_recently.clear();
+    _machines_that_became_unavailable_recently.clear();
     _nopped_recently = false;
     _consumed_joules_updated_recently = false;
     _consumed_joules = -1;
@@ -61,11 +63,12 @@
                                 job_ids.end());
 }
 
-void ISchedulingAlgorithm::on_job_killed(double date, const std::vector<string> &job_ids)
+
+
+void ISchedulingAlgorithm::on_job_killed(double date, const std::unordered_map<std::string,double> &job_ids)
 {
     (void) date;
-    _jobs_killed_recently.insert(_jobs_killed_recently.end(),
-                                 job_ids.begin(),
+    _jobs_killed_recently.insert(job_ids.begin(),
                                  job_ids.end());
 }
 
@@ -91,6 +94,12 @@
     _no_more_static_job_to_submit_received = true;
 }
 
+void ISchedulingAlgorithm::on_no_more_external_event_to_occur(double date)
+{
+    (void) date;
+    _no_more_external_event_to_occur_received = true;
+}
+
 void ISchedulingAlgorithm::on_answer_energy_consumption(double date, double consumed_joules)
 {
     (void) date;
@@ -98,6 +107,27 @@
     _consumed_joules_updated_recently = true;
 }
 
+void ISchedulingAlgorithm::on_machine_available_notify_event(double date, IntervalSet machines)
+{
+    (void) date;
+    _machines_that_became_available_recently += machines;
+}
+void ISchedulingAlgorithm::set_workloads(myBatsched::Workloads *w){
+    (void) w;
+}
+
+void ISchedulingAlgorithm::on_machine_unavailable_notify_event(double date, IntervalSet machines)
+{
+    (void) date;
+    _machines_that_became_unavailable_recently += machines;
+}
+void ISchedulingAlgorithm::on_job_fault_notify_event(double date, std::string job)  // ****************added
+{
+    (void) date;
+}
+void ISchedulingAlgorithm::on_myKillJob_notify_event(double date){
+    (void) date;
+}
 void ISchedulingAlgorithm::on_query_estimate_waiting_time(double date, const string &job_id)
 {
     (void) date;
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/isalgorithm.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/isalgorithm.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/isalgorithm.hpp	2021-04-04 01:01:41.813519288 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/isalgorithm.hpp	2021-04-02 05:00:12.000000000 -0400
@@ -5,6 +5,7 @@
 
 #include "decision.hpp"
 #include "queue.hpp"
+#include "external/batsched_workload.hpp"
 
 /**
  * @brief The base abstract class of (scheduling & machine state) decision algorithms
@@ -62,7 +63,7 @@
      * @param[in] date The date at which the jobs have been killed
      * @param[in] job_ids The identifiers of the jobs which have been finished
      */
-    virtual void on_job_killed(double date, const std::vector<std::string> & job_ids);
+    virtual void on_job_killed(double date, const std::unordered_map<std::string,double> & job_ids);
 
     /**
      * @brief This function is called when the power state of some machines have been changed
@@ -85,6 +86,12 @@
     virtual void on_no_more_static_job_to_submit_received(double date);
 
     /**
+     * @brief This function is called when the on_no_more_external_event_to_occur
+     *        notification is received
+     */
+    virtual void on_no_more_external_event_to_occur(double date);
+
+    /**
      * @brief This function is called when an ANSWER message about energy consumption is received
      * @param[in] date The date at which the ANSWER message has been received
      * @param[in] consumed_joules The number of joules consumed since time 0
@@ -92,6 +99,22 @@
     virtual void on_answer_energy_consumption(double date, double consumed_joules);
 
     /**
+     * @brief This function is called when a machine_available NOTIFY event is received.
+     * @param[in] date The date at which the NOTIFY event has been received.
+     * @param[in] machines The machines whose availability has changed.
+     */
+    virtual void on_machine_available_notify_event(double date, IntervalSet machines);
+
+    /**
+     * @brief This function is called when a machine_unavailable NOTIFY event is received.
+     * @param[in] date The date at which the NOTIFY event has been received.
+     * @param[in] machines The machines whose availability has changed.
+     */
+    virtual void on_machine_unavailable_notify_event(double date, IntervalSet machines);
+    
+    virtual void on_job_fault_notify_event(double date, std::string job);   //*********************added
+    virtual void on_myKillJob_notify_event(double date);
+    /**
      * @brief This function is called when a QUERY message about estimating waiting time of potential jobs is received.
      * @param[in] date The date at which the QUERY message has been received
      * @param[in] job_id The identifier of the potential job
@@ -126,6 +149,8 @@
      * @details This function should be called between make_decisions calls!
      */
     void clear_recent_data_structures();
+    virtual void set_workloads(myBatsched::Workloads *w);
+    
 
 protected:
     Workload * _workload;
@@ -137,13 +162,16 @@
     int _nb_machines = -1;
     RedisStorage * _redis = nullptr;
     bool _no_more_static_job_to_submit_received = false;
+    bool _no_more_external_event_to_occur_received = false;
 
 protected:
     std::vector<std::string> _jobs_released_recently;
     std::vector<std::string> _jobs_ended_recently;
-    std::vector<std::string> _jobs_killed_recently;
+    std::unordered_map<std::string, double> _jobs_killed_recently;
     std::vector<std::string> _jobs_whose_waiting_time_estimation_has_been_requested_recently;
     std::map<int, IntervalSet> _machines_whose_pstate_changed_recently;
+    IntervalSet _machines_that_became_available_recently;
+    IntervalSet _machines_that_became_unavailable_recently;
     bool _nopped_recently;
     bool _consumed_joules_updated_recently;
     double _consumed_joules;
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/main.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/main.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/main.cpp	2021-04-04 01:01:41.813519288 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/main.cpp	2021-04-02 05:00:17.000000000 -0400
@@ -1,5 +1,9 @@
 #include <stdio.h>
+#include <chrono>
+#include <thread>
+
 #include <vector>
+#include <unordered_map>
 #include <fstream>
 #include <set>
 
@@ -13,6 +17,12 @@
 
 #include "external/taywee_args.hpp"
 
+// Added to get profiles into batsched but we get the whole workload
+#include "external/batsched_workload.hpp"
+#include "external/batsched_job.hpp"
+#include "external/batsched_profile.hpp"
+
+
 #include "isalgorithm.hpp"
 #include "decision.hpp"
 #include "network.hpp"
@@ -34,6 +44,7 @@
 #include "algo/energy_watcher.hpp"
 #include "algo/filler.hpp"
 #include "algo/fcfs_fast.hpp"
+#include "algo/fcfs_fast2.hpp"
 #include "algo/killer.hpp"
 #include "algo/killer2.hpp"
 #include "algo/random.hpp"
@@ -46,6 +57,8 @@
 using namespace std;
 using namespace boost;
 
+namespace myB = myBatsched;
+
 namespace n = network;
 namespace r = rapidjson;
 
@@ -78,7 +91,7 @@
                                       "energy_bf", "energy_bf_dicho", "energy_bf_idle_sleeper",
                                       "energy_bf_monitoring",
                                       "energy_bf_monitoring_inertial", "energy_bf_subpart_sleeper",
-                                      "energy_watcher", "fcfs_fast",
+                                      "energy_watcher", "fcfs_fast", "fcfs_fast2",
                                       "filler", "killer", "killer2", "random", "rejecter",
                                       "sequencer", "sleeper", "submitter", "waiting_time_estimator"};
     const set<string> policies_set = {"basic", "contiguous"};
@@ -99,7 +112,7 @@
     args::ArgumentParser parser("A Batsim-compatible scheduler in C++.");
     args::HelpFlag flag_help(parser, "help", "Display this help menu", {'h', "help"});
     args::CompletionFlag completion(parser, {"complete"});
-
+    
     args::ValueFlag<double> flag_rjms_delay(parser, "delay", "Sets the expected time that the RJMS takes to do some things like killing a job", {'d', "rjms_delay"}, 5.0);
     args::ValueFlag<string> flag_selection_policy(parser, "policy", "Sets the resource selection policy. Available values are " + policies_string, {'p', "policy"}, "basic");
     args::ValueFlag<string> flag_socket_endpoint(parser, "endpoint", "Sets the socket endpoint.", {'s', "socket-endpoint"}, "tcp://*:28000");
@@ -286,6 +299,8 @@
             algo = new EnergyWatcher(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
         else if (scheduling_variant == "fcfs_fast")
             algo = new FCFSFast(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "fcfs_fast2")
+            algo = new FCFSFast2(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
         else if (scheduling_variant == "killer")
             algo = new Killer(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
         else if (scheduling_variant == "killer2")
@@ -302,36 +317,38 @@
             algo = new Submitter(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
         else if (scheduling_variant == "waiting_time_estimator")
             algo = new WaitingTimeEstimator(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
-
-        // Network
+        bool success = false;
         Network n;
         n.bind(socket_endpoint);
-
-        // Run the simulation
-        run(n, algo, decision, w, call_make_decisions_on_single_nop);
+           
+        run(n,algo,decision,w,call_make_decisions_on_single_nop);
+       
+          
+      
     }
     catch(const std::exception & e)
     {
+        
         string what = e.what();
 
         if (what == "Connection lost")
         {
-            LOG_F(ERROR, "%s", what.c_str());
+            LOG_F(ERROR, "if %s", what.c_str());
         }
         else
         {
-            LOG_F(ERROR, "%s", what.c_str());
-
-            delete queue;
-            delete order;
-
-            delete algo;
-            delete selector;
-
-            throw;
+             LOG_F(ERROR, "else %s", what.c_str());
+             
+             delete queue;
+             delete order;
+                                                    
+             delete algo;
+             delete selector;
+                                                                            
+             throw;
+             
         }
     }
-
     delete queue;
     delete order;
 
@@ -345,7 +362,7 @@
          Workload & workload, bool call_make_decisions_on_single_nop)
 {
     bool simulation_finished = false;
-
+    myB::Workloads myWorkloads;
     // Redis creation
     RedisStorage redis;
     bool redis_enabled = false;
@@ -401,7 +418,35 @@
                     redis.connect_to_server(redis_hostname, redis_port, nullptr);
                     redis.set_instance_key_prefix(redis_prefix);
                 }
-
+                //get the workloads
+                
+                
+                for (auto &member : event_data["workloads"].GetObject())
+                {
+                        std::string workload_name = member.name.GetString();
+                        std::string workload_filename = member.value.GetString();
+                        myB::Workload * myWorkload= myB::Workload::new_static_workload(workload_name,event_data["workloads"][workload_name.c_str()].GetString());
+                        myWorkload->_checkpointing_on = event_data["config"]["checkpointing_on"].GetBool();
+                        myWorkload->_compute_checkpointing = event_data["config"]["compute_checkpointing"].GetBool();
+                        myWorkload->_MTBF = event_data["config"]["MTBF"].GetDouble();
+                        myWorkload->_SMTBF = event_data["config"]["SMTBF"].GetDouble();
+                        r::Document myCopy;
+                        myCopy.CopyFrom(event_data,myCopy.GetAllocator());
+                        r::Value & temp = myCopy["jobs"];
+                        r::Value & job_json = temp[workload_name.c_str()];
+                        r::Value & temp2 = myCopy["profiles"];
+                        r::Value &profile_json = temp2[workload_name.c_str()];
+                        myWorkload->load_from_batsim(workload_filename,
+                                                     job_json,
+                                                     profile_json);
+                        myWorkloads.insert_workload(workload_name,myWorkload);
+                }
+                myWorkloads._checkpointing_on = event_data["config"]["checkpointing_on"].GetBool();
+                myWorkloads._compute_checkpointing = event_data["config"]["compute_checkpointing"].GetBool();
+                myWorkloads._MTBF = event_data["config"]["MTBF"].GetDouble();
+                myWorkloads._SMTBF = event_data["config"]["SMTBF"].GetDouble();
+                algo->set_workloads(&myWorkloads);
+            
                 d.set_redis(redis_enabled, &redis);
 
                 algo->set_nb_machines(nb_resources);
@@ -440,12 +485,12 @@
                 const r::Value & job_ids_map = event_data["job_progress"];
                 PPK_ASSERT_ERROR(job_ids_map.GetType() == r::kObjectType);
 
-                vector<string> job_ids;
+                std::unordered_map<std::string,double> job_ids;
 
                 for (auto itr = job_ids_map.MemberBegin(); itr != job_ids_map.MemberEnd(); ++itr)
                 {
                     string job_id = itr->name.GetString();
-                    job_ids.push_back(job_id);
+                    job_ids.insert({job_id,itr->value["progress"].GetDouble()});
                 }
 
                 algo->on_job_killed(current_date, job_ids);
@@ -502,6 +547,42 @@
                 {
                     algo->on_no_more_static_job_to_submit_received(current_date);
                 }
+                else if (notify_type == "no_more_external_event_to_occur")
+                {
+                    algo->on_no_more_external_event_to_occur(current_date);
+                }
+                else if (notify_type == "event_machine_available")
+                {
+                    IntervalSet resources = IntervalSet::from_string_hyphen(event_data["resources"].GetString(), " ");
+                    algo->on_machine_available_notify_event(current_date, resources);
+                }
+                else if (notify_type == "event_machine_unavailable")
+                {
+                    IntervalSet resources = IntervalSet::from_string_hyphen(event_data["resources"].GetString(), " ");
+                    algo->on_machine_unavailable_notify_event(current_date, resources);
+                }
+                else if (notify_type == "myKillJob")
+                {
+                    
+                        algo->on_myKillJob_notify_event(current_date);
+                    
+                }
+                else if (notify_type == "job_fault")
+                {
+                    LOG_F(INFO,"main.cpp notify_type==jobfault");
+                    std::string job = event_data["job"].GetString();
+                    algo->on_job_fault_notify_event(current_date,job);
+                }
+                else if (notify_type == "batsim_metadata")
+                {
+                    
+                    std::string json_desc = event_data["metadata"].GetString();
+                    LOG_F(INFO,"batsim_meta: %s",json_desc.c_str());
+                }
+                else if (notify_type == "test")
+                {
+                        LOG_F(INFO,"test %f",current_date);
+                }
                 else
                 {
                     throw runtime_error("Unknown NOTIFY type received. Type = " + notify_type);
@@ -519,9 +600,12 @@
         // make_decisions is not called if (!call_make_decisions_on_single_nop && single_nop_received)
         if (!(!call_make_decisions_on_single_nop && requested_callback_only))
         {
+            
             SortableJobOrder::UpdateInformation update_info(current_date);
             algo->make_decisions(message_date, &update_info, nullptr);
+            
             algo->clear_recent_data_structures();
+            
         }
 
         message_date = max(message_date, d.last_date());
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/main.cpp.orig /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/main.cpp.orig
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/main.cpp.orig	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/main.cpp.orig	2021-04-02 05:00:21.000000000 -0400
@@ -0,0 +1,613 @@
+#include <stdio.h>
+#include <vector>
+#include <unordered_map>
+#include <fstream>
+#include <set>
+
+#include <boost/program_options.hpp>
+#include <boost/algorithm/string.hpp>
+#include <boost/format.hpp>
+
+#include <rapidjson/document.h>
+
+#include <loguru.hpp>
+
+#include "external/taywee_args.hpp"
+
+// Added to get profiles into batsched but we get the whole workload
+#include "external/batsched_workload.hpp"
+#include "external/batsched_job.hpp"
+#include "external/batsched_profile.hpp"
+
+
+#include "isalgorithm.hpp"
+#include "decision.hpp"
+#include "network.hpp"
+#include "json_workload.hpp"
+#include "pempek_assert.hpp"
+#include "data_storage.hpp"
+
+#include "algo/conservative_bf.hpp"
+#include "algo/crasher.hpp"
+#include "algo/easy_bf.hpp"
+#include "algo/easy_bf_fast.hpp"
+#include "algo/easy_bf_plot_liquid_load_horizon.hpp"
+#include "algo/energy_bf.hpp"
+#include "algo/energy_bf_dicho.hpp"
+#include "algo/energy_bf_idle_sleeper.hpp"
+#include "algo/energy_bf_monitoring_period.hpp"
+#include "algo/energy_bf_monitoring_inertial_shutdown.hpp"
+#include "algo/energy_bf_machine_subpart_sleeper.hpp"
+#include "algo/energy_watcher.hpp"
+#include "algo/filler.hpp"
+#include "algo/fcfs_fast.hpp"
+#include "algo/fcfs_fast2.hpp"
+#include "algo/killer.hpp"
+#include "algo/killer2.hpp"
+#include "algo/random.hpp"
+#include "algo/rejecter.hpp"
+#include "algo/sleeper.hpp"
+#include "algo/sequencer.hpp"
+#include "algo/submitter.hpp"
+#include "algo/wt_estimator.hpp"
+
+using namespace std;
+using namespace boost;
+
+namespace myB = myBatsched;
+
+namespace n = network;
+namespace r = rapidjson;
+
+void run(Network & n, ISchedulingAlgorithm * algo, SchedulingDecision &d,
+         Workload &workload, bool call_make_decisions_on_single_nop = true);
+
+/** @def STR_HELPER(x)
+ *  @brief Helper macro to retrieve the string view of a macro.
+ */
+#define STR_HELPER(x) #x
+
+/** @def STR(x)
+ *  @brief Macro to get a const char* from a macro
+ */
+#define STR(x) STR_HELPER(x)
+
+/** @def BATSCHED_VERSION
+ *  @brief What batsched --version should return.
+ *
+ *  It is either set by CMake or set to vUNKNOWN_PLEASE_COMPILE_VIA_CMAKE
+**/
+#ifndef BATSCHED_VERSION
+    #define BATSCHED_VERSION vUNKNOWN_PLEASE_COMPILE_VIA_CMAKE
+#endif
+
+int main(int argc, char ** argv)
+{
+    const set<string> variants_set = {"conservative_bf", "crasher", "easy_bf", "easy_bf_fast",
+                                      "easy_bf_plot_liquid_load_horizon",
+                                      "energy_bf", "energy_bf_dicho", "energy_bf_idle_sleeper",
+                                      "energy_bf_monitoring",
+                                      "energy_bf_monitoring_inertial", "energy_bf_subpart_sleeper",
+                                      "energy_watcher", "fcfs_fast", "fcfs_fast2",
+                                      "filler", "killer", "killer2", "random", "rejecter",
+                                      "sequencer", "sleeper", "submitter", "waiting_time_estimator"};
+    const set<string> policies_set = {"basic", "contiguous"};
+    const set<string> queue_orders_set = {"fcfs", "lcfs", "desc_bounded_slowdown", "desc_slowdown",
+                                          "asc_size", "desc_size", "asc_walltime", "desc_walltime"};
+    const set<string> verbosity_levels_set = {"debug", "info", "quiet", "silent"};
+
+    const string variants_string = "{" + boost::algorithm::join(variants_set, ", ") + "}";
+    const string policies_string = "{" + boost::algorithm::join(policies_set, ", ") + "}";
+    const string queue_orders_string = "{" + boost::algorithm::join(queue_orders_set, ", ") + "}";
+    const string verbosity_levels_string = "{" + boost::algorithm::join(verbosity_levels_set, ", ") + "}";
+
+    ISchedulingAlgorithm * algo = nullptr;
+    ResourceSelector * selector = nullptr;
+    Queue * queue = nullptr;
+    SortableJobOrder * order = nullptr;
+
+    args::ArgumentParser parser("A Batsim-compatible scheduler in C++.");
+    args::HelpFlag flag_help(parser, "help", "Display this help menu", {'h', "help"});
+    args::CompletionFlag completion(parser, {"complete"});
+    
+    args::ValueFlag<double> flag_rjms_delay(parser, "delay", "Sets the expected time that the RJMS takes to do some things like killing a job", {'d', "rjms_delay"}, 5.0);
+    args::ValueFlag<string> flag_selection_policy(parser, "policy", "Sets the resource selection policy. Available values are " + policies_string, {'p', "policy"}, "basic");
+    args::ValueFlag<string> flag_socket_endpoint(parser, "endpoint", "Sets the socket endpoint.", {'s', "socket-endpoint"}, "tcp://*:28000");
+    args::ValueFlag<string> flag_scheduling_variant(parser, "variant", "Sets the scheduling variant. Available values are " + variants_string, {'v', "variant"}, "filler");
+    args::ValueFlag<string> flag_variant_options(parser, "options", "Sets the scheduling variant options. Must be formatted as a JSON object.", {"variant_options"}, "{}");
+    args::ValueFlag<string> flag_variant_options_filepath(parser, "options-filepath", "Sets the scheduling variant options as the content of the given filepath. Overrides the variant_options options.", {"variant_options_filepath"}, "");
+    args::ValueFlag<string> flag_queue_order(parser, "order", "Sets the queue order. Available values are " + queue_orders_string, {'o', "queue_order"}, "fcfs");
+    args::ValueFlag<string> flag_verbosity_level(parser, "verbosity-level", "Sets the verbosity level. Available values are " + verbosity_levels_string, {"verbosity"}, "info");
+    args::ValueFlag<bool> flag_call_make_decisions_on_single_nop(parser, "flag", "If set to true, make_decisions will be called after single NOP messages.", {"call_make_decisions_on_single_nop"}, true);
+    args::Flag flag_version(parser, "version", "Shows batsched version", {"version"});
+
+    try
+    {
+        parser.ParseCLI(argc, argv);
+
+        if (flag_rjms_delay.Get() < 0)
+            throw args::ValidationError(str(format("Invalid '%1%' parameter value (%2%): Must be non-negative.")
+                                            % flag_rjms_delay.Name()
+                                            % flag_rjms_delay.Get()));
+
+        if (queue_orders_set.find(flag_queue_order.Get()) == queue_orders_set.end())
+            throw args::ValidationError(str(format("Invalid '%1%' value (%2%): Not in %3%")
+                                            % flag_queue_order.Name()
+                                            % flag_queue_order.Get()
+                                            % queue_orders_string));
+
+        if (variants_set.find(flag_scheduling_variant.Get()) == variants_set.end())
+            throw args::ValidationError(str(format("Invalid '%1%' value (%2%): Not in %3%")
+                                            % flag_scheduling_variant.Name()
+                                            % flag_scheduling_variant.Get()
+                                            % variants_string));
+
+        if (verbosity_levels_set.find(flag_verbosity_level.Get()) == verbosity_levels_set.end())
+            throw args::ValidationError(str(format("Invalid '%1%' value (%2%): Not in %3%")
+                                            % flag_verbosity_level.Name()
+                                            % flag_verbosity_level.Get()
+                                            % verbosity_levels_string));
+    }
+    catch(args::Help)
+    {
+        parser.helpParams.addDefault = true;
+        printf("%s", parser.Help().c_str());
+        return 0;
+    }
+    catch (args::Completion & e)
+    {
+        printf("%s", e.what());
+        return 0;
+    }
+    catch(args::ParseError e)
+    {
+        printf("%s\n", e.what());
+        return 1;
+    }
+    catch(args::ValidationError e)
+    {
+        printf("%s\n", e.what());
+        return 1;
+    }
+
+    if (flag_version)
+    {
+        printf("%s\n", STR(BATSCHED_VERSION));
+        return 0;
+    }
+
+    string socket_endpoint = flag_socket_endpoint.Get();
+    string scheduling_variant = flag_scheduling_variant.Get();
+    string selection_policy = flag_selection_policy.Get();
+    string queue_order = flag_queue_order.Get();
+    string variant_options = flag_variant_options.Get();
+    string variant_options_filepath = flag_variant_options_filepath.Get();
+    string verbosity_level = flag_verbosity_level.Get();
+    double rjms_delay = flag_rjms_delay.Get();
+    bool call_make_decisions_on_single_nop = flag_call_make_decisions_on_single_nop.Get();
+
+    try
+    {
+        // Logging configuration
+        if (verbosity_level == "debug")
+            loguru::g_stderr_verbosity = loguru::Verbosity_1;
+        else if (verbosity_level == "quiet")
+            loguru::g_stderr_verbosity = loguru::Verbosity_WARNING;
+        else if (verbosity_level == "silent")
+            loguru::g_stderr_verbosity = loguru::Verbosity_OFF;
+        else
+            loguru::g_stderr_verbosity = loguru::Verbosity_INFO;
+
+        // Workload creation
+        Workload w;
+        w.set_rjms_delay(rjms_delay);
+
+        // Scheduling parameters
+        SchedulingDecision decision;
+
+        // Queue order
+        if (queue_order == "fcfs")
+            order = new FCFSOrder;
+        else if (queue_order == "lcfs")
+            order = new LCFSOrder;
+        else if (queue_order == "desc_bounded_slowdown")
+            order = new DescendingBoundedSlowdownOrder(1);
+        else if (queue_order == "desc_slowdown")
+            order = new DescendingSlowdownOrder;
+        else if (queue_order == "asc_size")
+            order = new AscendingSizeOrder;
+        else if (queue_order == "desc_size")
+            order = new DescendingSizeOrder;
+        else if (queue_order == "asc_walltime")
+            order = new AscendingWalltimeOrder;
+        else if (queue_order == "desc_walltime")
+            order = new DescendingWalltimeOrder;
+
+        queue = new Queue(order);
+
+        // Resource selector
+        if (selection_policy == "basic")
+            selector = new BasicResourceSelector;
+        else if (selection_policy == "contiguous")
+            selector = new ContiguousResourceSelector;
+        else
+        {
+            printf("Invalid resource selection policy '%s'. Available options are %s\n", selection_policy.c_str(), policies_string.c_str());
+            return 1;
+        }
+
+        // Scheduling variant options
+        if (!variant_options_filepath.empty())
+        {
+            ifstream variants_options_file(variant_options_filepath);
+
+            if (variants_options_file.is_open())
+            {
+                // Let's put the whole file content into one string
+                variants_options_file.seekg(0, ios::end);
+                variant_options.reserve(variants_options_file.tellg());
+                variants_options_file.seekg(0, ios::beg);
+
+                variant_options.assign((std::istreambuf_iterator<char>(variants_options_file)),
+                                        std::istreambuf_iterator<char>());
+            }
+            else
+            {
+                printf("Couldn't open variants options file '%s'. Aborting.\n", variant_options_filepath.c_str());
+                return 1;
+            }
+        }
+
+        rapidjson::Document json_doc_variant_options;
+        json_doc_variant_options.Parse(variant_options.c_str());
+        if (!json_doc_variant_options.IsObject())
+        {
+            printf("Invalid variant options: Not a JSON object. variant_options='%s'\n", variant_options.c_str());
+            return 1;
+        }
+        LOG_F(1, "variant_options = '%s'", variant_options.c_str());
+
+        // Scheduling variant
+        if (scheduling_variant == "filler")
+            algo = new Filler(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "conservative_bf")
+            algo = new ConservativeBackfilling(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "crasher")
+            algo = new Crasher(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "easy_bf")
+            algo = new EasyBackfilling(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "easy_bf_fast")
+            algo = new EasyBackfillingFast(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "easy_bf_plot_liquid_load_horizon")
+            algo = new EasyBackfillingPlotLiquidLoadHorizon(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "energy_bf")
+            algo = new EnergyBackfilling(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "energy_bf_dicho")
+            algo = new EnergyBackfillingDichotomy(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "energy_bf_idle_sleeper")
+            algo = new EnergyBackfillingIdleSleeper(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "energy_bf_monitoring")
+            algo = new EnergyBackfillingMonitoringPeriod(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "energy_bf_monitoring_inertial")
+            algo = new EnergyBackfillingMonitoringInertialShutdown(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "energy_bf_subpart_sleeper")
+            algo = new EnergyBackfillingMachineSubpartSleeper(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "energy_watcher")
+            algo = new EnergyWatcher(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "fcfs_fast")
+            algo = new FCFSFast(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "fcfs_fast2")
+            algo = new FCFSFast2(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "killer")
+            algo = new Killer(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "killer2")
+            algo = new Killer2(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "random")
+            algo = new Random(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "rejecter")
+            algo = new Rejecter(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "sequencer")
+            algo = new Sequencer(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "sleeper")
+            algo = new Sleeper(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "submitter")
+            algo = new Submitter(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+        else if (scheduling_variant == "waiting_time_estimator")
+            algo = new WaitingTimeEstimator(&w, &decision, queue, selector, rjms_delay, &json_doc_variant_options);
+
+        // Network
+        Network n;
+        n.bind(socket_endpoint);
+
+        // Run the simulation
+        run(n, algo, decision, w, call_make_decisions_on_single_nop);
+    }
+    catch(const std::exception & e)
+    {
+        string what = e.what();
+
+        if (what == "Connection lost")
+        {
+            LOG_F(ERROR, "if %s", what.c_str());
+        }
+        else
+        {
+            LOG_F(ERROR, "else %s", what.c_str());
+
+            delete queue;
+            delete order;
+
+            delete algo;
+            delete selector;
+
+            throw;
+        }
+    }
+    delete queue;
+    delete order;
+
+    delete algo;
+    delete selector;
+
+    return 0;
+}
+
+void run(Network & n, ISchedulingAlgorithm * algo, SchedulingDecision & d,
+         Workload & workload, bool call_make_decisions_on_single_nop)
+{
+    bool simulation_finished = false;
+    myB::Workloads myWorkloads;
+    // Redis creation
+    RedisStorage redis;
+    bool redis_enabled = false;
+    algo->set_redis(&redis);
+
+    while (!simulation_finished)
+    {
+        string received_message;
+        n.read(received_message);
+
+        if (boost::trim_copy(received_message).empty())
+            throw runtime_error("Empty message received (connection lost ?)");
+
+        d.clear();
+
+        r::Document doc;
+        doc.Parse(received_message.c_str());
+
+        double message_date = doc["now"].GetDouble();
+        double current_date = message_date;
+        bool requested_callback_received = false;
+
+        // Let's handle all received events
+        const r::Value & events_array = doc["events"];
+
+        for (unsigned int event_i = 0; event_i < events_array.Size(); ++event_i)
+        {
+            const r::Value & event_object = events_array[event_i];
+            const std::string event_type = event_object["type"].GetString();
+            current_date = event_object["timestamp"].GetDouble();
+            const r::Value & event_data = event_object["data"];
+
+            if (event_type == "SIMULATION_BEGINS")
+            {
+                int nb_resources;
+                // DO this for retrocompatibility with batsim 2 API
+                if (event_data.HasMember("nb_compute_resources"))
+                {
+                    nb_resources = event_data["nb_compute_resources"].GetInt();
+                }
+                else
+                {
+                    nb_resources = event_data["nb_resources"].GetInt();
+                }
+                redis_enabled = event_data["config"]["redis-enabled"].GetBool();
+
+                if (redis_enabled)
+                {
+                    string redis_hostname = event_data["config"]["redis-hostname"].GetString();
+                    int redis_port = event_data["config"]["redis-port"].GetInt();
+                    string redis_prefix = event_data["config"]["redis-prefix"].GetString();
+
+                    redis.connect_to_server(redis_hostname, redis_port, nullptr);
+                    redis.set_instance_key_prefix(redis_prefix);
+                }
+<<<<<<< HEAD
+                if (event_data["workloads"].IsArray())
+                    LOG_F(INFO,"Yup it's an array");
+                else{
+                    myWorkload= myB::Workload::new_static_workload("w0",event_data["workloads"]["w0"].GetString());
+                    myWorkload->_checkpointing_on = event_data["config"]["checkpointing_on"].GetBool();
+                    myWorkload->load_from_json(myWorkload->file);
+                    
+                    algo->set_workload_file(myWorkload);
+=======
+                //get the workloads
+                
+                for (auto &member : event_data["workloads"].GetObject())
+                {
+                        std::string workload_name = member.name.GetString();
+                        std::string workload_filename = member.value.GetString();
+                        myB::Workload * myWorkload= myB::Workload::new_static_workload(workload_name,event_data["workloads"][workload_name.c_str()].GetString());
+                        myWorkload->_checkpointing_on = event_data["config"]["checkpointing_on"].GetBool();
+                        const r::Value & temp = event_data["jobs"];
+                        const r::Value & job_json = temp[workload_name.c_str()];
+                        const r::Value & temp2 = event_data["profiles"];
+                        const r::Value &profile_json = temp2[workload_name.c_str()];
+                        myWorkload->load_from_batsim(workload_filename,
+                                                     job_json,
+                                                     profile_json);
+                        myWorkloads.insert_workload(workload_name,myWorkload);
+>>>>>>> 4740e92
+                }
+                myWorkloads._checkpointing_on = event_data["config"]["checkpointing_on"].GetBool();
+                
+                algo->set_workloads(&myWorkloads);
+            
+                d.set_redis(redis_enabled, &redis);
+
+                algo->set_nb_machines(nb_resources);
+                algo->on_simulation_start(current_date, event_data["config"]);
+            }
+            else if (event_type == "SIMULATION_ENDS")
+            {
+                algo->on_simulation_end(current_date);
+                simulation_finished = true;
+            }
+            else if (event_type == "JOB_SUBMITTED")
+            {
+                string job_id = event_data["job_id"].GetString();
+
+                if (redis_enabled)
+                    workload.add_job_from_redis(redis, job_id, current_date);
+                else
+                    workload.add_job_from_json_object(event_data["job"], job_id, current_date);
+
+                algo->on_job_release(current_date, {job_id});
+            }
+            else if (event_type == "JOB_COMPLETED")
+            {
+                string job_id = event_data["job_id"].GetString();
+                workload[job_id]->completion_time = current_date;
+                algo->on_job_end(current_date, {job_id});
+            }
+            else if (event_type == "RESOURCE_STATE_CHANGED")
+            {
+                IntervalSet resources = IntervalSet::from_string_hyphen(event_data["resources"].GetString(), " ");
+                string new_state = event_data["state"].GetString();
+                algo->on_machine_state_changed(current_date, resources, std::stoi(new_state));
+            }
+            else if (event_type == "JOB_KILLED")
+            {
+                const r::Value & job_ids_map = event_data["job_progress"];
+                PPK_ASSERT_ERROR(job_ids_map.GetType() == r::kObjectType);
+
+                std::unordered_map<std::string,double> job_ids;
+
+                for (auto itr = job_ids_map.MemberBegin(); itr != job_ids_map.MemberEnd(); ++itr)
+                {
+                    string job_id = itr->name.GetString();
+                    job_ids.insert({job_id,itr->value["progress"].GetDouble()});
+                }
+
+                algo->on_job_killed(current_date, job_ids);
+            }
+            else if (event_type == "REQUESTED_CALL")
+            {
+                requested_callback_received = true;
+                algo->on_requested_call(current_date);
+            }
+            else if (event_type == "ANSWER")
+            {
+                for (auto itr = event_data.MemberBegin(); itr != event_data.MemberEnd(); ++itr)
+                {
+                    string key_value = itr->name.GetString();
+
+                    if (key_value == "consumed_energy")
+                    {
+                        double consumed_joules = itr->value.GetDouble();
+                        algo->on_answer_energy_consumption(current_date, consumed_joules);
+                    }
+                    else
+                    {
+                        PPK_ASSERT_ERROR(false, "Unknown ANSWER type received '%s'", key_value.c_str());
+                    }
+                }
+            }
+            else if (event_type == "QUERY")
+            {
+                const r::Value & requests = event_data["requests"];
+
+                for (auto itr = requests.MemberBegin(); itr != requests.MemberEnd(); ++itr)
+                {
+                    string key_value = itr->name.GetString();
+
+                    if (key_value == "estimate_waiting_time")
+                    {
+                        const r::Value & request_object = itr->value;
+                        string job_id = request_object["job_id"].GetString();
+                        workload.add_job_from_json_object(request_object["job"], job_id, current_date);
+
+                        algo->on_query_estimate_waiting_time(current_date, job_id);
+                    }
+                    else
+                    {
+                        PPK_ASSERT_ERROR(false, "Unknown QUERY type received '%s'", key_value.c_str());
+                    }
+                }
+            }
+            else if (event_type == "NOTIFY")
+            {
+                string notify_type = event_data["type"].GetString();
+
+                if (notify_type == "no_more_static_job_to_submit")
+                {
+                    algo->on_no_more_static_job_to_submit_received(current_date);
+                }
+                else if (notify_type == "no_more_external_event_to_occur")
+                {
+                    algo->on_no_more_external_event_to_occur(current_date);
+                }
+                else if (notify_type == "event_machine_available")
+                {
+                    IntervalSet resources = IntervalSet::from_string_hyphen(event_data["resources"].GetString(), " ");
+                    algo->on_machine_available_notify_event(current_date, resources);
+                }
+                else if (notify_type == "event_machine_unavailable")
+                {
+                    IntervalSet resources = IntervalSet::from_string_hyphen(event_data["resources"].GetString(), " ");
+                    algo->on_machine_unavailable_notify_event(current_date, resources);
+                }
+                else if (notify_type == "myKillJob")
+                {
+                    
+                        algo->on_myKillJob_notify_event(current_date);
+                    
+                }
+                else if (notify_type == "job_fault")
+                {
+                    LOG_F(INFO,"main.cpp notify_type==jobfault");
+                    std::string job = event_data["job"].GetString();
+                    algo->on_job_fault_notify_event(current_date,job);
+                }
+                else if (notify_type == "batsim_metadata")
+                {
+                    
+                    std::string json_desc = event_data["metadata"].GetString();
+                    LOG_F(INFO,"batsim_meta: %s",json_desc.c_str());
+                }
+                else if (notify_type == "test")
+                {
+                        LOG_F(INFO,"test %f",current_date);
+                }
+                else
+                {
+                    throw runtime_error("Unknown NOTIFY type received. Type = " + notify_type);
+                }
+
+            }
+            else
+            {
+                throw runtime_error("Unknown event received. Type = " + event_type);
+            }
+        }
+
+        bool requested_callback_only = requested_callback_received && (events_array.Size() == 1);
+
+        // make_decisions is not called if (!call_make_decisions_on_single_nop && single_nop_received)
+        if (!(!call_make_decisions_on_single_nop && requested_callback_only))
+        {
+            
+            SortableJobOrder::UpdateInformation update_info(current_date);
+            algo->make_decisions(message_date, &update_info, nullptr);
+            
+            algo->clear_recent_data_structures();
+            
+        }
+
+        message_date = max(message_date, d.last_date());
+
+        const string & message_to_send = d.content(message_date);
+        n.write(message_to_send);
+    }
+}
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/protocol.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/protocol.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/protocol.cpp	2021-04-04 01:01:32.689607402 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/protocol.cpp	2021-04-02 05:00:20.000000000 -0400
@@ -315,6 +315,48 @@
 
     _events.PushBack(event, _alloc);
 }
+void JsonProtocolWriter::append_set_resources_available(IntervalSet machines,double date)
+{
+  /*{
+    "timestamp": 10.0,
+    "type": "SCHED_RESOURCES_AVAILABLE",
+    "data": {"resources": "1 2 3-5"}
+  }
+
+  */
+ PPK_ASSERT_ERROR(date >= _last_date, "Date inconsistency");
+ _last_date = date;
+ _is_empty = false;
+ Value data(rapidjson::kObjectType);
+ data.AddMember("resources",Value().SetString(machines.to_string_hyphen(" ", "-").c_str(),_alloc),_alloc);
+
+ Value event(rapidjson::kObjectType);
+ event.AddMember("timestamp",Value().SetDouble(date),_alloc);
+ event.AddMember("type",Value().SetString("SCHED_RESOURCES_AVAILABLE"),_alloc);
+ event.AddMember("data",data,_alloc);
+ _events.PushBack(event, _alloc);
+}
+void JsonProtocolWriter::append_set_resources_unavailable(IntervalSet machines, double date)
+{
+  /*{
+    "timestamp": 10.0,
+    "type": "SCHED_RESOURCES_UNAVAILABLE",
+    "data": {"resources": "1 2 3-5"}
+  }
+
+  */
+ PPK_ASSERT_ERROR(date >= _last_date, "Date inconsistency");
+ _last_date = date;
+ _is_empty = false;
+ Value data(rapidjson::kObjectType);
+ data.AddMember("resources",Value().SetString(machines.to_string_hyphen(" ", "-").c_str(),_alloc),_alloc);
+
+ Value event(rapidjson::kObjectType);
+ event.AddMember("timestamp",Value().SetDouble(date),_alloc);
+ event.AddMember("type",Value().SetString("SCHED_RESOURCES_UNAVAILABLE"),_alloc);
+ event.AddMember("data",data,_alloc);
+ _events.PushBack(event, _alloc);
+}
 
 void JsonProtocolWriter::append_set_job_metadata(const string & job_id,
                                                  const string & metadata,
@@ -386,6 +428,29 @@
 
     Value event(rapidjson::kObjectType);
     event.AddMember("timestamp", Value().SetDouble(date), _alloc);
+    event.AddMember("type", Value().SetString("NOTIFY"), _alloc);
+    event.AddMember("data", data, _alloc);
+
+    _events.PushBack(event, _alloc);
+}
+
+void JsonProtocolWriter::append_scheduler_continue_submitting_jobs(double date)
+{
+    /* {
+      "timestamp": 42.0,
+      "type": "NOTIFY",
+      "data": { "type": "continue_registration" }
+    } */
+
+    PPK_ASSERT_ERROR(date >= _last_date, "Date inconsistency");
+    _last_date = date;
+    _is_empty = false;
+
+    Value data(rapidjson::kObjectType);
+    data.AddMember("type", Value().SetString("continue_registration", _alloc), _alloc);
+
+    Value event(rapidjson::kObjectType);
+    event.AddMember("timestamp", Value().SetDouble(date), _alloc);
     event.AddMember("type", Value().SetString("NOTIFY"), _alloc);
     event.AddMember("data", data, _alloc);
 
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/protocol.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/protocol.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/protocol.hpp	2021-04-04 01:01:32.689607402 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/protocol.hpp	2021-04-02 05:00:18.000000000 -0400
@@ -104,6 +104,8 @@
                                            const std::string & new_state,
                                            double date)  = 0;
 
+    virtual void append_set_resources_available(IntervalSet machines,double date) =0;
+    virtual void append_set_resources_unavailable(IntervalSet machines, double date)=0;
     /**
      * @brief Appends a SET_JOB_METADATA event.
      * @param[in] job_id The job identifier
@@ -129,6 +131,8 @@
      */
     virtual void append_scheduler_finished_submitting_jobs(double date) = 0;
 
+    virtual void append_scheduler_continue_submitting_jobs(double date) = 0;
+
 
     // Management functions
     /**
@@ -256,7 +260,8 @@
     void append_set_resource_state(IntervalSet resources,
                                    const std::string & new_state,
                                    double date);
-
+    void append_set_resources_available(IntervalSet machines,double date);
+    void append_set_resources_unavailable(IntervalSet machines, double date);
     /**
      * @brief Appends a SET_JOB_METADATA event.
      * @param[in] job_id The job identifier
@@ -282,6 +287,7 @@
      */
     void append_scheduler_finished_submitting_jobs(double date);
 
+    void append_scheduler_continue_submitting_jobs(double date);
     // Management functions
     /**
      * @brief Clears inner content. Should be called directly after generate_current_message.
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/schedule.cpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/schedule.cpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/schedule.cpp	2021-04-04 01:01:41.817519250 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/schedule.cpp	2021-04-02 05:00:44.000000000 -0400
@@ -896,7 +896,7 @@
 
 void Schedule::output_to_svg(const string &filename_prefix)
 {
-    const int bufsize = 128;
+    const int bufsize = 4096;
     char *buf = new char[bufsize];
 
     snprintf(buf, bufsize, "%s%06d.svg", filename_prefix.c_str(), _output_number);
@@ -906,6 +906,96 @@
 
     delete[] buf;
 }
+
+void Schedule::dump_to_batsim_jobs_file(const string &filename) const
+{
+    ofstream f(filename);
+    if (f.is_open())
+    {
+        f << "job_id,submission_time,requested_number_of_resources,requested_time,starting_time,finish_time,allocated_resources\n";
+
+        PPK_ASSERT_ERROR(_profile.size() > 0);
+
+        const int buf_size = 4096;
+        char *buf = new char[buf_size];
+
+        map<const Job *, Rational> jobs_starting_times;
+        set<const Job *> current_jobs;
+        for (auto mit : _profile.begin()->allocated_jobs)
+        {
+            const Job *allocated_job = mit.first;
+            current_jobs.insert(allocated_job);
+            jobs_starting_times[allocated_job] = _profile.begin()->begin;
+        }
+
+        // Let's traverse the profile to find the beginning of each job
+        for (auto slice_it = _profile.begin(); slice_it != _profile.end(); ++slice_it)
+        {
+            const TimeSlice &slice = *slice_it;
+            set<const Job *> allocated_jobs;
+            for (auto mit : slice.allocated_jobs)
+            {
+                const Job *job = mit.first;
+                allocated_jobs.insert(job);
+            }
+
+            set<const Job *> finished_jobs;
+            set_difference(current_jobs.begin(), current_jobs.end(), allocated_jobs.begin(), allocated_jobs.end(),
+                std::inserter(finished_jobs, finished_jobs.end()));
+
+            for (const Job *job : finished_jobs)
+            {
+                // Find where the job has been allocated
+                PPK_ASSERT_ERROR(slice_it != _profile.begin());
+                auto previous_slice_it = slice_it;
+                --previous_slice_it;
+                IntervalSet job_machines = previous_slice_it->allocated_jobs.at(job);
+
+                snprintf(buf, buf_size, "%s,%g,%d,%g,%g,%g,%s\n",
+                         job->id.c_str(),
+                         job->submission_time,
+                         job->nb_requested_resources,
+                         (double)job->walltime,
+                         (double)jobs_starting_times[job],
+                         (double)slice_it->begin,
+                         job_machines.to_string_hyphen(" ", "-").c_str());
+                f << buf;
+            }
+
+            set<const Job *> new_jobs;
+            set_difference(allocated_jobs.begin(), allocated_jobs.end(), current_jobs.begin(), current_jobs.end(),
+                std::inserter(new_jobs, new_jobs.end()));
+
+            for (const Job *job : new_jobs)
+            {
+                jobs_starting_times[job] = slice.begin;
+            }
+
+            // Update current_jobs
+            for (const Job *job : finished_jobs)
+                current_jobs.erase(job);
+            for (const Job *job : new_jobs)
+                current_jobs.insert(job);
+        }
+
+        delete[] buf;
+    }
+
+    f.close();
+}
+
+void Schedule::incremental_dump_as_batsim_jobs_file(const string &filename_prefix)
+{
+    const int bufsize = 4096;
+    char *buf = new char[bufsize];
+
+    snprintf(buf, bufsize, "%s%06d.csv", filename_prefix.c_str(), _output_number);
+    _output_number = (_output_number + 1) % 10000000;
+
+    dump_to_batsim_jobs_file(buf);
+
+    delete[] buf;
+}
 
 int Schedule::nb_machines() const
 {
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/schedule.hpp /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/schedule.hpp
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/schedule.hpp	2021-04-04 01:01:41.817519250 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/schedule.hpp	2021-04-02 05:00:11.000000000 -0400
@@ -101,6 +101,9 @@
     void write_svg_to_file(const std::string & filename) const;
     void output_to_svg(const std::string & filename_prefix = "/tmp/schedule");
 
+    void dump_to_batsim_jobs_file(const std::string & filename) const;
+    void incremental_dump_as_batsim_jobs_file(const std::string & filename_prefix = "/tmp/schedule");
+
     int nb_machines() const;
 
 private:
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/src/.vscode/c_cpp_properties.json /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/.vscode/c_cpp_properties.json
--- /home/craig/LANL/Batsim/April_9/theres/batsched/src/.vscode/c_cpp_properties.json	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/src/.vscode/c_cpp_properties.json	2021-04-02 05:00:10.000000000 -0400
@@ -0,0 +1,16 @@
+{
+    "configurations": [
+        {
+            "name": "Linux",
+            "includePath": [
+                "${workspaceFolder}/**"
+            ],
+            "defines": [],
+            "compilerPath": "/usr/bin/clang",
+            "cStandard": "c11",
+            "cppStandard": "c++14",
+            "intelliSenseMode": "clang-x64"
+        }
+    ],
+    "version": 4
+}
\ No newline at end of file
diff -Naur '--exclude=.git' "/home/craig/LANL/Batsim/April_9/theres/batsched/test/conftest (2).py" "/home/craig/LANL/Batsim/April_9/theres/batsched_edit/test/conftest (2).py"
--- "/home/craig/LANL/Batsim/April_9/theres/batsched/test/conftest (2).py"	1969-12-31 19:00:00.000000000 -0500
+++ "/home/craig/LANL/Batsim/April_9/theres/batsched_edit/test/conftest (2).py"	2021-04-02 05:00:03.000000000 -0400
@@ -0,0 +1,84 @@
+#!/usr/bin/env python3
+import glob
+import pytest
+import subprocess
+from collections import namedtuple
+from os.path import abspath, basename
+
+Workload = namedtuple('Workload', ['name', 'filename'])
+Platform = namedtuple('Platform', ['name', 'filename'])
+
+def pytest_generate_tests(metafunc):
+    if 'platform' in metafunc.fixturenames:
+        platform_files = glob.glob('platforms/*.xml')
+        platforms = [Platform(
+            name=basename(platform_file).replace('.xml', ''),
+            filename=abspath(platform_file)) for platform_file in platform_files]
+        metafunc.parametrize('platform', platforms)
+
+    if 'workload' in metafunc.fixturenames:
+        workload_files = glob.glob('workloads/*.json')
+        workloads = [Workload(
+            name=basename(workload_file).replace('.json', ''),
+            filename=abspath(workload_file)) for workload_file in workload_files]
+        metafunc.parametrize('workload', workloads)
+
+    if 'basic_algo_no_param' in metafunc.fixturenames:
+        algos = [
+            'conservative_bf',
+            'easy_bf',
+            'easy_bf_fast',
+            'fcfs_fast',
+            'filler',
+            'rejecter',
+            'sequencer',
+            'waiting_time_estimator'
+        ]
+        metafunc.parametrize('basic_algo_no_param', algos)
+
+    if 'one_basic_algo' in metafunc.fixturenames:
+        algos = [
+            'conservative_bf',
+        ]
+        metafunc.parametrize('one_basic_algo', algos)
+
+    if 'redis_enabled' in metafunc.fixturenames:
+        metafunc.parametrize('redis_enabled', [True, False])
+
+    if 'monitoring_period' in metafunc.fixturenames:
+        metafunc.parametrize('monitoring_period', [600])
+
+    if 'inertial_function' in metafunc.fixturenames:
+        metafunc.parametrize('inertial_function', ['x2', 'p1'])
+
+    if 'idle_time_to_sedate' in metafunc.fixturenames:
+        metafunc.parametrize('idle_time_to_sedate', [0, 120])
+
+    if 'sedate_idle_on_classical_events' in metafunc.fixturenames:
+        metafunc.parametrize('sedate_idle_on_classical_events', [True, False])
+
+    if 'allow_future_switches' in metafunc.fixturenames:
+        metafunc.parametrize('allow_future_switches', [True])
+
+    if 'upper_llh_threshold' in metafunc.fixturenames:
+        metafunc.parametrize('upper_llh_threshold', [60])
+
+@pytest.fixture(scope="session", autouse=True)
+def manage_redis_server(request):
+    print('Trying to run a redis-server...')
+    proc = subprocess.Popen('redis-server', stdout=subprocess.PIPE)
+    try:
+        out, _ = proc.communicate(timeout=1)
+        if 'Address already in use' in str(out):
+            print("Could not run redis-server (address already in use).")
+            print("Assuming that the process using the TCP port is another redis-server instance and going on.")
+        else:
+            raise Exception("Could not run redis-server (unhandled reason), aborting.")
+    except subprocess.TimeoutExpired:
+        print('redis-server has been running for 1 second.')
+        print('Assuming redis-server has started successfully and going on.')
+
+    def on_finalize():
+        print('Killing the spawned redis-server (if any)...')
+        proc.kill()
+    request.addfinalizer(on_finalize)
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/test/conftest.py /home/craig/LANL/Batsim/April_9/theres/batsched_edit/test/conftest.py
--- /home/craig/LANL/Batsim/April_9/theres/batsched/test/conftest.py	2021-04-04 01:01:41.817519250 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/test/conftest.py	2021-04-02 05:00:03.000000000 -0400
@@ -1,6 +1,8 @@
 #!/usr/bin/env python3
-from collections import namedtuple
 import glob
+import pytest
+import subprocess
+from collections import namedtuple
 from os.path import abspath, basename
 
 Workload = namedtuple('Workload', ['name', 'filename'])
@@ -60,3 +62,23 @@
 
     if 'upper_llh_threshold' in metafunc.fixturenames:
         metafunc.parametrize('upper_llh_threshold', [60])
+
+@pytest.fixture(scope="session", autouse=True)
+def manage_redis_server(request):
+    print('Trying to run a redis-server...')
+    proc = subprocess.Popen('redis-server', stdout=subprocess.PIPE)
+    try:
+        out, _ = proc.communicate(timeout=1)
+        if 'Address already in use' in str(out):
+            print("Could not run redis-server (address already in use).")
+            print("Assuming that the process using the TCP port is another redis-server instance and going on.")
+        else:
+            raise Exception("Could not run redis-server (unhandled reason), aborting.")
+    except subprocess.TimeoutExpired:
+        print('redis-server has been running for 1 second.')
+        print('Assuming redis-server has started successfully and going on.')
+
+    def on_finalize():
+        print('Killing the spawned redis-server (if any)...')
+        proc.kill()
+    request.addfinalizer(on_finalize)
diff -Naur '--exclude=.git' "/home/craig/LANL/Batsim/April_9/theres/batsched/test/.gitignore (2)" "/home/craig/LANL/Batsim/April_9/theres/batsched_edit/test/.gitignore (2)"
--- "/home/craig/LANL/Batsim/April_9/theres/batsched/test/.gitignore (2)"	1969-12-31 19:00:00.000000000 -0500
+++ "/home/craig/LANL/Batsim/April_9/theres/batsched_edit/test/.gitignore (2)"	2021-04-02 05:00:04.000000000 -0400
@@ -0,0 +1,2 @@
+__pycache__
+.pytest_cache
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/test/Makefile /home/craig/LANL/Batsim/April_9/theres/batsched_edit/test/Makefile
--- /home/craig/LANL/Batsim/April_9/theres/batsched/test/Makefile	2021-04-04 01:01:41.817519250 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/test/Makefile	1969-12-31 19:00:00.000000000 -0500
@@ -1,2 +0,0 @@
-test:
-	nix-shell ../ci -A test_deps_pinned --command 'cd .. && bash ./ci/run-tests.bash'
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/test/README.md /home/craig/LANL/Batsim/April_9/theres/batsched_edit/test/README.md
--- /home/craig/LANL/Batsim/April_9/theres/batsched/test/README.md	2021-04-04 01:01:41.817519250 -0400
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/test/README.md	2021-04-02 05:00:02.000000000 -0400
@@ -1,10 +1,18 @@
 ### Running tests
+``` bash
+nix-shell ../release.nix -A integration_tests --command 'pytest'
+# or just pytest, but you must prepare your env...
 ```
-make test
-# or pytest, but you must prepare your env, run redis...
+
+Optionally, use Batsim's binary cache to avoid recompiling many things (e.g., SimGrid).
+
+``` bash
+nix-env -iA cachix -f https://cachix.org/api/v1/install # installs cachix
+cachix use batsim # add Batsim's cachix storage as a Nix remote cache
 ```
 
-### How it works?
+### How does it work?
+0. nix-shell puts you into an environment where batsched, batsim, robin, redis, etc. are available (code in [release.nix])
 1. pytest generates combinations of test input (code in [conftest.py])
 2. for each combination of inputs: (code in [test_runner.py])
   1. pytest generates a [robin] input file
@@ -19,11 +27,12 @@
 
 You can also run batsim and batsched in different terminals:
 ``` bash
-# feel free to hack  e.g., prepend commands with gdb, valgrind...
+# feel free to hack these files  e.g., prepend commands with gdb, valgrind...
 ./test-out/FAILING-TEST/cmd/batsim.bash
 ./test-out/FAILING-TEST/cmd/sched.bash
 ```
 
+[release.nix]: ../release.nix
 [conftest.py]: ./conftest.py
 [test_runner.py]: ./test_runner.py
 [robin]: https://framagit.org/batsim/batexpe
diff -Naur '--exclude=.git' "/home/craig/LANL/Batsim/April_9/theres/batsched/test/test_energy (2).py" "/home/craig/LANL/Batsim/April_9/theres/batsched_edit/test/test_energy (2).py"
--- "/home/craig/LANL/Batsim/April_9/theres/batsched/test/test_energy (2).py"	1969-12-31 19:00:00.000000000 -0500
+++ "/home/craig/LANL/Batsim/April_9/theres/batsched_edit/test/test_energy (2).py"	2021-04-02 05:00:00.000000000 -0400
@@ -0,0 +1,74 @@
+import json
+
+from helper import *
+
+def energy_model_instance():
+    return {
+        "power_sleep":9.75,
+        "power_idle":95,
+        "energy_switch_on":19030,
+        "power_compute":190.738,
+        "energy_switch_off":620,
+        "time_switch_off":6.1,
+        "pstate_sleep":13,
+        "pstate_compute":0,
+        "time_switch_on":152
+    }
+
+# def test_inertial_shutdown(platform, workload,
+#     monitoring_period,
+#     allow_future_switches,
+#     upper_llh_threshold ,
+#     inertial_function,
+#     idle_time_to_sedate,
+#     sedate_idle_on_classical_events):
+#     algo = 'energy_bf_monitoring_inertial'
+#     test_name = f'{algo}-{platform.name}-{workload.name}-{inertial_function}-{idle_time_to_sedate}-{sedate_idle_on_classical_events}'
+#     output_dir, robin_filename, schedconf_filename = init_instance(test_name)
+
+#     batcmd = gen_batsim_cmd(platform.filename, workload.filename, output_dir, "--energy")
+
+#     schedconf_content = {
+#         "output_dir": output_dir,
+#         "monitoring_period": monitoring_period,
+#         "trace_output_filename": f'{output_dir}/batsched_llh.trace',
+#         "allow_future_switches": allow_future_switches,
+#         "upper_llh_threshold": upper_llh_threshold,
+#         "inertial_alteration": inertial_function,
+#         "idle_time_to_sedate": idle_time_to_sedate,
+#         "sedate_idle_on_classical_events": sedate_idle_on_classical_events
+#     }
+#     schedconf_content = dict(schedconf_content, **energy_model_instance())
+#     write_file(schedconf_filename, json.dumps(schedconf_content))
+
+#     instance = RobinInstance(output_dir=output_dir,
+#         batcmd=batcmd,
+#         schedcmd=f"batsched -v '{algo}' --variant_options_filepath '{schedconf_filename}'",
+#         simulation_timeout=30, ready_timeout=5,
+#         success_timeout=10, failure_timeout=0
+#     )
+
+#     instance.to_file(robin_filename)
+#     ret = run_robin(robin_filename)
+#     assert ret.returncode == 0
+
+def test_sleeper(platform, workload):
+    algo = 'sleeper'
+    test_name = f'{algo}-{platform.name}-{workload.name}'
+    output_dir, robin_filename, schedconf_filename = init_instance(test_name)
+
+    batcmd = gen_batsim_cmd(platform.filename, workload.filename, output_dir, "--energy")
+
+    schedconf_content = energy_model_instance()
+    write_file(schedconf_filename, json.dumps(schedconf_content))
+
+    instance = RobinInstance(output_dir=output_dir,
+        batcmd=batcmd,
+        schedcmd=f"batsched -v '{algo}' --variant_options_filepath '{schedconf_filename}'",
+        simulation_timeout=30, ready_timeout=5,
+        success_timeout=10, failure_timeout=0
+    )
+
+    instance.to_file(robin_filename)
+    ret = run_robin(robin_filename)
+    assert ret.returncode == 0
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/.vscode/c_cpp_properties.json /home/craig/LANL/Batsim/April_9/theres/batsched_edit/.vscode/c_cpp_properties.json
--- /home/craig/LANL/Batsim/April_9/theres/batsched/.vscode/c_cpp_properties.json	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/.vscode/c_cpp_properties.json	2021-04-02 05:00:05.000000000 -0400
@@ -0,0 +1,15 @@
+{
+    "configurations": [
+      {
+        "name": "Linux",
+        "includePath": ["${workspaceFolder}/src/**","/nix/store/**"],
+        "defines": [],
+        "compilerPath": "/usr/bin/gcc",
+        "cStandard": "c11",
+        "cppStandard": "c++11",
+        "intelliSenseMode": "clang-x64"
+
+      }
+    ],
+    "version": 4
+}
diff -Naur '--exclude=.git' /home/craig/LANL/Batsim/April_9/theres/batsched/.vscode/settings.json /home/craig/LANL/Batsim/April_9/theres/batsched_edit/.vscode/settings.json
--- /home/craig/LANL/Batsim/April_9/theres/batsched/.vscode/settings.json	1969-12-31 19:00:00.000000000 -0500
+++ /home/craig/LANL/Batsim/April_9/theres/batsched_edit/.vscode/settings.json	2021-04-02 05:00:06.000000000 -0400
@@ -0,0 +1,3 @@
+{
+    "C_Cpp.loggingLevel": "Debug"
+}
\ No newline at end of file
